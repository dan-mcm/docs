{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dan\u2019s Docs Who am I? Daniel McMahon . Connect Github LinkedIn Education Higher Diploma in Computer Science (Conversion) 2016-17 Professional Diploma in Education 2012-2013 BA Music & Geography 2009-2012 Work Software Engineer 2017-Present Education Officer 2013-2016 Student Teacher 2012-2013 Various Retail Locations 2007-2008 What are these docs and who are they for? (Me) => { Making notes & resources helps me concrete my own knowledge. Docs provide a useful and quick reference for day to day work. An excuse to experiment with a new documentation library. } (You) => { Maybe there is some useful knowledge in here relating to a language or tech concept that will help Maybe you would like to adopt something similar for your own purposes and will use this as a template } Why am I using mkdocs? It\u2019s quick and easy. I wanted to have a docs page I can quickly update when the mood strikes. Being able to update the documentation using markdown and not worrying about custom styling and css is crucial to achieve this. Once I have my knowledge solidified in a markdown structure I may consider switching to something like Gatsby or Hugo. Having previously designed page search functionality, it\u2019s also so much easier to use a pre-made solution like mkdocs that has search integrated. There\u2019s no point having all these resources if I can\u2019t search for what I need quickly and efficiently now is there?","title":"Home"},{"location":"#dans-docs","text":"","title":"Dan's Docs"},{"location":"#who-am-i","text":"Daniel McMahon . Connect Github LinkedIn Education Higher Diploma in Computer Science (Conversion) 2016-17 Professional Diploma in Education 2012-2013 BA Music & Geography 2009-2012 Work Software Engineer 2017-Present Education Officer 2013-2016 Student Teacher 2012-2013 Various Retail Locations 2007-2008","title":"Who am I?"},{"location":"#what-are-these-docs-and-who-are-they-for","text":"(Me) => { Making notes & resources helps me concrete my own knowledge. Docs provide a useful and quick reference for day to day work. An excuse to experiment with a new documentation library. } (You) => { Maybe there is some useful knowledge in here relating to a language or tech concept that will help Maybe you would like to adopt something similar for your own purposes and will use this as a template }","title":"What are these docs and who are they for?"},{"location":"#why-am-i-using-mkdocs","text":"It\u2019s quick and easy. I wanted to have a docs page I can quickly update when the mood strikes. Being able to update the documentation using markdown and not worrying about custom styling and css is crucial to achieve this. Once I have my knowledge solidified in a markdown structure I may consider switching to something like Gatsby or Hugo. Having previously designed page search functionality, it\u2019s also so much easier to use a pre-made solution like mkdocs that has search integrated. There\u2019s no point having all these resources if I can\u2019t search for what I need quickly and efficiently now is there?","title":"Why am I using mkdocs?"},{"location":"aws/api-gateway/","text":"AWS API Gateway What is an API An API is an Application Programming Interface. Types of APIs REST APIs (Representational State Transfer) Uses JSON SOAP APIs (Simple Object Access Protocol) Uses XML What is API Gateway? Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a \u2018front door\u2019 for applications to access data, business logic, or functionality from your back-end services, such as applications running on EC2, code running on Lambda, or any web application. What can API Gateway Do? Expose HTTPS endpoints to define a RESTful API Serverless-ly connect to services like Lambda & DynamoDB Send each API endpoint to a different target Run efficiently with low cost Scale effortlessly Track and control usage by API key Throttle requests to prevent attacks Connect to CloudWatch to log all requests for monitoring Maintain multiple versions of your API How do I configure API Gateway? Define an API (container) Define Resources and nested Resources (URL paths) For each Resource: Select supported HTTP methods (verbs) Set security Choose target (i.e. EC2, Lambda, DynamoDB, etc.) Set request and response transformations Deploy API to a Stage uses API Gateway domain, by default Can use custom domain Now supports AWS Cert Manager: Free SSL/TLS certs! What is API Caching? You can enable API caching in Amazon API Gateway to cache your endpoint\u2019s response. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of the requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint. Same Origin Policy In computing, the same-origin policy is an important concept in the web application secruiy model. Under the policy, a web browser ermits scripts contained in a frist web page to access data in a second web page, but only if both web pages have the same origin. This is done to prevent Cross-Site Scripting (XSS) attacks. Enforced by web browsers. Ignored by tools like Postman and curl. Cross-Origin Resource Sharing (CORS) Cross-Origin Resource Sharing (CORS) is one way the server at the other end (not client code in browser) can relax the same-origin policy. Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources (e.g. fonts) on a web page to be requested from another domain outside the domain from which the first resource was served. Browser makes an HTTPS OPTIONS call for a URL OPTIONS is an HTTP method like GET, PUT and POST Server returns a response that says: \u201cThese other domains are approved to GET this URL\u201d Error - \u201cOrigin policy cannot be read at the remote resource?\u201d You need to enable CORS on API Gateway API Gateway Exam Tips Remember what API Gateway is at a high level API Gateway has caching capabilities to increase performance API Gateway is low cost and scales automatically. (don\u2019t need autoscale groups) You can throttle API Gateway to prevent attacks You can log results to CloudWatch If you are using JavaScript/AJAX that uses multiple domains with API Gateway, ensure that you have enabled CORS on API Gateway CORS is enforced by the client Advanced API Gateway Import APIs You can use the API Gateway Import API feature to import an API from an external definition file into API Gateway. Currently, the Import API feature supports Swagger v2.0 definition files. With the Import API, you can either create a new API by submitting a POST request that includes a Swagger definition in the payload and endpoint configuration, or you can update an existing API by using a PUT request that contains a Swagger definition in the payload. You can update an API by overwriting it with a new definition, or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL. API Throttling By default, API Gateway limits the steady-state request rate to 10,000 requests per second (RPS). The maximum concurrent requests is 5000 requests across all APIs within an AWS account. If you go over 10,000 requests per second or 5000 concurrent requests you will receive a 429 Too Many Requests error response. If a caller submits 10k requests in a one second period evenly (i.e. 10 requests every millisecond), API Gateway process all requests without dropping any. If the caller sends 10k requests in the first millisecond, API Gateway serves 5k of those requests and throttles the rest in the one-second period. If the caller submits 5k requests in the first millisecond and then evenly spread another 5k requests through the remaining 999 milliseconds (e.g. about 5 requests every millisecond), API Gateway processes all 10k requests in the one-second period without returning 429 Too Many Requests error responses. SOAP Webservice Passthrough You can configure API Gateway as a SOAP webservice passthrough. Advanced API Gateway Exam Tips Import API\u2019s using Swagger 2.0 definition files API Gateway can be throttled Default limits are 10k RPS or 5k concurrently You can configure API Gateway as a SOAP webservice passthrough","title":"API Gateway"},{"location":"aws/api-gateway/#aws-api-gateway","text":"","title":"AWS API Gateway"},{"location":"aws/api-gateway/#what-is-an-api","text":"An API is an Application Programming Interface.","title":"What is an API"},{"location":"aws/api-gateway/#types-of-apis","text":"REST APIs (Representational State Transfer) Uses JSON SOAP APIs (Simple Object Access Protocol) Uses XML","title":"Types of APIs"},{"location":"aws/api-gateway/#what-is-api-gateway","text":"Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, and secure APIs at any scale. With a few clicks in the AWS Management Console, you can create an API that acts as a \u2018front door\u2019 for applications to access data, business logic, or functionality from your back-end services, such as applications running on EC2, code running on Lambda, or any web application.","title":"What is API Gateway?"},{"location":"aws/api-gateway/#what-can-api-gateway-do","text":"Expose HTTPS endpoints to define a RESTful API Serverless-ly connect to services like Lambda & DynamoDB Send each API endpoint to a different target Run efficiently with low cost Scale effortlessly Track and control usage by API key Throttle requests to prevent attacks Connect to CloudWatch to log all requests for monitoring Maintain multiple versions of your API","title":"What can API Gateway Do?"},{"location":"aws/api-gateway/#how-do-i-configure-api-gateway","text":"Define an API (container) Define Resources and nested Resources (URL paths) For each Resource: Select supported HTTP methods (verbs) Set security Choose target (i.e. EC2, Lambda, DynamoDB, etc.) Set request and response transformations Deploy API to a Stage uses API Gateway domain, by default Can use custom domain Now supports AWS Cert Manager: Free SSL/TLS certs!","title":"How do I configure API Gateway?"},{"location":"aws/api-gateway/#what-is-api-caching","text":"You can enable API caching in Amazon API Gateway to cache your endpoint\u2019s response. With caching, you can reduce the number of calls made to your endpoint and also improve the latency of the requests to your API. When you enable caching for a stage, API Gateway caches responses from your endpoint for a specified time-to-live (TTL) period, in seconds. API Gateway then responds to the request by looking up the endpoint response from the cache instead of making a request to your endpoint.","title":"What is API Caching?"},{"location":"aws/api-gateway/#same-origin-policy","text":"In computing, the same-origin policy is an important concept in the web application secruiy model. Under the policy, a web browser ermits scripts contained in a frist web page to access data in a second web page, but only if both web pages have the same origin. This is done to prevent Cross-Site Scripting (XSS) attacks. Enforced by web browsers. Ignored by tools like Postman and curl.","title":"Same Origin Policy"},{"location":"aws/api-gateway/#cross-origin-resource-sharing-cors","text":"Cross-Origin Resource Sharing (CORS) is one way the server at the other end (not client code in browser) can relax the same-origin policy. Cross-origin resource sharing (CORS) is a mechanism that allows restricted resources (e.g. fonts) on a web page to be requested from another domain outside the domain from which the first resource was served. Browser makes an HTTPS OPTIONS call for a URL OPTIONS is an HTTP method like GET, PUT and POST Server returns a response that says: \u201cThese other domains are approved to GET this URL\u201d Error - \u201cOrigin policy cannot be read at the remote resource?\u201d You need to enable CORS on API Gateway","title":"Cross-Origin Resource Sharing (CORS)"},{"location":"aws/api-gateway/#api-gateway-exam-tips","text":"Remember what API Gateway is at a high level API Gateway has caching capabilities to increase performance API Gateway is low cost and scales automatically. (don\u2019t need autoscale groups) You can throttle API Gateway to prevent attacks You can log results to CloudWatch If you are using JavaScript/AJAX that uses multiple domains with API Gateway, ensure that you have enabled CORS on API Gateway CORS is enforced by the client","title":"API Gateway Exam Tips"},{"location":"aws/api-gateway/#advanced-api-gateway","text":"","title":"Advanced API Gateway"},{"location":"aws/api-gateway/#import-apis","text":"You can use the API Gateway Import API feature to import an API from an external definition file into API Gateway. Currently, the Import API feature supports Swagger v2.0 definition files. With the Import API, you can either create a new API by submitting a POST request that includes a Swagger definition in the payload and endpoint configuration, or you can update an existing API by using a PUT request that contains a Swagger definition in the payload. You can update an API by overwriting it with a new definition, or merge a definition with an existing API. You specify the options using a mode query parameter in the request URL.","title":"Import APIs"},{"location":"aws/api-gateway/#api-throttling","text":"By default, API Gateway limits the steady-state request rate to 10,000 requests per second (RPS). The maximum concurrent requests is 5000 requests across all APIs within an AWS account. If you go over 10,000 requests per second or 5000 concurrent requests you will receive a 429 Too Many Requests error response. If a caller submits 10k requests in a one second period evenly (i.e. 10 requests every millisecond), API Gateway process all requests without dropping any. If the caller sends 10k requests in the first millisecond, API Gateway serves 5k of those requests and throttles the rest in the one-second period. If the caller submits 5k requests in the first millisecond and then evenly spread another 5k requests through the remaining 999 milliseconds (e.g. about 5 requests every millisecond), API Gateway processes all 10k requests in the one-second period without returning 429 Too Many Requests error responses.","title":"API Throttling"},{"location":"aws/api-gateway/#soap-webservice-passthrough","text":"You can configure API Gateway as a SOAP webservice passthrough.","title":"SOAP Webservice Passthrough"},{"location":"aws/api-gateway/#advanced-api-gateway-exam-tips","text":"Import API\u2019s using Swagger 2.0 definition files API Gateway can be throttled Default limits are 10k RPS or 5k concurrently You can configure API Gateway as a SOAP webservice passthrough","title":"Advanced API Gateway Exam Tips"},{"location":"aws/aws/","text":"Amazon Web Services The following section outlines notes taken during the AWS Certified Developer - Associate course run via Udemy.","title":"AWS"},{"location":"aws/aws/#amazon-web-services","text":"The following section outlines notes taken during the AWS Certified Developer - Associate course run via Udemy.","title":"Amazon Web Services"},{"location":"aws/cli/","text":"AWS CLI SSH/Putty into EC2 instance aws s3 ls aws configure [ copy paste details from programmatic IAM user ] aws s3 mb s3://experimenting-aws [ make bucket ] echo \"Hello world\" > hello.txt aws s3 cp ./hello.txt s3://experimenting-aws aws s3 ls s3://experimenting-aws AWS CLI reference: https://docs.aws.amazon.com/cli/latest/index.html Exam Tips: Least Privilege - always give your users the minimum amount of access required Create Groups - assign users to groups. Your users with auto inherit the permissions of the group - the groups permissions are assigned using policy documents. Secret Access Key - you will see this only once - if you do not save it you can delete the key pair (access key id and secret access key) and regenerate it. You will need to run aws configure again. Do not use just one access key - do not create just one access key and share that with all your developers. If someone leaves the company on bad terms, then you will need to delete the key and create a new one and every developer would then need to update their keys. Instead create on key pair per developer. You can use the CLI on your PC [Mac, Linux or Windows]","title":"CLI"},{"location":"aws/cli/#aws-cli","text":"","title":"AWS CLI"},{"location":"aws/cli/#sshputty-into-ec2-instance","text":"aws s3 ls aws configure [ copy paste details from programmatic IAM user ] aws s3 mb s3://experimenting-aws [ make bucket ] echo \"Hello world\" > hello.txt aws s3 cp ./hello.txt s3://experimenting-aws aws s3 ls s3://experimenting-aws AWS CLI reference: https://docs.aws.amazon.com/cli/latest/index.html","title":"SSH/Putty into EC2 instance"},{"location":"aws/cli/#exam-tips","text":"Least Privilege - always give your users the minimum amount of access required Create Groups - assign users to groups. Your users with auto inherit the permissions of the group - the groups permissions are assigned using policy documents. Secret Access Key - you will see this only once - if you do not save it you can delete the key pair (access key id and secret access key) and regenerate it. You will need to run aws configure again. Do not use just one access key - do not create just one access key and share that with all your developers. If someone leaves the company on bad terms, then you will need to delete the key and create a new one and every developer would then need to update their keys. Instead create on key pair per developer. You can use the CLI on your PC [Mac, Linux or Windows]","title":"Exam Tips:"},{"location":"aws/cloudfront/","text":"AWS CloudFront Amazon\u2019s Content Delivery Network. What is a CDN? A content delivery network (CDN) is a system of distributed servers (network) that deliver webpages and other web content to a user based on the geographic locations of the user, the origin of the webpage, and a content delivery server. CloudFront focused on Content Delivery. Transfer Acceleration is about Faster Uploads into S3. Edge locations are a collection of servers in a geographically dispersed data centers. These are used by CloudFront to keep a cache of your content. Instead of requesting data from your main server, its requested from the edge location which is geographically closer to the user. The edge location forwards request on to your main server, then caches them locally. Next time the user or another user requests the object, they get it directly from the edge location. This greatly reduces the load time of your content. After set period of time (TTL) the objects are automatically cleared form the edge location caches. You can clear objects in caches yourself i.e. if file is updated to ensure users get latest version but you will be charged. Key Terminology Edge Location - location where content is cached and can also be written. Separate to an AWS Region/AZ. More Edge Locations than Regions! Origin - This is the origin of all the files that the CDN will distribute. Origins can be an S3 Bucket, an EC2 Instance, an Elastic Load Balancer, or Route53 Distribution - This is the name given the CDN, which consists of a collection of Edge Locations. Web Distribution - Typically used for Websites RTMP - used for Media Streaming (Real Time Messaging Protocol) What is CloudFront? Amazon CloudFront can be used to deliver your entire website, including dynamic, static, streaming and interactive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance. e.g. Can be used to optimize performance for users accessing a website backed by S3. Amazons CloudFront is optimized to work with other AWS Service i.e. S3, EC2, ELB, Route53. CloudFront also works seamlessly with any non-AWS origin server, which stores the original, definitive versions of your files. CloudFront Distribution Types Web Distribution - Used for Websites, HTTP/HTTPS (can\u2019t serve adobe flash) RTMP Distribution - (Adobe Real Time Messaging Protocol) Used for Media Streaming / Flash multi-media content Transfer Acceleration Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your end users and an S3 bucket. Transfer Acceleration takes advantage of CloudFront\u2019s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. CloudFront Exam Tips Edge Location - where content will be cached. Separate to an AWS Region/AZ. Origin - origin of all files that CDN will distribute. May be S3, EC2, ELB, Route53 Distribution - name given the CDN, which consists of a collection of Edge locations Web Distribution - Typically used for websites RTMP - Used for Media Streaming Edge locations are not just READ only - you can WRITE to them too (i.e. PUIT an object on to them) CloudFront Edge Locations are utilised by S3 Transfer Acceleration to reduce latency for S3 uploads. Objects are cached for the life of the TTL (Time To Live) You can clear cached objects, but you will be charged. CloudFront Lab Options to know about when Creating CloudFront Distribution. Origin Settings Origin Domain Name (of S3/LoadBalancer/EC2/Route53) Origin Path (folders within origin) Restrict Bucket Access (enforce all access via CloudFront - unable to access via S3 -> also need Origin Access Identity) Grant Read Permissions on Bucket -> Yes, Update Bucket Policy (defaults to No) Default Cache Behaviour Settings Viewer Protocol Policy (can restrict redirect http -> https) Allowed HTTP Methods (GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE) Minimum TTL (min time in seconds to stay in CloudFront cache) Maximum TTL (365 days) Default TTL (24 hours - may be too short depending how frequently you update content) Restrict Viewer Access (Use Signed URLs or Signed Cookies -> useful for paid content restriction) Distribution Settings Web Application Firewalls (WAF) - Protects at Application Layer Can add your own Domain Name for CloudFront (CNAMEs) SSL Certificates (required for http -> https, can use custom cert) Supported HTTP Versions Enable IPv6 Takes Approx 15-20 minutes to setup CDN due to setup Edge Locations (around 100+ in 25 countries). In CDN General - Get Domain Name to access the CDN. In CDN Restrictions - you can maintain whitelist or blacklist for certain countries. In CDN Invalidations - removes objects from ClouldFront caches -> i.e. to manually remove objects from caches (charged fee).","title":"CloudFront"},{"location":"aws/cloudfront/#aws-cloudfront","text":"Amazon\u2019s Content Delivery Network.","title":"AWS CloudFront"},{"location":"aws/cloudfront/#what-is-a-cdn","text":"A content delivery network (CDN) is a system of distributed servers (network) that deliver webpages and other web content to a user based on the geographic locations of the user, the origin of the webpage, and a content delivery server. CloudFront focused on Content Delivery. Transfer Acceleration is about Faster Uploads into S3. Edge locations are a collection of servers in a geographically dispersed data centers. These are used by CloudFront to keep a cache of your content. Instead of requesting data from your main server, its requested from the edge location which is geographically closer to the user. The edge location forwards request on to your main server, then caches them locally. Next time the user or another user requests the object, they get it directly from the edge location. This greatly reduces the load time of your content. After set period of time (TTL) the objects are automatically cleared form the edge location caches. You can clear objects in caches yourself i.e. if file is updated to ensure users get latest version but you will be charged.","title":"What is a CDN?"},{"location":"aws/cloudfront/#key-terminology","text":"Edge Location - location where content is cached and can also be written. Separate to an AWS Region/AZ. More Edge Locations than Regions! Origin - This is the origin of all the files that the CDN will distribute. Origins can be an S3 Bucket, an EC2 Instance, an Elastic Load Balancer, or Route53 Distribution - This is the name given the CDN, which consists of a collection of Edge Locations. Web Distribution - Typically used for Websites RTMP - used for Media Streaming (Real Time Messaging Protocol)","title":"Key Terminology"},{"location":"aws/cloudfront/#what-is-cloudfront","text":"Amazon CloudFront can be used to deliver your entire website, including dynamic, static, streaming and interactive content using a global network of edge locations. Requests for your content are automatically routed to the nearest edge location, so content is delivered with the best possible performance. e.g. Can be used to optimize performance for users accessing a website backed by S3. Amazons CloudFront is optimized to work with other AWS Service i.e. S3, EC2, ELB, Route53. CloudFront also works seamlessly with any non-AWS origin server, which stores the original, definitive versions of your files.","title":"What is CloudFront?"},{"location":"aws/cloudfront/#cloudfront-distribution-types","text":"Web Distribution - Used for Websites, HTTP/HTTPS (can\u2019t serve adobe flash) RTMP Distribution - (Adobe Real Time Messaging Protocol) Used for Media Streaming / Flash multi-media content","title":"CloudFront Distribution Types"},{"location":"aws/cloudfront/#transfer-acceleration","text":"Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your end users and an S3 bucket. Transfer Acceleration takes advantage of CloudFront\u2019s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path.","title":"Transfer Acceleration"},{"location":"aws/cloudfront/#cloudfront-exam-tips","text":"Edge Location - where content will be cached. Separate to an AWS Region/AZ. Origin - origin of all files that CDN will distribute. May be S3, EC2, ELB, Route53 Distribution - name given the CDN, which consists of a collection of Edge locations Web Distribution - Typically used for websites RTMP - Used for Media Streaming Edge locations are not just READ only - you can WRITE to them too (i.e. PUIT an object on to them) CloudFront Edge Locations are utilised by S3 Transfer Acceleration to reduce latency for S3 uploads. Objects are cached for the life of the TTL (Time To Live) You can clear cached objects, but you will be charged.","title":"CloudFront Exam Tips"},{"location":"aws/cloudfront/#cloudfront-lab","text":"Options to know about when Creating CloudFront Distribution. Origin Settings Origin Domain Name (of S3/LoadBalancer/EC2/Route53) Origin Path (folders within origin) Restrict Bucket Access (enforce all access via CloudFront - unable to access via S3 -> also need Origin Access Identity) Grant Read Permissions on Bucket -> Yes, Update Bucket Policy (defaults to No) Default Cache Behaviour Settings Viewer Protocol Policy (can restrict redirect http -> https) Allowed HTTP Methods (GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE) Minimum TTL (min time in seconds to stay in CloudFront cache) Maximum TTL (365 days) Default TTL (24 hours - may be too short depending how frequently you update content) Restrict Viewer Access (Use Signed URLs or Signed Cookies -> useful for paid content restriction) Distribution Settings Web Application Firewalls (WAF) - Protects at Application Layer Can add your own Domain Name for CloudFront (CNAMEs) SSL Certificates (required for http -> https, can use custom cert) Supported HTTP Versions Enable IPv6 Takes Approx 15-20 minutes to setup CDN due to setup Edge Locations (around 100+ in 25 countries). In CDN General - Get Domain Name to access the CDN. In CDN Restrictions - you can maintain whitelist or blacklist for certain countries. In CDN Invalidations - removes objects from ClouldFront caches -> i.e. to manually remove objects from caches (charged fee).","title":"CloudFront Lab"},{"location":"aws/cloudwatch/","text":"CloudWatch Amazon CloudWatch is a monitoring service to monitor your AWS resources, as well as the applications that you run on AWS. It can monitor things like: Compute Autoscaling Groups Elastic Load Balancers Route53 Health Checks Storage & Content Delivery: EBS Volumes Storage Gateways CloudFront Database & Analytics DynamoDB Elasticache Nodes RDS Instances Elastic MapReduce Job Flows Redshift Other SNS Topics SQS Queues Opsworks CloudWatch Logs Estimated Charges on AWS Bill CloudWatch and EC2 Host Level Metrics Consist of: CPU Network Disk Status Check Exam Tip: RAM Utilization is a custom metrics. By Default EC2 monitoring is 5 minute intervals, unless you enable detailed monitoring which will then make it 1 minute intervals. How long are the Metrics Stored? You can retrieve data using the GetMetricStatics API or by using third party tools offered by AWS partners. You can store your log data in CloudWatch Logs for as long as you want. By default, CloudWatch Logs will store your log data indefinitely. You can change the retention for each Log Group at any time. You can retrieve data from any terminated EC2 or ELB instance after its termination. Metric Granularity It depends on the AWS service. Many default metrics for many default services are 1 minute, but it can be 3 or 5 minutes depending on the service. Exam Tip: for custom metrics the minimum granularity that you can have is 1 minute. CloudWatch Alarms You can create an alarm to monitor any Amazon CloudWatch metric in your account. This can include EC2 CPU Utilization, Elastic Load Balancer Latency or even the charges on your AWS bill. You can set the appropriate thresholds in which to trigger the alarms and also set what actions should be taken if an alarm state is reached. Note: CloudWatch can be used on premise - Not restricted to just AWS resources. Can be on premise too. Just need to download and install the SSM agent and Cloudwatch agent. Note: Detailed Monitoring - 1 Minute; Standard Monitoring - 5 Minutes. CloudWatch Lab Monitoring EC2 with Custom Metrics IAM -> Roles -> Create Role -> Service use Role: EC2 -> Apply \u2018CloudWatchFullAccess\u2019 Policy EC2 -> Launch Instance -> Amazon Linux 2 AMI -> Defaults (Add custom IAM roll) -> Bootstrap Script: (Generate Security Group & KeyPair as usual\u2026) #!/bin/bash yum update -y sudo yum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64 cd /home/ec2-user/ curl https://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip -O unzip CloudWatchMonitoringScripts-1.2.2.zip rm -rf CloudWatchMonitoringScripts-1.2.2.zip Get started with sshing into EC2 chmod 400 myKeyPair.pem ssh ec2-user@public-ip -i myKeyPair.pem yes sudo su ls (should have aws-scripts-mon) cd aws-scripts-mon ls Access CloudWatch from AWS UI. Click \u2018Browse Metrics\u2019 -> EC2 -> Per Instance Metrics (default metrics) /home/ec2-user/aws-scripts-mon/mon-put-instance-data.pl --mem-util --verify --verbose ('verifciation completed successfully. Not actual metrics were sent to CloudWatch') /home/ec2-user/aws-scripts-mon/mon-put-instance-data.pl --mem-util --mem-used --mem-avail ('Successfully reported metrics to CloudWatch.') Can take up to 5 minutes\u2026 but back to AWS UI -> CloudWatch should see \u2018Customer Namespaces\u2019 -> Linux System. cd /etc nano crontab # add the cron below & save # pushing this memory data every 1 minute */1 * * * * root /home/ec2-user/aws-scripts-mon/mon-put-instance-data.pl --mem-util --mem-used --mem-avail Wait up to 20 minutes\u2026 see data populate in CloudWatch. As detailed monitoring not turned on -> only have data points every 5 minutes. Exam Tips Host Level Metrics Consist of: CPU Network Disk (I/O - not storage) Status Check Ram Utilization -is a custom metric Customer Metrics - minimum granularity is 1 minute (will require detailed monitoring turned on) CloudWatch -v- CloudTrail -v- Config CloudWatch monitor performance CloudTrail monitors API calls in the AWS platform AWS Config records the state of your AWS environment and can notify you of changes (\u2018CCTV\u2019)","title":"CloudWatch"},{"location":"aws/cloudwatch/#cloudwatch","text":"Amazon CloudWatch is a monitoring service to monitor your AWS resources, as well as the applications that you run on AWS. It can monitor things like: Compute Autoscaling Groups Elastic Load Balancers Route53 Health Checks Storage & Content Delivery: EBS Volumes Storage Gateways CloudFront Database & Analytics DynamoDB Elasticache Nodes RDS Instances Elastic MapReduce Job Flows Redshift Other SNS Topics SQS Queues Opsworks CloudWatch Logs Estimated Charges on AWS Bill","title":"CloudWatch"},{"location":"aws/cloudwatch/#cloudwatch-and-ec2","text":"Host Level Metrics Consist of: CPU Network Disk Status Check Exam Tip: RAM Utilization is a custom metrics. By Default EC2 monitoring is 5 minute intervals, unless you enable detailed monitoring which will then make it 1 minute intervals. How long are the Metrics Stored? You can retrieve data using the GetMetricStatics API or by using third party tools offered by AWS partners. You can store your log data in CloudWatch Logs for as long as you want. By default, CloudWatch Logs will store your log data indefinitely. You can change the retention for each Log Group at any time. You can retrieve data from any terminated EC2 or ELB instance after its termination.","title":"CloudWatch and EC2"},{"location":"aws/cloudwatch/#metric-granularity","text":"It depends on the AWS service. Many default metrics for many default services are 1 minute, but it can be 3 or 5 minutes depending on the service. Exam Tip: for custom metrics the minimum granularity that you can have is 1 minute.","title":"Metric Granularity"},{"location":"aws/cloudwatch/#cloudwatch-alarms","text":"You can create an alarm to monitor any Amazon CloudWatch metric in your account. This can include EC2 CPU Utilization, Elastic Load Balancer Latency or even the charges on your AWS bill. You can set the appropriate thresholds in which to trigger the alarms and also set what actions should be taken if an alarm state is reached. Note: CloudWatch can be used on premise - Not restricted to just AWS resources. Can be on premise too. Just need to download and install the SSM agent and Cloudwatch agent. Note: Detailed Monitoring - 1 Minute; Standard Monitoring - 5 Minutes.","title":"CloudWatch Alarms"},{"location":"aws/cloudwatch/#cloudwatch-lab","text":"","title":"CloudWatch Lab"},{"location":"aws/cloudwatch/#monitoring-ec2-with-custom-metrics","text":"IAM -> Roles -> Create Role -> Service use Role: EC2 -> Apply \u2018CloudWatchFullAccess\u2019 Policy EC2 -> Launch Instance -> Amazon Linux 2 AMI -> Defaults (Add custom IAM roll) -> Bootstrap Script: (Generate Security Group & KeyPair as usual\u2026) #!/bin/bash yum update -y sudo yum install -y perl-Switch perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https perl-Digest-SHA.x86_64 cd /home/ec2-user/ curl https://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.2.zip -O unzip CloudWatchMonitoringScripts-1.2.2.zip rm -rf CloudWatchMonitoringScripts-1.2.2.zip Get started with sshing into EC2 chmod 400 myKeyPair.pem ssh ec2-user@public-ip -i myKeyPair.pem yes sudo su ls (should have aws-scripts-mon) cd aws-scripts-mon ls Access CloudWatch from AWS UI. Click \u2018Browse Metrics\u2019 -> EC2 -> Per Instance Metrics (default metrics) /home/ec2-user/aws-scripts-mon/mon-put-instance-data.pl --mem-util --verify --verbose ('verifciation completed successfully. Not actual metrics were sent to CloudWatch') /home/ec2-user/aws-scripts-mon/mon-put-instance-data.pl --mem-util --mem-used --mem-avail ('Successfully reported metrics to CloudWatch.') Can take up to 5 minutes\u2026 but back to AWS UI -> CloudWatch should see \u2018Customer Namespaces\u2019 -> Linux System. cd /etc nano crontab # add the cron below & save # pushing this memory data every 1 minute */1 * * * * root /home/ec2-user/aws-scripts-mon/mon-put-instance-data.pl --mem-util --mem-used --mem-avail Wait up to 20 minutes\u2026 see data populate in CloudWatch. As detailed monitoring not turned on -> only have data points every 5 minutes.","title":"Monitoring EC2 with Custom Metrics"},{"location":"aws/cloudwatch/#exam-tips","text":"Host Level Metrics Consist of: CPU Network Disk (I/O - not storage) Status Check Ram Utilization -is a custom metric Customer Metrics - minimum granularity is 1 minute (will require detailed monitoring turned on)","title":"Exam Tips"},{"location":"aws/cloudwatch/#cloudwatch-v-cloudtrail-v-config","text":"CloudWatch monitor performance CloudTrail monitors API calls in the AWS platform AWS Config records the state of your AWS environment and can notify you of changes (\u2018CCTV\u2019)","title":"CloudWatch -v- CloudTrail -v- Config"},{"location":"aws/dax/","text":"DAX (Dynamo DB Accelerator) A fully managed, clustered in-memory cache for DynamoDB Delivers up to 10x read performance improvement Microsecond performance for millions of requests per second * Ideal for Read-Heavy and bursty workloads e.g. auction applications, gaming and retail sites during black Friday promotions How DAX Works Dax is a write-through caching service - data is written to the Cache as well as the back end store at the same time Allows you to point your DynamoDB API calls at the DAX cluster If the item you are querying is in the cache (cache hit), DAX returns the result to the application If the item is not available (cache miss) then DAX performs an Eventually Consistent GetItem operation against DynamoDB Retrieval of data from DAX reduces the read load on DynamoDB tables May be able to reduce Provisioned Read Capacity What DAX is not suitable for Caters for Eventually Consistent reads only - no not suitable for applications that require Strongly Consistent Reads Write intensive applications Applications that do not perform many read operations Applications that do not require microsecond response times Exam Tips - DAX Provides in-memory caching for DynamoDB tables Improves response times for Eventually Consistent reads only You point your API calls to the DAX cluster instead of your table If item you are querying is on the cache, DAX with return it; otherwise it will perform an Eventually Consistent GetItem operation to your DynamoDB table Not suitable for write-intensive applications or applications that require Strongly Consistent Reads","title":"Dax"},{"location":"aws/dax/#dax-dynamo-db-accelerator","text":"A fully managed, clustered in-memory cache for DynamoDB Delivers up to 10x read performance improvement Microsecond performance for millions of requests per second * Ideal for Read-Heavy and bursty workloads e.g. auction applications, gaming and retail sites during black Friday promotions","title":"DAX (Dynamo DB Accelerator)"},{"location":"aws/dax/#how-dax-works","text":"Dax is a write-through caching service - data is written to the Cache as well as the back end store at the same time Allows you to point your DynamoDB API calls at the DAX cluster If the item you are querying is in the cache (cache hit), DAX returns the result to the application If the item is not available (cache miss) then DAX performs an Eventually Consistent GetItem operation against DynamoDB Retrieval of data from DAX reduces the read load on DynamoDB tables May be able to reduce Provisioned Read Capacity","title":"How DAX Works"},{"location":"aws/dax/#what-dax-is-not-suitable-for","text":"Caters for Eventually Consistent reads only - no not suitable for applications that require Strongly Consistent Reads Write intensive applications Applications that do not perform many read operations Applications that do not require microsecond response times","title":"What DAX is not suitable for"},{"location":"aws/dax/#exam-tips-dax","text":"Provides in-memory caching for DynamoDB tables Improves response times for Eventually Consistent reads only You point your API calls to the DAX cluster instead of your table If item you are querying is on the cache, DAX with return it; otherwise it will perform an Eventually Consistent GetItem operation to your DynamoDB table Not suitable for write-intensive applications or applications that require Strongly Consistent Reads","title":"Exam Tips - DAX"},{"location":"aws/dynamo/","text":"Dynamo DB A fast and flexible noSQL DB for applications that need consistent, single-digit millisecond latency at any scale. Fully managed DB, can be configured to autoscale, integrates well with Lambda. Supports both document and key-value data models Flexible data model and reliable performance. Stored on SSD Spread across 3 geographically distinct dat centres Choice of 2 consistency models: Eventual Consistent Reads (Default) Strongly Consistent Reads Reads Eventually Consistent Reads Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance) Strongly Consistent Reads A strongly consistent read returns a result that reflects all writes that received a successful response prior to the end Structure Tables Items Attributes Supports key-value and document data structures Key = Name of the data, Value = Data itself Documents can be written in JSON, HTML or XML Primary Keys Dynamo DB Stores and retrieves data based on Primary Key - 2 Types: Partition Key - unique attribute (e.g. user ID): Value of partition key is input to an internal hash function which determines the partition or physical location on which the data is Stored If you are using the partition key as your primary key, then no two items can have the same partition key. Composite Key (Partition key + Sort Key) in combination: Primary key would be a composite key consisting of Partition Key - User ID Sort Key - Timestamp of Post 2 items may have the same Partition key, but they must have a different sort key. All items with the same Partition Key are stored together, then sorted according to the Sort key Value Allows you to store multiple items with the same Partition Key Access Control Authentication and Access Control is managed using AWS IAM You can create an IAM user within your AWS account which has specific permissions to access and create DynamoDB tables. You can create an IAM role which enables you to obtain temporary access keys which can be used to access DynamoDB You can also use a special IAM Condition to restrict user access to only their own records IAM Conditions Example Imagine a mobile gaming app with millions of users: Users need to access high scores for each game they are playing Access must be restricted to ensure they cannot view anyone else\u2019s data This can be done by adding a Condition to an IAM Policy to allow access only to items where the Partition Key value matches their User ID. Exam Tips - Dynamo DB DynamoDB is a low latency noSQL DB Consists of Tables Items and Attributes Supports both document and key-value data models Supported document formats are JSON, HTML, XML 2 types of Primary key - Partition Key and combination of Partition Key + Sort Key (Composite Key) 2 consistency models: Strongly Consistent / Eventually Consistent Access is controlled using IAM policies Fine grained access control using IAM Condition parameter: dynamodb:LeadingKeys to allow users to access only the items where the partition key value matches their user ID Useful Example Reference with PHP related scripts. Sample CLI Commands aws dynamodb get-item --table-name ProductCatalog --key '{\"Id\": {\"N\":\"205\"}}' Index In SQL DBs an index is a data structure which allows you to perform fast queries on specific columns in a table. You select the columns that you want included in the index and run your searches on the index - rather than the entire dataset. Dynamo DB has two types of Index (even though its NoSQL): 1. Local Secondary Index 2. Global Secondary Index Local Secondary Index Can only be created when you are creating your table You cannot add, remove or modify it later It has the same partition key as your original table But a different Sort Key Gives you a different view of your data, organized according to an alternative Sort Key Any queries based on this Sort Key are much faster using the index than the main table e.g. Partition Key: User ID, Sort Key: account creation date Global Secondary Index You can create when you create your table, or add it later Different Partition Key as well as Sort Key Gives a completely different view of the data Speeds up any queries related to this alternative partition and sort key e.g. Partition Key: email address, Sort Key: last log-in date Exam Tips - Indexes Enable fast queries on specific data columns Give you a different view of your data, based on alternative Partition / Sort Keys Important to understand difference Local Secondary Index Global Secondary Index Must be created at same time table is created Can create at any time (including table creation) Same Partition Key as your Table Different Partition Key Different Sort Key Different Sort Key Query & Scan Query A query operation finds item in a table based on the Primary Key attribute and a distinct value to search for e.g. select and item where the user ID is equal to 212, will select all attributes for that name e.g. first name, surname, email etc. Use an optional Sort Key name and value to refine the results e.g. if Sort Key is a timestamp, you can refine query to only select items from last 7 days By default a query returns all the attributes for the items but you can use the ProjectionExpression parameter if you want the query to only return the specific attributes you want e.g. if you only want to see the email address rather than all the attributes Results are always sorted by the Sort Key Numeric order - by default in ascending order (1,2,3,4) ASCII character code values You can reverse the order by setting the ScanIndexForward parameter to false - (not this param is only related to queries no scan) By default, Queries are Eventually Consistent You need to explicitly set the query to be Strongly Consistent Scan A scan operation examines every item in the table. By default returns all data Attributes Use the ProjectionExpression parameter to refine the scan to return only the attributes you want Query or Scan? Query is more efficient than a Scan Scan dumps the entire table, then filters out the values to provide the desired result - removing the unwanted data This adds an extra step of removing the data you don\u2019t want As the table grows, the scan operation takes longer Scan operation on a larger table can use up the provisioned throughput for a large table in just a single operation How to Improve Performance You can reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations e.g. set the page size to return 40 Items Larger number of smaller operations will allow other requests to succeed without throttling Avoid using scan operations if you can: design tables in a way that you can use Query, Get, or BatchGetItem APIs By default, a scan operation processes data sequentially in returning 1MB increments before moving on to retrieve the next 1MB of data. It can only scan one partition at a time. You can configure DynamoDB to use Parallel scans instead by logically dividing a table or index into segments and scanning each segment in parallel Best to avoid parallel scans if your table or index is already incurring heavy read / write activity from other applications Exam Tips - Scan -v- Query A Query operation finds items in a table using only the Primary Key attribute -> You provide the primary key and a distinct value to search for A scan operation examines every item in the table -> By default returns all data attributes Use the ProjectionExpression parameter to refine the results Query results always sorted by Sort Key (if there is one) Sorted in ascending order Set ScanIndexForward parameter to false to reverse the order - queries only Query operation is generally more efficient than a Scan Reduce the impact of a query of scan by setting a smaller page size which uses fewer read operations Isolate scan operations to specific tables and segregate them from your mission-critical traffic Try Parallel scans, rather than the default sequential scan Avoid using scan operations if you can: design tables in a way that you can use the Query, Get, or BatchGetItem APIs Provisioned Throughput Dynamo DB Provisioned Throughput is measure in Capacity Units. When you create your table, you specify your requirements in terms of Read Capacity Units and Write Capacity Units 1x Write Capacity Unit = 1x 1KB write per second 1x Read Capacity Unit = 1x Strongly Consistent Read of 4KB per second OR 2x Eventually ConsistentReads of 4KB per second (Default) Strongly Consistent Reads Calculation Your application needs to read 80 items (table rows) per second. Each item is 3KB in size. You need Strongly consistent reads. First: Calculate how many Read Capacity Units needed for each read: size of each item / 4KB i.e. 3KB/4Kb = 0.75. Rounded up to the nearest whole number, each read will need 1x Read Capacity Unit per read operation. Multiplied by the number of reads per second = 80 Read Capacity Units required Eventually Consistent Reads Calculation Same as above BUT 2x 4KB reads per second - double the throughput of Strongly Consistent Reads 3KB/4KB = 0.75, round to nearest whole number = 1 Multiply by number of reads per second = 80 Divided 80 by 2, only need 40 read capacity units for Eventually Consistent Reads Write Capacity Unit Calculation You want to write 100 items per second. Each item 512 bytes in size First: Calculate how many Capacity Units for each write: Size of each item /1KB (for Write CU) 512 bytes / 1KB = 0.5 Round to nearest whole number = 1 Write Capacity Unit per write operation operation Multiplied by number of writes per second = 100 Write Capacity Units required Exam Tips - Provisioned throughput Measured in Capacity Units 1x Write Capacity Unit = 1x 1KB Write per Second 1x Read Capacity Unit = 1x 4KB Strongly Consistent Read OR 2x 4KB Eventually Consistent Read On Demand Capacity Charges apply for: Reading, Writing, Storing data Don\u2019t need to specify your requirements DynamoDB instantly scales up and down based on the activity of your application Great for unpredictable workloads You want to pay for only what you use (pay per request) Which Pricing Model to Use On-Demand Capacity Provisioned Capacity Unknown Workloads Forecast read & write capacity requirements Unpredictable Application Traffic Predictable Application Traffic Want Pay-Per-Use Model App traffic is consistent or increases gradually Spikey/Short-lived Projects DynamoDB Transactions ACID Transactions (Atomic, Consistent, Isolated, Durable) Read or write multiple items across multiple tables as an all or nothing operations Check for a pre-requisite condition before writing to a table DynamoDB TTL TTL attribute defines an expiry time for your data Expired items marked for deletion Great for removing irrelevant or old data: Session data Event logs Temporary data Reduces cost by automatically removing data which is no longer relevant TTL expressed as epoch time i.e. when current time > TTL item expired and marked for deletion Sample Commands # ensure you have right IAM role & access permissions aws iam get-user # create sessiondata table aws dynamodb create-table --table-name SessionData --attribute-defintiions \\ AttributeName = UserID,AttributeType = N --key-schema \\ AttributeName = UserID,KeyType = HASH \\ --provisioned-throughput ReadCapacityUnits = 5 ,WriteCapacityUnits = 5 # populate SessionData table aws dynamodb bach-write-item --request-items file://items.json DynamoDB Streams Time-ordered sequence of item level modifications (insert, update, delete) Logs are encrypted at rest and stored for 24hrs Accessed using a dedicated endpoint By default the Primary Key is recorded Before and After images can be captured Processing Streams Events are recorded in near real-time Apps can take actions based on contents of stream Event source for Lambda Lambda polls the DynamoDB Stream Executes Lambda code based on a DynamoDB Streams event Exam Tips - DynamoDB Streams Time-ordered sequence of item level modifications in your DynamoDB Tables Data is stored for 24 hours only Can be used as an event source for Lambda so you can create applications which take actions based on events in your DynamoDB table Provisioned Throughput Exceeded & Exponential Backoff ProvisionedThroughputExceededException Your request rate is too high for the read/write capacity provisioned on your DynamoDB table SDK will automatically retries the requests until successful If you are not using the SDK you can: Reduce request frequency Use Exponential Backoff Exponential Backoff Many components in a network can generate errors due to being overloaded In addition to simple retries all AWS SDKs use Exponential Backoff Progressively longer waits between consecutive retries e.g. 50ms, 100ms, 200ms for improved flow control If after 1 minute this doesn\u2019t work, your request size may be exceeding the throughput for your read/write capacity Exam Tips - Provisioned Throughput & Exponential Backoff If you see a ProvisionedThoguhputExceeded Error, this means the number of requests is too high Exponential Backoff improves flow by retrying requests using progressively longer waits This is not just true for DynamoDB, Exponential Backoff is a feature of every AWS SDK and applies to many services within AWS e.g. S#, CloudFormation, SES","title":"Dynamo DB"},{"location":"aws/dynamo/#dynamo-db","text":"A fast and flexible noSQL DB for applications that need consistent, single-digit millisecond latency at any scale. Fully managed DB, can be configured to autoscale, integrates well with Lambda. Supports both document and key-value data models Flexible data model and reliable performance. Stored on SSD Spread across 3 geographically distinct dat centres Choice of 2 consistency models: Eventual Consistent Reads (Default) Strongly Consistent Reads","title":"Dynamo DB"},{"location":"aws/dynamo/#reads","text":"","title":"Reads"},{"location":"aws/dynamo/#eventually-consistent-reads","text":"Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance)","title":"Eventually Consistent Reads"},{"location":"aws/dynamo/#strongly-consistent-reads","text":"A strongly consistent read returns a result that reflects all writes that received a successful response prior to the end","title":"Strongly Consistent Reads"},{"location":"aws/dynamo/#structure","text":"Tables Items Attributes Supports key-value and document data structures Key = Name of the data, Value = Data itself Documents can be written in JSON, HTML or XML","title":"Structure"},{"location":"aws/dynamo/#primary-keys","text":"Dynamo DB Stores and retrieves data based on Primary Key - 2 Types: Partition Key - unique attribute (e.g. user ID): Value of partition key is input to an internal hash function which determines the partition or physical location on which the data is Stored If you are using the partition key as your primary key, then no two items can have the same partition key. Composite Key (Partition key + Sort Key) in combination: Primary key would be a composite key consisting of Partition Key - User ID Sort Key - Timestamp of Post 2 items may have the same Partition key, but they must have a different sort key. All items with the same Partition Key are stored together, then sorted according to the Sort key Value Allows you to store multiple items with the same Partition Key","title":"Primary Keys"},{"location":"aws/dynamo/#access-control","text":"Authentication and Access Control is managed using AWS IAM You can create an IAM user within your AWS account which has specific permissions to access and create DynamoDB tables. You can create an IAM role which enables you to obtain temporary access keys which can be used to access DynamoDB You can also use a special IAM Condition to restrict user access to only their own records","title":"Access Control"},{"location":"aws/dynamo/#iam-conditions-example","text":"Imagine a mobile gaming app with millions of users: Users need to access high scores for each game they are playing Access must be restricted to ensure they cannot view anyone else\u2019s data This can be done by adding a Condition to an IAM Policy to allow access only to items where the Partition Key value matches their User ID.","title":"IAM Conditions Example"},{"location":"aws/dynamo/#exam-tips-dynamo-db","text":"DynamoDB is a low latency noSQL DB Consists of Tables Items and Attributes Supports both document and key-value data models Supported document formats are JSON, HTML, XML 2 types of Primary key - Partition Key and combination of Partition Key + Sort Key (Composite Key) 2 consistency models: Strongly Consistent / Eventually Consistent Access is controlled using IAM policies Fine grained access control using IAM Condition parameter: dynamodb:LeadingKeys to allow users to access only the items where the partition key value matches their user ID Useful Example Reference with PHP related scripts.","title":"Exam Tips - Dynamo DB"},{"location":"aws/dynamo/#sample-cli-commands","text":"aws dynamodb get-item --table-name ProductCatalog --key '{\"Id\": {\"N\":\"205\"}}'","title":"Sample CLI Commands"},{"location":"aws/dynamo/#index","text":"In SQL DBs an index is a data structure which allows you to perform fast queries on specific columns in a table. You select the columns that you want included in the index and run your searches on the index - rather than the entire dataset. Dynamo DB has two types of Index (even though its NoSQL): 1. Local Secondary Index 2. Global Secondary Index","title":"Index"},{"location":"aws/dynamo/#local-secondary-index","text":"Can only be created when you are creating your table You cannot add, remove or modify it later It has the same partition key as your original table But a different Sort Key Gives you a different view of your data, organized according to an alternative Sort Key Any queries based on this Sort Key are much faster using the index than the main table e.g. Partition Key: User ID, Sort Key: account creation date","title":"Local Secondary Index"},{"location":"aws/dynamo/#global-secondary-index","text":"You can create when you create your table, or add it later Different Partition Key as well as Sort Key Gives a completely different view of the data Speeds up any queries related to this alternative partition and sort key e.g. Partition Key: email address, Sort Key: last log-in date","title":"Global Secondary Index"},{"location":"aws/dynamo/#exam-tips-indexes","text":"Enable fast queries on specific data columns Give you a different view of your data, based on alternative Partition / Sort Keys Important to understand difference Local Secondary Index Global Secondary Index Must be created at same time table is created Can create at any time (including table creation) Same Partition Key as your Table Different Partition Key Different Sort Key Different Sort Key","title":"Exam Tips - Indexes"},{"location":"aws/dynamo/#query-scan","text":"","title":"Query &amp; Scan"},{"location":"aws/dynamo/#query","text":"A query operation finds item in a table based on the Primary Key attribute and a distinct value to search for e.g. select and item where the user ID is equal to 212, will select all attributes for that name e.g. first name, surname, email etc. Use an optional Sort Key name and value to refine the results e.g. if Sort Key is a timestamp, you can refine query to only select items from last 7 days By default a query returns all the attributes for the items but you can use the ProjectionExpression parameter if you want the query to only return the specific attributes you want e.g. if you only want to see the email address rather than all the attributes Results are always sorted by the Sort Key Numeric order - by default in ascending order (1,2,3,4) ASCII character code values You can reverse the order by setting the ScanIndexForward parameter to false - (not this param is only related to queries no scan) By default, Queries are Eventually Consistent You need to explicitly set the query to be Strongly Consistent","title":"Query"},{"location":"aws/dynamo/#scan","text":"A scan operation examines every item in the table. By default returns all data Attributes Use the ProjectionExpression parameter to refine the scan to return only the attributes you want","title":"Scan"},{"location":"aws/dynamo/#query-or-scan","text":"Query is more efficient than a Scan Scan dumps the entire table, then filters out the values to provide the desired result - removing the unwanted data This adds an extra step of removing the data you don\u2019t want As the table grows, the scan operation takes longer Scan operation on a larger table can use up the provisioned throughput for a large table in just a single operation","title":"Query or Scan?"},{"location":"aws/dynamo/#how-to-improve-performance","text":"You can reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations e.g. set the page size to return 40 Items Larger number of smaller operations will allow other requests to succeed without throttling Avoid using scan operations if you can: design tables in a way that you can use Query, Get, or BatchGetItem APIs By default, a scan operation processes data sequentially in returning 1MB increments before moving on to retrieve the next 1MB of data. It can only scan one partition at a time. You can configure DynamoDB to use Parallel scans instead by logically dividing a table or index into segments and scanning each segment in parallel Best to avoid parallel scans if your table or index is already incurring heavy read / write activity from other applications","title":"How to Improve Performance"},{"location":"aws/dynamo/#exam-tips-scan-v-query","text":"A Query operation finds items in a table using only the Primary Key attribute -> You provide the primary key and a distinct value to search for A scan operation examines every item in the table -> By default returns all data attributes Use the ProjectionExpression parameter to refine the results Query results always sorted by Sort Key (if there is one) Sorted in ascending order Set ScanIndexForward parameter to false to reverse the order - queries only Query operation is generally more efficient than a Scan Reduce the impact of a query of scan by setting a smaller page size which uses fewer read operations Isolate scan operations to specific tables and segregate them from your mission-critical traffic Try Parallel scans, rather than the default sequential scan Avoid using scan operations if you can: design tables in a way that you can use the Query, Get, or BatchGetItem APIs","title":"Exam Tips - Scan -v- Query"},{"location":"aws/dynamo/#provisioned-throughput","text":"Dynamo DB Provisioned Throughput is measure in Capacity Units. When you create your table, you specify your requirements in terms of Read Capacity Units and Write Capacity Units 1x Write Capacity Unit = 1x 1KB write per second 1x Read Capacity Unit = 1x Strongly Consistent Read of 4KB per second OR 2x Eventually ConsistentReads of 4KB per second (Default)","title":"Provisioned Throughput"},{"location":"aws/dynamo/#strongly-consistent-reads-calculation","text":"Your application needs to read 80 items (table rows) per second. Each item is 3KB in size. You need Strongly consistent reads. First: Calculate how many Read Capacity Units needed for each read: size of each item / 4KB i.e. 3KB/4Kb = 0.75. Rounded up to the nearest whole number, each read will need 1x Read Capacity Unit per read operation. Multiplied by the number of reads per second = 80 Read Capacity Units required","title":"Strongly Consistent Reads Calculation"},{"location":"aws/dynamo/#eventually-consistent-reads-calculation","text":"Same as above BUT 2x 4KB reads per second - double the throughput of Strongly Consistent Reads 3KB/4KB = 0.75, round to nearest whole number = 1 Multiply by number of reads per second = 80 Divided 80 by 2, only need 40 read capacity units for Eventually Consistent Reads","title":"Eventually Consistent Reads Calculation"},{"location":"aws/dynamo/#write-capacity-unit-calculation","text":"You want to write 100 items per second. Each item 512 bytes in size First: Calculate how many Capacity Units for each write: Size of each item /1KB (for Write CU) 512 bytes / 1KB = 0.5 Round to nearest whole number = 1 Write Capacity Unit per write operation operation Multiplied by number of writes per second = 100 Write Capacity Units required","title":"Write Capacity Unit Calculation"},{"location":"aws/dynamo/#exam-tips-provisioned-throughput","text":"Measured in Capacity Units 1x Write Capacity Unit = 1x 1KB Write per Second 1x Read Capacity Unit = 1x 4KB Strongly Consistent Read OR 2x 4KB Eventually Consistent Read","title":"Exam Tips - Provisioned throughput"},{"location":"aws/dynamo/#on-demand-capacity","text":"Charges apply for: Reading, Writing, Storing data Don\u2019t need to specify your requirements DynamoDB instantly scales up and down based on the activity of your application Great for unpredictable workloads You want to pay for only what you use (pay per request)","title":"On Demand Capacity"},{"location":"aws/dynamo/#which-pricing-model-to-use","text":"On-Demand Capacity Provisioned Capacity Unknown Workloads Forecast read & write capacity requirements Unpredictable Application Traffic Predictable Application Traffic Want Pay-Per-Use Model App traffic is consistent or increases gradually Spikey/Short-lived Projects","title":"Which Pricing Model to Use"},{"location":"aws/dynamo/#dynamodb-transactions","text":"ACID Transactions (Atomic, Consistent, Isolated, Durable) Read or write multiple items across multiple tables as an all or nothing operations Check for a pre-requisite condition before writing to a table","title":"DynamoDB Transactions"},{"location":"aws/dynamo/#dynamodb-ttl","text":"TTL attribute defines an expiry time for your data Expired items marked for deletion Great for removing irrelevant or old data: Session data Event logs Temporary data Reduces cost by automatically removing data which is no longer relevant TTL expressed as epoch time i.e. when current time > TTL item expired and marked for deletion","title":"DynamoDB TTL"},{"location":"aws/dynamo/#sample-commands","text":"# ensure you have right IAM role & access permissions aws iam get-user # create sessiondata table aws dynamodb create-table --table-name SessionData --attribute-defintiions \\ AttributeName = UserID,AttributeType = N --key-schema \\ AttributeName = UserID,KeyType = HASH \\ --provisioned-throughput ReadCapacityUnits = 5 ,WriteCapacityUnits = 5 # populate SessionData table aws dynamodb bach-write-item --request-items file://items.json","title":"Sample Commands"},{"location":"aws/dynamo/#dynamodb-streams","text":"Time-ordered sequence of item level modifications (insert, update, delete) Logs are encrypted at rest and stored for 24hrs Accessed using a dedicated endpoint By default the Primary Key is recorded Before and After images can be captured","title":"DynamoDB Streams"},{"location":"aws/dynamo/#processing-streams","text":"Events are recorded in near real-time Apps can take actions based on contents of stream Event source for Lambda Lambda polls the DynamoDB Stream Executes Lambda code based on a DynamoDB Streams event","title":"Processing Streams"},{"location":"aws/dynamo/#exam-tips-dynamodb-streams","text":"Time-ordered sequence of item level modifications in your DynamoDB Tables Data is stored for 24 hours only Can be used as an event source for Lambda so you can create applications which take actions based on events in your DynamoDB table","title":"Exam Tips - DynamoDB Streams"},{"location":"aws/dynamo/#provisioned-throughput-exceeded-exponential-backoff","text":"ProvisionedThroughputExceededException Your request rate is too high for the read/write capacity provisioned on your DynamoDB table SDK will automatically retries the requests until successful If you are not using the SDK you can: Reduce request frequency Use Exponential Backoff","title":"Provisioned Throughput Exceeded &amp; Exponential Backoff"},{"location":"aws/dynamo/#exponential-backoff","text":"Many components in a network can generate errors due to being overloaded In addition to simple retries all AWS SDKs use Exponential Backoff Progressively longer waits between consecutive retries e.g. 50ms, 100ms, 200ms for improved flow control If after 1 minute this doesn\u2019t work, your request size may be exceeding the throughput for your read/write capacity","title":"Exponential Backoff"},{"location":"aws/dynamo/#exam-tips-provisioned-throughput-exponential-backoff","text":"If you see a ProvisionedThoguhputExceeded Error, this means the number of requests is too high Exponential Backoff improves flow by retrying requests using progressively longer waits This is not just true for DynamoDB, Exponential Backoff is a feature of every AWS SDK and applies to many services within AWS e.g. S#, CloudFormation, SES","title":"Exam Tips - Provisioned Throughput &amp; Exponential Backoff"},{"location":"aws/ec2/","text":"AWS EC2 EC2 Types On Demand - pay fixed rate by hour (or second) with no commitment Reserved - provides capacity reservation with significant discount on hourly charge - 1 Year or 3 Year terms Spot - enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexibile start and end times Dedicated Hosts - physical EC2 server dedicated for your use. Can help reduce costs by allowing you to use existing server-bound software licenses On Spot Instances : If they are terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged forthe complete hour in which the instance ran. EC2 Instance Types FIGHT DR MC PX: EBS Volume Types SSD General Purpose SSD (<10,000 IOPS) - balances price and performance for wide variety of workloads Provisioned IOPS SSD (>10,000 IOPS) - highest-performance SSD volume for mission-critical low-latency or high-throughput workloads MAGNETIC Throughput Optimized HDD - Low cost HDD volume designed for frequently accessed, throughput-intensive workloads Cold HDD - Lowest cost HDD volume designed for less frequently accessed workloads Magnetic - Previous Generation. Can be a boot volume. EC2 Lab Commands Basic WebServer sudo su [ superuser access ] yum update -y [ update ec2 instance ] uum install httpd -y [ install apache server ] service httpd start [ start apache server ] chkconfig httpd on [ on ec2 restart server will auto restart ] service httpd status [ check apache server status ] cd /var/www/html [ change to server root directory ] vim index.html [ create basic web page index file ]","title":"EC2"},{"location":"aws/ec2/#aws-ec2","text":"","title":"AWS EC2"},{"location":"aws/ec2/#ec2-types","text":"On Demand - pay fixed rate by hour (or second) with no commitment Reserved - provides capacity reservation with significant discount on hourly charge - 1 Year or 3 Year terms Spot - enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexibile start and end times Dedicated Hosts - physical EC2 server dedicated for your use. Can help reduce costs by allowing you to use existing server-bound software licenses On Spot Instances : If they are terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged forthe complete hour in which the instance ran.","title":"EC2 Types"},{"location":"aws/ec2/#ec2-instance-types","text":"FIGHT DR MC PX:","title":"EC2 Instance Types"},{"location":"aws/ec2/#ebs-volume-types","text":"SSD General Purpose SSD (<10,000 IOPS) - balances price and performance for wide variety of workloads Provisioned IOPS SSD (>10,000 IOPS) - highest-performance SSD volume for mission-critical low-latency or high-throughput workloads MAGNETIC Throughput Optimized HDD - Low cost HDD volume designed for frequently accessed, throughput-intensive workloads Cold HDD - Lowest cost HDD volume designed for less frequently accessed workloads Magnetic - Previous Generation. Can be a boot volume.","title":"EBS Volume Types"},{"location":"aws/ec2/#ec2-lab-commands","text":"Basic WebServer sudo su [ superuser access ] yum update -y [ update ec2 instance ] uum install httpd -y [ install apache server ] service httpd start [ start apache server ] chkconfig httpd on [ on ec2 restart server will auto restart ] service httpd status [ check apache server status ] cd /var/www/html [ change to server root directory ] vim index.html [ create basic web page index file ]","title":"EC2 Lab Commands"},{"location":"aws/elasticache/","text":"AWS Elasticache Elasticache is a web services that makes it easy to deploy, operate and scale an in-memory cache in cloud. Improve performance of web apps by allowing retrieval of info from fast managed in memory caches instead of relying on slower disk-based DBs. Can be used to significantly improve latency for read-heavy app workloads (social networking/gaming) or compute-intensive workloads (recommendation engine) Caching improves app performance by storing critical piece of data in memory for low-latency access. Cached information may include the results of I/O-intensive DB queries or the results of computation intense calculations. Types Memcached Widely adopted memory object caching system. Protocol compliant with Memcached, so popular tools with existing Memcached environment will work with the services Pure caching solution with no persistence, ElastiCache manages Memcached nodes as a pool that can grow and shrink similar to an EC2 auto scaling group. individual nodes are expendable, and ElastiCache provides additional capabilities here, such as automatic node replacement and auto discovery. Use Cases: Object caching your goal - to offland DB. Simple caching model as possible Running large cache nodes & multithreaded performance with use of multiple cores Grow & scale cache horizontally Redis Popular open source in-memory key-value store supports data structs such as sorted sets and lists. Supports master/slave replication and Multi-AZ which can be used to achieve cross AZ redundancy. ElastiCache manages Redis more as a relational DB. Redis ElastiCache clusters are managed as stateful entities that include failover, similar to how Amazon RDS manages DB failover. Use Cases: More adv data types such as lists hashes & sets Sorting and ranking datasets in memory i.e. leaderboards Persistence of key store important Run multiple AZs with failover Exam Tips Typically scenario where DB is under stress/load -> may be asked which service you should use to alleviate this. Elasticache is a good choice if DB is read-heavy and not prone to frequent changing Redshift good answer if reason the DB is feeling stress is because management keep running OLAP transactions on it etc. Elasticache - contd. In memory cache in the cloud Improves performance of web apps, allowing retrieval of info from fast in-memory caches rather than slower disk based DBs. Sits between your app and the DB e.g. application frequently requesting specific product information for your best selling products Takes the load off your DBs Good if DB is particularly read-heavy and the data is not changing frequently Benefits & Use Cases Improves performance for read-heavy workloads e.g. Social Networking, gaming, Q&A Portals Frequently-accessed dat is store din memory for low-latency access, improving overall performance of your app. Also good for compute heavy workloads e.g. recommendation engines Can be used to store results of I/O intensive DB queries or output of compute-intensive calculations. Types Memcached Wiedly adopted memory object cachign system Multi threaded No Multi-AZ capability Redis Open-source in-memory key-value store Supports more complex data structures: sorted sets and lists Supports Master / SLave replicatoin and Multi-AZ for cross AZ redundancy Caching Strategies 2 strategies available: Lazy Loading Loads data into the cache only when necessary If requested data is in the cache, Elasticache returns the data to the application If the data is not in the cache or has expired, Elasticache returns a null Your application then fetches the data from the DB and writes the data received into the cache so that it is available next time. Advantages Disadvantages Only requested data cached: Avoids filling up cache with useless data Cache miss penalty: initial request, query to DB, writing of data to the cache Node failures are not fatal a new empty node will just have a lot of cache misses initially Stale data - if data is only updated when there is a cache miss, it can become stale. Doesn\u2019t automatically update if the data in the database changes Lazy Loading TTL (Time To Live) Specifies number of seconds until the key (data) expires to avoid keeping stale data in the cache Lazy loading treats an expired key as a cache miss and causes the application to retrieve the data from the DB and subsequently write the data into the cache with a new TTL Does not eliminate stale data - but helps to avoid it Write-Through Adds or updates data to the cache whenever data is written to the database Advantages Disadvantages Data in the cache never stale Write Penalty: every write involves a write to the cache as well as a write to the DB Users are generally more tolerant of additional latency when updating data than when retrieving it If a node fails and a new one is spun up, data is missing until added or updated in the DB (mitigate by implementing Lazy Loading in conjunction with write-through) Wasted resources if most of the data is never read Exam Tips - Elasticache In-memory cache sites between your application and DB 2 different caching strats - Lazy Loading & Write Through Lazy loading only caches data when it is requested Elasticache Node failures not fatal, just lots of cache misses Cache miss penalty: Initial request, query DB, writing to cache Avoid stale data by implementing a TTL Write through strategy writes data into the cache whenever there is a change to the DB Data is never Stale Write penalty: each write involves a write to the cache Elasticache node failure means that data is missing until added or updated in the DB Wasted resources if most of the data is never used","title":"Elasticache"},{"location":"aws/elasticache/#aws-elasticache","text":"Elasticache is a web services that makes it easy to deploy, operate and scale an in-memory cache in cloud. Improve performance of web apps by allowing retrieval of info from fast managed in memory caches instead of relying on slower disk-based DBs. Can be used to significantly improve latency for read-heavy app workloads (social networking/gaming) or compute-intensive workloads (recommendation engine) Caching improves app performance by storing critical piece of data in memory for low-latency access. Cached information may include the results of I/O-intensive DB queries or the results of computation intense calculations.","title":"AWS Elasticache"},{"location":"aws/elasticache/#types","text":"","title":"Types"},{"location":"aws/elasticache/#memcached","text":"Widely adopted memory object caching system. Protocol compliant with Memcached, so popular tools with existing Memcached environment will work with the services Pure caching solution with no persistence, ElastiCache manages Memcached nodes as a pool that can grow and shrink similar to an EC2 auto scaling group. individual nodes are expendable, and ElastiCache provides additional capabilities here, such as automatic node replacement and auto discovery. Use Cases: Object caching your goal - to offland DB. Simple caching model as possible Running large cache nodes & multithreaded performance with use of multiple cores Grow & scale cache horizontally","title":"Memcached"},{"location":"aws/elasticache/#redis","text":"Popular open source in-memory key-value store supports data structs such as sorted sets and lists. Supports master/slave replication and Multi-AZ which can be used to achieve cross AZ redundancy. ElastiCache manages Redis more as a relational DB. Redis ElastiCache clusters are managed as stateful entities that include failover, similar to how Amazon RDS manages DB failover. Use Cases: More adv data types such as lists hashes & sets Sorting and ranking datasets in memory i.e. leaderboards Persistence of key store important Run multiple AZs with failover","title":"Redis"},{"location":"aws/elasticache/#exam-tips","text":"Typically scenario where DB is under stress/load -> may be asked which service you should use to alleviate this. Elasticache is a good choice if DB is read-heavy and not prone to frequent changing Redshift good answer if reason the DB is feeling stress is because management keep running OLAP transactions on it etc.","title":"Exam Tips"},{"location":"aws/elasticache/#elasticache-contd","text":"In memory cache in the cloud Improves performance of web apps, allowing retrieval of info from fast in-memory caches rather than slower disk based DBs. Sits between your app and the DB e.g. application frequently requesting specific product information for your best selling products Takes the load off your DBs Good if DB is particularly read-heavy and the data is not changing frequently","title":"Elasticache - contd."},{"location":"aws/elasticache/#benefits-use-cases","text":"Improves performance for read-heavy workloads e.g. Social Networking, gaming, Q&A Portals Frequently-accessed dat is store din memory for low-latency access, improving overall performance of your app. Also good for compute heavy workloads e.g. recommendation engines Can be used to store results of I/O intensive DB queries or output of compute-intensive calculations.","title":"Benefits &amp; Use Cases"},{"location":"aws/elasticache/#types_1","text":"","title":"Types"},{"location":"aws/elasticache/#memcached_1","text":"Wiedly adopted memory object cachign system Multi threaded No Multi-AZ capability","title":"Memcached"},{"location":"aws/elasticache/#redis_1","text":"Open-source in-memory key-value store Supports more complex data structures: sorted sets and lists Supports Master / SLave replicatoin and Multi-AZ for cross AZ redundancy","title":"Redis"},{"location":"aws/elasticache/#caching-strategies","text":"2 strategies available:","title":"Caching Strategies"},{"location":"aws/elasticache/#lazy-loading","text":"Loads data into the cache only when necessary If requested data is in the cache, Elasticache returns the data to the application If the data is not in the cache or has expired, Elasticache returns a null Your application then fetches the data from the DB and writes the data received into the cache so that it is available next time. Advantages Disadvantages Only requested data cached: Avoids filling up cache with useless data Cache miss penalty: initial request, query to DB, writing of data to the cache Node failures are not fatal a new empty node will just have a lot of cache misses initially Stale data - if data is only updated when there is a cache miss, it can become stale. Doesn\u2019t automatically update if the data in the database changes","title":"Lazy Loading"},{"location":"aws/elasticache/#lazy-loading-ttl-time-to-live","text":"Specifies number of seconds until the key (data) expires to avoid keeping stale data in the cache Lazy loading treats an expired key as a cache miss and causes the application to retrieve the data from the DB and subsequently write the data into the cache with a new TTL Does not eliminate stale data - but helps to avoid it","title":"Lazy Loading TTL (Time To Live)"},{"location":"aws/elasticache/#write-through","text":"Adds or updates data to the cache whenever data is written to the database Advantages Disadvantages Data in the cache never stale Write Penalty: every write involves a write to the cache as well as a write to the DB Users are generally more tolerant of additional latency when updating data than when retrieving it If a node fails and a new one is spun up, data is missing until added or updated in the DB (mitigate by implementing Lazy Loading in conjunction with write-through) Wasted resources if most of the data is never read","title":"Write-Through"},{"location":"aws/elasticache/#exam-tips-elasticache","text":"In-memory cache sites between your application and DB 2 different caching strats - Lazy Loading & Write Through Lazy loading only caches data when it is requested Elasticache Node failures not fatal, just lots of cache misses Cache miss penalty: Initial request, query DB, writing to cache Avoid stale data by implementing a TTL Write through strategy writes data into the cache whenever there is a change to the DB Data is never Stale Write penalty: each write involves a write to the cache Elasticache node failure means that data is missing until added or updated in the DB Wasted resources if most of the data is never used","title":"Exam Tips - Elasticache"},{"location":"aws/elasticbeanstalk/","text":"AWS Elastic Beanstalk Elastic beanstalk is a service to deploying and scaling web applications developed in many languages: Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker onto widely used application server platforms like Apache Tomcat, Nginx, Passenger, and IIS. Developers can focus on writing code and don\u2019t need to worry about any of the underlying infrastructure needed to run the application. You upload the code and Elastic Beanstalk will handle deployment, capacity provisioning, load balancing, auto-scaling and application health. You retain full control of the underlying AWS resources powering your application and you pay only for the AWS resources required to store and run your applications e.g. EC2 instances & S3 buckets. Fastest and simplest way to deploy your application in AWS Automatically scales your application up and down You can select the EC2 instance type that is optimal for your application You can retain full admin control over the resources powering your app, or have EB do it for you Managed Platform Updates feature automatically applies & updates your OS, Java, PHP, Node.js, etc. Monitor and manage application health via dashboard Integrated with CloudWatch and X-Ray for performance data and metrics Exam Tips - EB Deploys and scales your web applications including the web application server platform where required Supports widely used programming tech - Java, PHP, Python, Ruby, Go, Docker, .NET, Node.js Support application server platforms - Tomcat, Passenger, Puma, IIS Provisions the underlying resources for you Can fully manage the EC2 instances for you or you can take full admin control Updates, monitoring, metrics and health checks all included. Updating Elastic Beanstalk EBS (Elastic Beanstalk) Deployment Policies: Elastic Beanstalk supports several options for processing deployments (deployment policies): All at once Rolling Rolling with additional batch Immutable All at once Deploys the new version to all instances simultaneously All of your instances are out of service while the deployment takes place You will experience an outage while the deployment is taking place - not ideal for mission-critical production systems If the update fails, you need to roll back the changes by re-deploying the original version to all your instances Rolling Deploys the new version in batches Each batch of instances is taken out of service while the deployment takes place Your environment capacity will be reduced by the number of instances in a batch while the deployment takes place Not ideal for performance sensitive systems If the update fails, you need to perform an additional rolling update to roll back the changes Rolling with Additional Batch Launches an additional batch of instances Deploy the new version in batches Maintains full capacity during the deployment process If the update fails, you need to perform an additional rolling update to roll back the changes Immutable Deploys the new version to a fresh group of instances in their own new auto-scaling group When the new instances pass their health checks they are moved to your existing auto scaling group; and finally, the old instances are terminated Maintains full capacity during the deployment process The impact of a failed update is far less, and the rollback process requires only terminating the new auto scaling group Preferred option for Mission Critical production systems. Exam Tips - Updating EB Remember the 4 different deployment approaches: All at Once Service interruption while you update the entire env. at once To roll back, perform a further All at Once upgrade. Rolling Reduced capacity during deployment To roll back, perform a further rolling update Rolling with Additional Batch Maintains full capacity To roll back, perform a further rolling update Immutable Preferred option for mission critical production systems Maintains full capacity To roll back, just delete the new instances and auto-scaling group Advanced Elastic Beanstalk Configuring Elastic Beanstalk You can customize your Elastic Beanstalk environment using Elastic Beanstalk configuration files e.g. you can define packages to install, create Linux users and groups, run shell commands, specify services to enable or configure your load balancer, etc.) These are written in YAML or JSON format. They can have a filename of your choice but must have a .config extension and be saved inside a folder called .ebextensions. Location of Configuration Files The .ebextensions folder must be included in the top-level directory of your application source code bundle. This means that the configuration files can be placed under source control along with the rest of your application code. Example - myhealthcheckurl.config This .config file configures an Application health-check URL which will be used by the Elastic Load Balancer. { \"option_settings\" : [ { \"namespace\" : \"aws:elasticbeanstalk:application\" , \"option_name\" : \"My Application Healthcheck URL\" , \"value\" : \"/healthcheck\" } ] } Exam Tips - Advanced Elastic Beanstalk You can customize your Elastic Beanstalk environment by adding configuration files The files are written in YAML or JSON Files have a .config extension The .config files are saved to the .ebextensions folder Your .ebextensions folder must be located in the top level directory of your application source code bundle Elastic Beanstalk & RDS Elastic Beanstalk supports two ways of integrating an RDS database with your Beanstalk environment. You can launch the RDS instance from within the Elastic Beanstalk console, which means the RDS instance is created within your Elastic Beanstalk environment - a good option for Dev and Test deployments. However this may not be ideal for production environments because it means the lifecycle of your database is tied to the lifecycle of your application environment. If your terminate the environment, the database instance will be terminated too. For Production environments, the preferred option is to decouple the RDS instance from your EBS environment i.e. launch it outside of Elastic Beanstalk, directly from the RDS section of the console. This option gives you a lot more flexibility, allows you to connect multiple environments to the same database, provides a wider choice of database types, and allows you to tear down your application environment without affecting the database instance. Access to RDS from Elastic Beanstalk To allows the EC2 instances in your Elastic Beanstalk environment to connect to an outside database, there are two additional configuration steps required: An additional Security Group must be added to your environment\u2019s Auto Scaling group You\u2019ll need to provide connection string configuration information to your application servers (endpoint, password using Elastic Beanstalk environment properties) Exam Tips - RDS Elastic Beanstalk Two different options for launching your RDS instance: Launch Within Elastic Beanstalk When your terminate the Beanstalk env, the DB will also be terminated Quick and easy to add your DB and get started Suitable for Dev and Test environments only Launch Outside Elastic Beanstalk Additional config. steps required - security group & connection information Suitable for Prod environments, more flexibility Allows connection from multiple envs, you can tear down the app stack without impacting the DB","title":"Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#aws-elastic-beanstalk","text":"Elastic beanstalk is a service to deploying and scaling web applications developed in many languages: Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker onto widely used application server platforms like Apache Tomcat, Nginx, Passenger, and IIS. Developers can focus on writing code and don\u2019t need to worry about any of the underlying infrastructure needed to run the application. You upload the code and Elastic Beanstalk will handle deployment, capacity provisioning, load balancing, auto-scaling and application health. You retain full control of the underlying AWS resources powering your application and you pay only for the AWS resources required to store and run your applications e.g. EC2 instances & S3 buckets. Fastest and simplest way to deploy your application in AWS Automatically scales your application up and down You can select the EC2 instance type that is optimal for your application You can retain full admin control over the resources powering your app, or have EB do it for you Managed Platform Updates feature automatically applies & updates your OS, Java, PHP, Node.js, etc. Monitor and manage application health via dashboard Integrated with CloudWatch and X-Ray for performance data and metrics","title":"AWS Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#exam-tips-eb","text":"Deploys and scales your web applications including the web application server platform where required Supports widely used programming tech - Java, PHP, Python, Ruby, Go, Docker, .NET, Node.js Support application server platforms - Tomcat, Passenger, Puma, IIS Provisions the underlying resources for you Can fully manage the EC2 instances for you or you can take full admin control Updates, monitoring, metrics and health checks all included.","title":"Exam Tips - EB"},{"location":"aws/elasticbeanstalk/#updating-elastic-beanstalk","text":"EBS (Elastic Beanstalk) Deployment Policies: Elastic Beanstalk supports several options for processing deployments (deployment policies): All at once Rolling Rolling with additional batch Immutable","title":"Updating Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#all-at-once","text":"Deploys the new version to all instances simultaneously All of your instances are out of service while the deployment takes place You will experience an outage while the deployment is taking place - not ideal for mission-critical production systems If the update fails, you need to roll back the changes by re-deploying the original version to all your instances","title":"All at once"},{"location":"aws/elasticbeanstalk/#rolling","text":"Deploys the new version in batches Each batch of instances is taken out of service while the deployment takes place Your environment capacity will be reduced by the number of instances in a batch while the deployment takes place Not ideal for performance sensitive systems If the update fails, you need to perform an additional rolling update to roll back the changes","title":"Rolling"},{"location":"aws/elasticbeanstalk/#rolling-with-additional-batch","text":"Launches an additional batch of instances Deploy the new version in batches Maintains full capacity during the deployment process If the update fails, you need to perform an additional rolling update to roll back the changes","title":"Rolling with Additional Batch"},{"location":"aws/elasticbeanstalk/#immutable","text":"Deploys the new version to a fresh group of instances in their own new auto-scaling group When the new instances pass their health checks they are moved to your existing auto scaling group; and finally, the old instances are terminated Maintains full capacity during the deployment process The impact of a failed update is far less, and the rollback process requires only terminating the new auto scaling group Preferred option for Mission Critical production systems.","title":"Immutable"},{"location":"aws/elasticbeanstalk/#exam-tips-updating-eb","text":"Remember the 4 different deployment approaches: All at Once Service interruption while you update the entire env. at once To roll back, perform a further All at Once upgrade. Rolling Reduced capacity during deployment To roll back, perform a further rolling update Rolling with Additional Batch Maintains full capacity To roll back, perform a further rolling update Immutable Preferred option for mission critical production systems Maintains full capacity To roll back, just delete the new instances and auto-scaling group","title":"Exam Tips - Updating EB"},{"location":"aws/elasticbeanstalk/#advanced-elastic-beanstalk","text":"","title":"Advanced Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#configuring-elastic-beanstalk","text":"You can customize your Elastic Beanstalk environment using Elastic Beanstalk configuration files e.g. you can define packages to install, create Linux users and groups, run shell commands, specify services to enable or configure your load balancer, etc.) These are written in YAML or JSON format. They can have a filename of your choice but must have a .config extension and be saved inside a folder called .ebextensions.","title":"Configuring Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#location-of-configuration-files","text":"The .ebextensions folder must be included in the top-level directory of your application source code bundle. This means that the configuration files can be placed under source control along with the rest of your application code.","title":"Location of Configuration Files"},{"location":"aws/elasticbeanstalk/#example-myhealthcheckurlconfig","text":"This .config file configures an Application health-check URL which will be used by the Elastic Load Balancer. { \"option_settings\" : [ { \"namespace\" : \"aws:elasticbeanstalk:application\" , \"option_name\" : \"My Application Healthcheck URL\" , \"value\" : \"/healthcheck\" } ] }","title":"Example - myhealthcheckurl.config"},{"location":"aws/elasticbeanstalk/#exam-tips-advanced-elastic-beanstalk","text":"You can customize your Elastic Beanstalk environment by adding configuration files The files are written in YAML or JSON Files have a .config extension The .config files are saved to the .ebextensions folder Your .ebextensions folder must be located in the top level directory of your application source code bundle","title":"Exam Tips - Advanced Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#elastic-beanstalk-rds","text":"Elastic Beanstalk supports two ways of integrating an RDS database with your Beanstalk environment. You can launch the RDS instance from within the Elastic Beanstalk console, which means the RDS instance is created within your Elastic Beanstalk environment - a good option for Dev and Test deployments. However this may not be ideal for production environments because it means the lifecycle of your database is tied to the lifecycle of your application environment. If your terminate the environment, the database instance will be terminated too. For Production environments, the preferred option is to decouple the RDS instance from your EBS environment i.e. launch it outside of Elastic Beanstalk, directly from the RDS section of the console. This option gives you a lot more flexibility, allows you to connect multiple environments to the same database, provides a wider choice of database types, and allows you to tear down your application environment without affecting the database instance.","title":"Elastic Beanstalk &amp; RDS"},{"location":"aws/elasticbeanstalk/#access-to-rds-from-elastic-beanstalk","text":"To allows the EC2 instances in your Elastic Beanstalk environment to connect to an outside database, there are two additional configuration steps required: An additional Security Group must be added to your environment\u2019s Auto Scaling group You\u2019ll need to provide connection string configuration information to your application servers (endpoint, password using Elastic Beanstalk environment properties)","title":"Access to RDS from Elastic Beanstalk"},{"location":"aws/elasticbeanstalk/#exam-tips-rds-elastic-beanstalk","text":"Two different options for launching your RDS instance: Launch Within Elastic Beanstalk When your terminate the Beanstalk env, the DB will also be terminated Quick and easy to add your DB and get started Suitable for Dev and Test environments only Launch Outside Elastic Beanstalk Additional config. steps required - security group & connection information Suitable for Prod environments, more flexibility Allows connection from multiple envs, you can tear down the app stack without impacting the DB","title":"Exam Tips - RDS Elastic Beanstalk"},{"location":"aws/elb/","text":"AWS ELB Types of Load Balancer Application Load Balancer [Layer 7] Network Load Balancer [Layer 4] - AWS most expensive Classic Load Balancer - legacy purposes - non-recommended Application Load Balancers Best suited for load balancing of http and https traffic Operate at layer 7 and are application-aware Intelligent - you can create advanced request routing, sending specified requests to specific web servers Network Load Balancers Best suited for load balancing of TCP traffic where extremer performance is required Operating at the connection level (Layer 4) Capable of handling millions of requests per second while maintaining ultra-low latencies Use for extreme performance. [most expensive to buy] Classic Load Balancers Legacy Elastic Load Balancers Load balance http/https applications and use Layer 7 specific features such as X-Forwarded and sticky sessions Can also use strict Layer 4 load balancing for applications that rely purely on the TCP protocol If application stops responding the ELB responds with a 504 error. This means app is having issues - at either Web Server or Database Layer Identify where the application is failing, and scale it up or out where possible (504 err) X-Forwarded-For Header How to get IPv4 public address when load balancer sends EC2 the private address? Uses X-Forwarded-For header","title":"ELB"},{"location":"aws/elb/#aws-elb","text":"","title":"AWS ELB"},{"location":"aws/elb/#types-of-load-balancer","text":"Application Load Balancer [Layer 7] Network Load Balancer [Layer 4] - AWS most expensive Classic Load Balancer - legacy purposes - non-recommended","title":"Types of Load Balancer"},{"location":"aws/elb/#application-load-balancers","text":"Best suited for load balancing of http and https traffic Operate at layer 7 and are application-aware Intelligent - you can create advanced request routing, sending specified requests to specific web servers","title":"Application Load Balancers"},{"location":"aws/elb/#network-load-balancers","text":"Best suited for load balancing of TCP traffic where extremer performance is required Operating at the connection level (Layer 4) Capable of handling millions of requests per second while maintaining ultra-low latencies Use for extreme performance. [most expensive to buy]","title":"Network Load Balancers"},{"location":"aws/elb/#classic-load-balancers","text":"Legacy Elastic Load Balancers Load balance http/https applications and use Layer 7 specific features such as X-Forwarded and sticky sessions Can also use strict Layer 4 load balancing for applications that rely purely on the TCP protocol If application stops responding the ELB responds with a 504 error. This means app is having issues - at either Web Server or Database Layer Identify where the application is failing, and scale it up or out where possible (504 err) X-Forwarded-For Header How to get IPv4 public address when load balancer sends EC2 the private address? Uses X-Forwarded-For header","title":"Classic Load Balancers"},{"location":"aws/iam/","text":"AWS IAM IAM consists of: Users Groups [Group users and apply policies collectively] Roles Policy Documents [JSON] Basics: IAM is universal - does not apply to regions at this time The \u2018root account\u2019 is account created when first setup your AWS account (complete admin access) New Users have no permissions when first created New Users are assigned Access Key ID & Secret Access Keys when first created These are not same as a password, you cannot use the key ID and secret access key to login to the AWS management console You can use this to access AWS via the APIs and CLI Account Setup: You only get to view Access Key ID & Secrete Access Key once. If lost, you have to regenerate them. Always setup Multifactor Authentication (MFA) on your root account You can create and customise your own password rotation policies Practical: Applying IAM role to EC2 : Go to EC2 in UI Select Actions/Instance-Settings/ Attach/Replace IAM role Modifying S3 within EC2 : cd ~/.aws [ contains config credentials ] aws s3 ls aws s3 ls s3://experimenting-aws echo \"Hello v2 from EC2\" > hello2.txt aws s3 cp hello2.txt s3://experimenting-aws Exam Tips Roles allow you to not use Access Key ID\u2019s and Secret Access Keys Roles are preferred from a security perspective Roles are controlled by policies You can change a policy on a role and it will take immediate affect You can attach and detach roles to running EC2 instances without having to stop or terminate these instances","title":"IAM"},{"location":"aws/iam/#aws-iam","text":"IAM consists of: Users Groups [Group users and apply policies collectively] Roles Policy Documents [JSON]","title":"AWS IAM"},{"location":"aws/iam/#basics","text":"IAM is universal - does not apply to regions at this time The \u2018root account\u2019 is account created when first setup your AWS account (complete admin access) New Users have no permissions when first created New Users are assigned Access Key ID & Secret Access Keys when first created These are not same as a password, you cannot use the key ID and secret access key to login to the AWS management console You can use this to access AWS via the APIs and CLI","title":"Basics:"},{"location":"aws/iam/#account-setup","text":"You only get to view Access Key ID & Secrete Access Key once. If lost, you have to regenerate them. Always setup Multifactor Authentication (MFA) on your root account You can create and customise your own password rotation policies","title":"Account Setup:"},{"location":"aws/iam/#practical","text":"Applying IAM role to EC2 : Go to EC2 in UI Select Actions/Instance-Settings/ Attach/Replace IAM role Modifying S3 within EC2 : cd ~/.aws [ contains config credentials ] aws s3 ls aws s3 ls s3://experimenting-aws echo \"Hello v2 from EC2\" > hello2.txt aws s3 cp hello2.txt s3://experimenting-aws","title":"Practical:"},{"location":"aws/iam/#exam-tips","text":"Roles allow you to not use Access Key ID\u2019s and Secret Access Keys Roles are preferred from a security perspective Roles are controlled by policies You can change a policy on a role and it will take immediate affect You can attach and detach roles to running EC2 instances without having to stop or terminate these instances","title":"Exam Tips"},{"location":"aws/kinesis/","text":"AWS Kinesis What is streaming data? Streaming data is data that is generated continuously by thousands of data sources, which typically send the data records simultaneously, and in small sizes (order of Kilobytes)/ Purchases from online stores (amazong.com) Stock Prices Game data (as the gamer plays) Social network data Geospatial data (uber.com) iOT sensor data What is Kinesis? Amazon Kinesis is a platform on AWS to send your streaming data too. Kinesis makes it easy to load and analyze streaming data, and also providing the ability for you to build your own custom applications for your business needs. What are the core Kinesis Services Kinesis Streams (video & data) Kinesis Firehose Kinesis Analytics Kinesis Streams Producers -> Kinesis Streams Stores data in shards. Data stored 24hrs, can be upped to 7 days Consumers -> EC2 instances taking data from shards & doing analysis Data outputted to DynamoDB, S3, EMR, Redshift etc. Kinesis Streams consist of shards 5 transactions per second for reads, up to max total data read rate of 2MB per second up to 1,000 records per second for writes, up to a maximum total data write rate of 1MB per second (including partition keys) The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards. Kinesis Firehouse Producers -> Kinesis Firehose (no concern for shards/streams as its automated) No data retention period -> once data enters its optionally analyzed via lambda or outputted (i.e. ElasticSearch Cluster / S3 -> RedShift) No concern for consumers (lambda analytics if you wish - optional) -> output S3 Kinesis Analytics Allows you to run SQL queries of data as it exists within Firehouse/Streams to output data inside Kinesis/Streams to S3/Redshift/ElasticSearch Exam Tips Know the difference between Kinesis Streams and Firehose. You will be given scenario questions and you must choose the most relevant service Understand what Kinesis Analytics is Sample CloudFormation Deployment with Kinesis { \"AWSTemplateFormatVersion\" : \"2010-09-09\" , \"Description\" : \"The Amazon Kinesis Data Visualization Sample Application\" , \"Parameters\" : { \"InstanceType\" : { \"Description\" : \"EC2 instance type\" , \"Type\" : \"String\" , \"Default\" : \"t2.micro\" , \"AllowedValues\" : [ \"t2.micro\" , \"t2.small\" , \"t2.medium\" , \"m3.medium\" , \"m3.large\" , \"m3.xlarge\" , \"m3.2xlarge\" , \"c3.large\" , \"c3.xlarge\" , \"c3.2xlarge\" , \"c3.4xlarge\" , \"c3.8xlarge\" ], \"ConstraintDescription\" : \"must be a supported EC2 instance type for this template.\" }, \"KeyName\" : { \"Description\" : \"(Optional) Name of an existing EC2 KeyPair to enable SSH access to the instance. If this is not provided you will not be able to SSH on to the EC2 instance.\" , \"Type\" : \"String\" , \"Default\" : \"\" , \"MinLength\" : \"0\" , \"MaxLength\" : \"255\" , \"AllowedPattern\" : \"[\\\\x20-\\\\x7E]*\" , \"ConstraintDescription\" : \"can contain only ASCII characters.\" }, \"SSHLocation\" : { \"Description\" : \"The IP address range that can be used to SSH to the EC2 instances\" , \"Type\" : \"String\" , \"MinLength\" : \"9\" , \"MaxLength\" : \"18\" , \"Default\" : \"0.0.0.0/0\" , \"AllowedPattern\" : \"(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})\" , \"ConstraintDescription\" : \"must be a valid IP CIDR range of the form x.x.x.x/x.\" }, \"ApplicationArchive\" : { \"Description\" : \"A publicly accessible URL to the sample application archive as produced by 'mvn package'\" , \"Type\" : \"String\" , \"MinLength\" : \"7\" , \"MaxLength\" : \"255\" , \"Default\" : \"https://github.com/awslabs/amazon-kinesis-data-visualization-sample/releases/download/v1.1.1/amazon-kinesis-data-visualization-sample-1.1.1-assembly.zip\" } }, \"Conditions\" : { \"UseEC2KeyName\" : { \"Fn::Not\" : [{ \"Fn::Equals\" : [{ \"Ref\" : \"KeyName\" }, \"\" ]}]} }, \"Mappings\" : { \"AWSInstanceType2Arch\" : { \"t2.micro\" : { \"Arch\" : \"64\" }, \"t2.small\" : { \"Arch\" : \"64\" }, \"t2.medium\" : { \"Arch\" : \"64\" }, \"m3.medium\" : { \"Arch\" : \"64\" }, \"m3.large\" : { \"Arch\" : \"64\" }, \"m3.xlarge\" : { \"Arch\" : \"64\" }, \"m3.2xlarge\" : { \"Arch\" : \"64\" }, \"c3.large\" : { \"Arch\" : \"64\" }, \"c3.xlarge\" : { \"Arch\" : \"64\" }, \"c3.2xlarge\" : { \"Arch\" : \"64\" }, \"c3.4xlarge\" : { \"Arch\" : \"64\" }, \"c3.8xlarge\" : { \"Arch\" : \"64\" } }, \"AWSRegionArch2AMI\" : { \"us-east-1\" : { \"64\" : \"ami-76817c1e\" }, \"us-west-2\" : { \"64\" : \"ami-d13845e1\" }, \"eu-west-1\" : { \"64\" : \"ami-892fe1fe\" }, \"ap-southeast-1\" : { \"64\" : \"ami-a6b6eaf4\" }, \"ap-southeast-2\" : { \"64\" : \"ami-d9fe9be3\" }, \"ap-northeast-1\" : { \"64\" : \"ami-29dc9228\" } } }, \"Resources\" : { \"KinesisStream\" : { \"Type\" : \"AWS::Kinesis::Stream\" , \"Properties\" : { \"ShardCount\" : \"2\" } }, \"KCLDynamoDBTable\" : { \"Type\" : \"AWS::DynamoDB::Table\" , \"Properties\" : { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"leaseKey\" , \"AttributeType\" : \"S\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"leaseKey\" , \"KeyType\" : \"HASH\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"10\" , \"WriteCapacityUnits\" : \"5\" } } }, \"CountsDynamoDBTable\" : { \"Type\" : \"AWS::DynamoDB::Table\" , \"Properties\" : { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"resource\" , \"AttributeType\" : \"S\" }, { \"AttributeName\" : \"timestamp\" , \"AttributeType\" : \"S\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"resource\" , \"KeyType\" : \"HASH\" }, { \"AttributeName\" : \"timestamp\" , \"KeyType\" : \"RANGE\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"10\" , \"WriteCapacityUnits\" : \"5\" } } }, \"Ec2SecurityGroup\" : { \"Type\" : \"AWS::EC2::SecurityGroup\" , \"Properties\" : { \"GroupDescription\" : \"Enable SSH access and HTTP access on the inbound port\" , \"SecurityGroupIngress\" : [{ \"IpProtocol\" : \"tcp\" , \"FromPort\" : \"22\" , \"ToPort\" : \"22\" , \"CidrIp\" : { \"Ref\" : \"SSHLocation\" } }, { \"IpProtocol\" : \"tcp\" , \"FromPort\" : \"80\" , \"ToPort\" : \"80\" , \"CidrIp\" : \"0.0.0.0/0\" }] } }, \"EIP\" : { \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"InstanceId\" : { \"Ref\" : \"Ec2Instance\" } } }, \"RootRole\" : { \"Type\" : \"AWS::IAM::Role\" , \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] } ] }, \"Path\" : \"/\" } }, \"RolePolicies\" : { \"Type\" : \"AWS::IAM::Policy\" , \"Properties\" : { \"PolicyName\" : \"root\" , \"PolicyDocument\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"kinesis:*\" , \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:kinesis:\" , { \"Ref\" : \"AWS::Region\" }, \":\" , { \"Ref\" : \"AWS::AccountId\" }, \":stream/\" , { \"Ref\" : \"KinesisStream\" } ]]} }, { \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:dynamodb:\" , { \"Ref\" : \"AWS::Region\" }, \":\" , { \"Ref\" : \"AWS::AccountId\" }, \":table/\" , { \"Ref\" : \"KCLDynamoDBTable\" } ]]} }, { \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:dynamodb:\" , { \"Ref\" : \"AWS::Region\" }, \":\" , { \"Ref\" : \"AWS::AccountId\" }, \":table/\" , { \"Ref\" : \"CountsDynamoDBTable\" } ]]} }, { \"Effect\" : \"Allow\" , \"Action\" : \"cloudwatch:*\" , \"Resource\" : \"*\" } ] }, \"Roles\" : [ { \"Ref\" : \"RootRole\" } ] } }, \"RootInstanceProfile\" : { \"Type\" : \"AWS::IAM::InstanceProfile\" , \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [ { \"Ref\" : \"RootRole\" } ] } }, \"Ec2Instance\" : { \"Type\" : \"AWS::EC2::Instance\" , \"Metadata\" : { \"AWS::CloudFormation::Init\" : { \"config\" : { \"packages\" : { \"yum\" : { \"java-1.7.0-openjdk\" : [] } }, \"files\" : { \"/var/kinesis-data-vis-sample-app/watchdog.sh\" : { \"content\" : { \"Fn::Join\" : [ \"\" , [ \"#!/bin/bash\\n\" , \"if ! ps aux | grep HttpReferrerCounterApplication | grep -v grep ; then\\n\" , \" # Launch the Kinesis application for counting HTTP referrer pairs\\n\" , \" java -cp /var/kinesis-data-vis-sample-app/lib/\\\\* com.amazonaws.services.kinesis.samples.datavis.HttpReferrerCounterApplication \" , { \"Ref\" : \"KCLDynamoDBTable\" }, \" \" , { \"Ref\" : \"KinesisStream\" }, \" \" , { \"Ref\" : \"CountsDynamoDBTable\" }, \" \" , { \"Ref\" : \"AWS::Region\" }, \" &>> /home/ec2-user/kinesis-data-vis-sample-app-kcl.log &\\n\" , \"fi\\n\" , \"if ! ps aux | grep HttpReferrerStreamWriter | grep -v grep ; then\\n\" , \" # Launch our Kinesis stream writer to fill our stream with generated HTTP (resource, referrer) pairs.\\n\" , \" # This will create a writer with 5 threads to send records indefinitely.\\n\" , \" java -cp /var/kinesis-data-vis-sample-app/lib/\\\\* com.amazonaws.services.kinesis.samples.datavis.HttpReferrerStreamWriter 5 \" , { \"Ref\" : \"KinesisStream\" }, \" \" , { \"Ref\" : \"AWS::Region\" }, \" &>> /home/ec2-user/kinesis-data-vis-sample-app-publisher.log &\\n\" , \"fi\\n\" , \"if ! ps aux | grep WebServer | grep -v grep ; then\\n\" , \" # Launch the webserver\\n\" , \" java -cp /var/kinesis-data-vis-sample-app/lib/\\\\* com.amazonaws.services.kinesis.samples.datavis.WebServer 80 /var/kinesis-data-vis-sample-app/wwwroot \" , { \"Ref\" : \"CountsDynamoDBTable\" }, \" \" , { \"Ref\" : \"AWS::Region\" }, \" &>> /home/ec2-user/kinesis-data-vis-sample-app-www.log &\\n\" , \"fi\\n\" ]]}, \"mode\" : \"000755\" , \"owner\" : \"ec2-user\" , \"group\" : \"ec2-user\" }, \"/var/kinesis-data-vis-sample-app/crontask\" : { \"content\" : { \"Fn::Join\" : [ \"\" , [ \"* * * * * bash /var/kinesis-data-vis-sample-app/watchdog.sh\\n\" ]]}, \"mode\" : \"000644\" , \"owner\" : \"ec2-user\" , \"group\" : \"ec2-user\" } }, \"sources\" : { \"/var/kinesis-data-vis-sample-app\" : { \"Ref\" : \"ApplicationArchive\" } } } } }, \"Properties\" : { \"KeyName\" : { \"Fn::If\" : [ \"UseEC2KeyName\" , { \"Ref\" : \"KeyName\" }, { \"Ref\" : \"AWS::NoValue\" } ]}, \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSRegionArch2AMI\" , { \"Ref\" : \"AWS::Region\" }, { \"Fn::FindInMap\" : [ \"AWSInstanceType2Arch\" , { \"Ref\" : \"InstanceType\" }, \"Arch\" ] } ] }, \"InstanceType\" : { \"Ref\" : \"InstanceType\" }, \"SecurityGroups\" : [{ \"Ref\" : \"Ec2SecurityGroup\" }], \"IamInstanceProfile\" : { \"Ref\" : \"RootInstanceProfile\" }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ \"#!/bin/bash\\n\" , \"yum update -y aws-cfn-bootstrap\\n\" , \"/opt/aws/bin/cfn-init -s \" , { \"Ref\" : \"AWS::StackId\" }, \" -r Ec2Instance \" , \" --region \" , { \"Ref\" : \"AWS::Region\" }, \"\\n\" , \"# Register watchdog script with cron\\n\" , \"crontab /var/kinesis-data-vis-sample-app/crontask\\n\" , \"# Launch watchdog script immediately so if it fails this stack fails to start\\n\" , \"/var/kinesis-data-vis-sample-app/watchdog.sh\\n\" , \"/opt/aws/bin/cfn-signal -e $? '\" , { \"Ref\" : \"WaitHandle\" }, \"'\\n\" ]]}} } }, \"WaitHandle\" : { \"Type\" : \"AWS::CloudFormation::WaitConditionHandle\" }, \"WaitCondition\" : { \"Type\" : \"AWS::CloudFormation::WaitCondition\" , \"DependsOn\" : \"Ec2Instance\" , \"Properties\" : { \"Handle\" : { \"Ref\" : \"WaitHandle\" }, \"Timeout\" : \"600\" } } }, \"Outputs\" : { \"URL\" : { \"Description\" : \"URL to the sample application's visualization\" , \"Value\" : { \"Fn::Join\" : [ \"\" , [ \"http://\" , { \"Fn::GetAtt\" : [ \"Ec2Instance\" , \"PublicDnsName\" ] }]]} }, \"InstanceId\" : { \"Description\" : \"InstanceId of the newly created EC2 instance\" , \"Value\" : { \"Ref\" : \"Ec2Instance\" } }, \"AZ\" : { \"Description\" : \"Availability Zone of the newly created EC2 instance\" , \"Value\" : { \"Fn::GetAtt\" : [ \"Ec2Instance\" , \"AvailabilityZone\" ] } }, \"StreamName\" : { \"Description\" : \"The name of the Kinesis Stream. This was autogenerated by the Kinesis Resource named 'KinesisStream'\" , \"Value\" : { \"Ref\" : \"KinesisStream\" } }, \"ApplicationName\" : { \"Description\" : \"The name of the Kinesis Client Application. This was autogenerated by the DynamoDB Resource named 'KCLDynamoDBTable'\" , \"Value\" : { \"Ref\" : \"KCLDynamoDBTable\" } }, \"CountsTable\" : { \"Description\" : \"The name of the DynamoDB table where counts are persisted. This was autogenerated by the DynamoDB Resource named 'CountsDynamoDBTable'\" , \"Value\" : { \"Ref\" : \"CountsDynamoDBTable\" } } } }","title":"Kinesis"},{"location":"aws/kinesis/#aws-kinesis","text":"","title":"AWS Kinesis"},{"location":"aws/kinesis/#what-is-streaming-data","text":"Streaming data is data that is generated continuously by thousands of data sources, which typically send the data records simultaneously, and in small sizes (order of Kilobytes)/ Purchases from online stores (amazong.com) Stock Prices Game data (as the gamer plays) Social network data Geospatial data (uber.com) iOT sensor data","title":"What is streaming data?"},{"location":"aws/kinesis/#what-is-kinesis","text":"Amazon Kinesis is a platform on AWS to send your streaming data too. Kinesis makes it easy to load and analyze streaming data, and also providing the ability for you to build your own custom applications for your business needs.","title":"What is Kinesis?"},{"location":"aws/kinesis/#what-are-the-core-kinesis-services","text":"Kinesis Streams (video & data) Kinesis Firehose Kinesis Analytics","title":"What are the core Kinesis Services"},{"location":"aws/kinesis/#kinesis-streams","text":"Producers -> Kinesis Streams Stores data in shards. Data stored 24hrs, can be upped to 7 days Consumers -> EC2 instances taking data from shards & doing analysis Data outputted to DynamoDB, S3, EMR, Redshift etc. Kinesis Streams consist of shards 5 transactions per second for reads, up to max total data read rate of 2MB per second up to 1,000 records per second for writes, up to a maximum total data write rate of 1MB per second (including partition keys) The data capacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the capacities of its shards.","title":"Kinesis Streams"},{"location":"aws/kinesis/#kinesis-firehouse","text":"Producers -> Kinesis Firehose (no concern for shards/streams as its automated) No data retention period -> once data enters its optionally analyzed via lambda or outputted (i.e. ElasticSearch Cluster / S3 -> RedShift) No concern for consumers (lambda analytics if you wish - optional) -> output S3","title":"Kinesis Firehouse"},{"location":"aws/kinesis/#kinesis-analytics","text":"Allows you to run SQL queries of data as it exists within Firehouse/Streams to output data inside Kinesis/Streams to S3/Redshift/ElasticSearch","title":"Kinesis Analytics"},{"location":"aws/kinesis/#exam-tips","text":"Know the difference between Kinesis Streams and Firehose. You will be given scenario questions and you must choose the most relevant service Understand what Kinesis Analytics is","title":"Exam Tips"},{"location":"aws/kinesis/#sample-cloudformation-deployment-with-kinesis","text":"{ \"AWSTemplateFormatVersion\" : \"2010-09-09\" , \"Description\" : \"The Amazon Kinesis Data Visualization Sample Application\" , \"Parameters\" : { \"InstanceType\" : { \"Description\" : \"EC2 instance type\" , \"Type\" : \"String\" , \"Default\" : \"t2.micro\" , \"AllowedValues\" : [ \"t2.micro\" , \"t2.small\" , \"t2.medium\" , \"m3.medium\" , \"m3.large\" , \"m3.xlarge\" , \"m3.2xlarge\" , \"c3.large\" , \"c3.xlarge\" , \"c3.2xlarge\" , \"c3.4xlarge\" , \"c3.8xlarge\" ], \"ConstraintDescription\" : \"must be a supported EC2 instance type for this template.\" }, \"KeyName\" : { \"Description\" : \"(Optional) Name of an existing EC2 KeyPair to enable SSH access to the instance. If this is not provided you will not be able to SSH on to the EC2 instance.\" , \"Type\" : \"String\" , \"Default\" : \"\" , \"MinLength\" : \"0\" , \"MaxLength\" : \"255\" , \"AllowedPattern\" : \"[\\\\x20-\\\\x7E]*\" , \"ConstraintDescription\" : \"can contain only ASCII characters.\" }, \"SSHLocation\" : { \"Description\" : \"The IP address range that can be used to SSH to the EC2 instances\" , \"Type\" : \"String\" , \"MinLength\" : \"9\" , \"MaxLength\" : \"18\" , \"Default\" : \"0.0.0.0/0\" , \"AllowedPattern\" : \"(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.(\\\\d{1,3})\\\\.(\\\\d{1,3})/(\\\\d{1,2})\" , \"ConstraintDescription\" : \"must be a valid IP CIDR range of the form x.x.x.x/x.\" }, \"ApplicationArchive\" : { \"Description\" : \"A publicly accessible URL to the sample application archive as produced by 'mvn package'\" , \"Type\" : \"String\" , \"MinLength\" : \"7\" , \"MaxLength\" : \"255\" , \"Default\" : \"https://github.com/awslabs/amazon-kinesis-data-visualization-sample/releases/download/v1.1.1/amazon-kinesis-data-visualization-sample-1.1.1-assembly.zip\" } }, \"Conditions\" : { \"UseEC2KeyName\" : { \"Fn::Not\" : [{ \"Fn::Equals\" : [{ \"Ref\" : \"KeyName\" }, \"\" ]}]} }, \"Mappings\" : { \"AWSInstanceType2Arch\" : { \"t2.micro\" : { \"Arch\" : \"64\" }, \"t2.small\" : { \"Arch\" : \"64\" }, \"t2.medium\" : { \"Arch\" : \"64\" }, \"m3.medium\" : { \"Arch\" : \"64\" }, \"m3.large\" : { \"Arch\" : \"64\" }, \"m3.xlarge\" : { \"Arch\" : \"64\" }, \"m3.2xlarge\" : { \"Arch\" : \"64\" }, \"c3.large\" : { \"Arch\" : \"64\" }, \"c3.xlarge\" : { \"Arch\" : \"64\" }, \"c3.2xlarge\" : { \"Arch\" : \"64\" }, \"c3.4xlarge\" : { \"Arch\" : \"64\" }, \"c3.8xlarge\" : { \"Arch\" : \"64\" } }, \"AWSRegionArch2AMI\" : { \"us-east-1\" : { \"64\" : \"ami-76817c1e\" }, \"us-west-2\" : { \"64\" : \"ami-d13845e1\" }, \"eu-west-1\" : { \"64\" : \"ami-892fe1fe\" }, \"ap-southeast-1\" : { \"64\" : \"ami-a6b6eaf4\" }, \"ap-southeast-2\" : { \"64\" : \"ami-d9fe9be3\" }, \"ap-northeast-1\" : { \"64\" : \"ami-29dc9228\" } } }, \"Resources\" : { \"KinesisStream\" : { \"Type\" : \"AWS::Kinesis::Stream\" , \"Properties\" : { \"ShardCount\" : \"2\" } }, \"KCLDynamoDBTable\" : { \"Type\" : \"AWS::DynamoDB::Table\" , \"Properties\" : { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"leaseKey\" , \"AttributeType\" : \"S\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"leaseKey\" , \"KeyType\" : \"HASH\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"10\" , \"WriteCapacityUnits\" : \"5\" } } }, \"CountsDynamoDBTable\" : { \"Type\" : \"AWS::DynamoDB::Table\" , \"Properties\" : { \"AttributeDefinitions\" : [ { \"AttributeName\" : \"resource\" , \"AttributeType\" : \"S\" }, { \"AttributeName\" : \"timestamp\" , \"AttributeType\" : \"S\" } ], \"KeySchema\" : [ { \"AttributeName\" : \"resource\" , \"KeyType\" : \"HASH\" }, { \"AttributeName\" : \"timestamp\" , \"KeyType\" : \"RANGE\" } ], \"ProvisionedThroughput\" : { \"ReadCapacityUnits\" : \"10\" , \"WriteCapacityUnits\" : \"5\" } } }, \"Ec2SecurityGroup\" : { \"Type\" : \"AWS::EC2::SecurityGroup\" , \"Properties\" : { \"GroupDescription\" : \"Enable SSH access and HTTP access on the inbound port\" , \"SecurityGroupIngress\" : [{ \"IpProtocol\" : \"tcp\" , \"FromPort\" : \"22\" , \"ToPort\" : \"22\" , \"CidrIp\" : { \"Ref\" : \"SSHLocation\" } }, { \"IpProtocol\" : \"tcp\" , \"FromPort\" : \"80\" , \"ToPort\" : \"80\" , \"CidrIp\" : \"0.0.0.0/0\" }] } }, \"EIP\" : { \"Type\" : \"AWS::EC2::EIP\" , \"Properties\" : { \"InstanceId\" : { \"Ref\" : \"Ec2Instance\" } } }, \"RootRole\" : { \"Type\" : \"AWS::IAM::Role\" , \"Properties\" : { \"AssumeRolePolicyDocument\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Principal\" : { \"Service\" : [ \"ec2.amazonaws.com\" ] }, \"Action\" : [ \"sts:AssumeRole\" ] } ] }, \"Path\" : \"/\" } }, \"RolePolicies\" : { \"Type\" : \"AWS::IAM::Policy\" , \"Properties\" : { \"PolicyName\" : \"root\" , \"PolicyDocument\" : { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"kinesis:*\" , \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:kinesis:\" , { \"Ref\" : \"AWS::Region\" }, \":\" , { \"Ref\" : \"AWS::AccountId\" }, \":stream/\" , { \"Ref\" : \"KinesisStream\" } ]]} }, { \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:dynamodb:\" , { \"Ref\" : \"AWS::Region\" }, \":\" , { \"Ref\" : \"AWS::AccountId\" }, \":table/\" , { \"Ref\" : \"KCLDynamoDBTable\" } ]]} }, { \"Effect\" : \"Allow\" , \"Action\" : \"dynamodb:*\" , \"Resource\" : { \"Fn::Join\" : [ \"\" , [ \"arn:aws:dynamodb:\" , { \"Ref\" : \"AWS::Region\" }, \":\" , { \"Ref\" : \"AWS::AccountId\" }, \":table/\" , { \"Ref\" : \"CountsDynamoDBTable\" } ]]} }, { \"Effect\" : \"Allow\" , \"Action\" : \"cloudwatch:*\" , \"Resource\" : \"*\" } ] }, \"Roles\" : [ { \"Ref\" : \"RootRole\" } ] } }, \"RootInstanceProfile\" : { \"Type\" : \"AWS::IAM::InstanceProfile\" , \"Properties\" : { \"Path\" : \"/\" , \"Roles\" : [ { \"Ref\" : \"RootRole\" } ] } }, \"Ec2Instance\" : { \"Type\" : \"AWS::EC2::Instance\" , \"Metadata\" : { \"AWS::CloudFormation::Init\" : { \"config\" : { \"packages\" : { \"yum\" : { \"java-1.7.0-openjdk\" : [] } }, \"files\" : { \"/var/kinesis-data-vis-sample-app/watchdog.sh\" : { \"content\" : { \"Fn::Join\" : [ \"\" , [ \"#!/bin/bash\\n\" , \"if ! ps aux | grep HttpReferrerCounterApplication | grep -v grep ; then\\n\" , \" # Launch the Kinesis application for counting HTTP referrer pairs\\n\" , \" java -cp /var/kinesis-data-vis-sample-app/lib/\\\\* com.amazonaws.services.kinesis.samples.datavis.HttpReferrerCounterApplication \" , { \"Ref\" : \"KCLDynamoDBTable\" }, \" \" , { \"Ref\" : \"KinesisStream\" }, \" \" , { \"Ref\" : \"CountsDynamoDBTable\" }, \" \" , { \"Ref\" : \"AWS::Region\" }, \" &>> /home/ec2-user/kinesis-data-vis-sample-app-kcl.log &\\n\" , \"fi\\n\" , \"if ! ps aux | grep HttpReferrerStreamWriter | grep -v grep ; then\\n\" , \" # Launch our Kinesis stream writer to fill our stream with generated HTTP (resource, referrer) pairs.\\n\" , \" # This will create a writer with 5 threads to send records indefinitely.\\n\" , \" java -cp /var/kinesis-data-vis-sample-app/lib/\\\\* com.amazonaws.services.kinesis.samples.datavis.HttpReferrerStreamWriter 5 \" , { \"Ref\" : \"KinesisStream\" }, \" \" , { \"Ref\" : \"AWS::Region\" }, \" &>> /home/ec2-user/kinesis-data-vis-sample-app-publisher.log &\\n\" , \"fi\\n\" , \"if ! ps aux | grep WebServer | grep -v grep ; then\\n\" , \" # Launch the webserver\\n\" , \" java -cp /var/kinesis-data-vis-sample-app/lib/\\\\* com.amazonaws.services.kinesis.samples.datavis.WebServer 80 /var/kinesis-data-vis-sample-app/wwwroot \" , { \"Ref\" : \"CountsDynamoDBTable\" }, \" \" , { \"Ref\" : \"AWS::Region\" }, \" &>> /home/ec2-user/kinesis-data-vis-sample-app-www.log &\\n\" , \"fi\\n\" ]]}, \"mode\" : \"000755\" , \"owner\" : \"ec2-user\" , \"group\" : \"ec2-user\" }, \"/var/kinesis-data-vis-sample-app/crontask\" : { \"content\" : { \"Fn::Join\" : [ \"\" , [ \"* * * * * bash /var/kinesis-data-vis-sample-app/watchdog.sh\\n\" ]]}, \"mode\" : \"000644\" , \"owner\" : \"ec2-user\" , \"group\" : \"ec2-user\" } }, \"sources\" : { \"/var/kinesis-data-vis-sample-app\" : { \"Ref\" : \"ApplicationArchive\" } } } } }, \"Properties\" : { \"KeyName\" : { \"Fn::If\" : [ \"UseEC2KeyName\" , { \"Ref\" : \"KeyName\" }, { \"Ref\" : \"AWS::NoValue\" } ]}, \"ImageId\" : { \"Fn::FindInMap\" : [ \"AWSRegionArch2AMI\" , { \"Ref\" : \"AWS::Region\" }, { \"Fn::FindInMap\" : [ \"AWSInstanceType2Arch\" , { \"Ref\" : \"InstanceType\" }, \"Arch\" ] } ] }, \"InstanceType\" : { \"Ref\" : \"InstanceType\" }, \"SecurityGroups\" : [{ \"Ref\" : \"Ec2SecurityGroup\" }], \"IamInstanceProfile\" : { \"Ref\" : \"RootInstanceProfile\" }, \"UserData\" : { \"Fn::Base64\" : { \"Fn::Join\" : [ \"\" , [ \"#!/bin/bash\\n\" , \"yum update -y aws-cfn-bootstrap\\n\" , \"/opt/aws/bin/cfn-init -s \" , { \"Ref\" : \"AWS::StackId\" }, \" -r Ec2Instance \" , \" --region \" , { \"Ref\" : \"AWS::Region\" }, \"\\n\" , \"# Register watchdog script with cron\\n\" , \"crontab /var/kinesis-data-vis-sample-app/crontask\\n\" , \"# Launch watchdog script immediately so if it fails this stack fails to start\\n\" , \"/var/kinesis-data-vis-sample-app/watchdog.sh\\n\" , \"/opt/aws/bin/cfn-signal -e $? '\" , { \"Ref\" : \"WaitHandle\" }, \"'\\n\" ]]}} } }, \"WaitHandle\" : { \"Type\" : \"AWS::CloudFormation::WaitConditionHandle\" }, \"WaitCondition\" : { \"Type\" : \"AWS::CloudFormation::WaitCondition\" , \"DependsOn\" : \"Ec2Instance\" , \"Properties\" : { \"Handle\" : { \"Ref\" : \"WaitHandle\" }, \"Timeout\" : \"600\" } } }, \"Outputs\" : { \"URL\" : { \"Description\" : \"URL to the sample application's visualization\" , \"Value\" : { \"Fn::Join\" : [ \"\" , [ \"http://\" , { \"Fn::GetAtt\" : [ \"Ec2Instance\" , \"PublicDnsName\" ] }]]} }, \"InstanceId\" : { \"Description\" : \"InstanceId of the newly created EC2 instance\" , \"Value\" : { \"Ref\" : \"Ec2Instance\" } }, \"AZ\" : { \"Description\" : \"Availability Zone of the newly created EC2 instance\" , \"Value\" : { \"Fn::GetAtt\" : [ \"Ec2Instance\" , \"AvailabilityZone\" ] } }, \"StreamName\" : { \"Description\" : \"The name of the Kinesis Stream. This was autogenerated by the Kinesis Resource named 'KinesisStream'\" , \"Value\" : { \"Ref\" : \"KinesisStream\" } }, \"ApplicationName\" : { \"Description\" : \"The name of the Kinesis Client Application. This was autogenerated by the DynamoDB Resource named 'KCLDynamoDBTable'\" , \"Value\" : { \"Ref\" : \"KCLDynamoDBTable\" } }, \"CountsTable\" : { \"Description\" : \"The name of the DynamoDB table where counts are persisted. This was autogenerated by the DynamoDB Resource named 'CountsDynamoDBTable'\" , \"Value\" : { \"Ref\" : \"CountsDynamoDBTable\" } } } }","title":"Sample CloudFormation Deployment with Kinesis"},{"location":"aws/kms/","text":"AWS KMS AWS Key Management Service is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. AWS KMS is integrated with other services including EBS, S3, Redshift, Elastic Transcoder, WorkMail, RDS & others to make it simple to encrypt your data with encryption keys that you manage. Encryption keys in AWS are regional (see IAM -> Encryption Keys to set region) The Customer Master Key (CMK): alias creation dataset description key state key material (either customer provided or AWS provided) Can never be exported Setup a CMK: Create Alias & description Choose material option\u2026 Define key Administrative permissions IAM users/roles that can administer (but not use) the key through the KMS API Define key Usage permissions IAM users/roles that can use the key to encrypt and decrypt data Key material options: Use KMS generated key material Your own key material KMS API Calls Launch an EC2 instance (Create Key/Pair) Click \u2018Encryption Keys\u2019 (IAM) -> Key Users Users -> Encryptors Name -> Access Key -> Create access key (show secret access Key) -> copy/pasta Grab public IP of EC2 To the CLI! ssh ec2-user@public-ip-address -i MyKeyName.pem sudo su echo \"hello world\" > secret.txt ls cat secret.txt aws configure [pass in AWS Access Key ID] [pass in AWS secret access key] [default region name - for KMS] [default output format - default] aws kms encrypt --key-id YOURKEYIDHERE --plaintext fileb://secret.txt --output text --query CiphertextBlob | base64 --decode > encryptedsecret.txt cat encryptedsecret.txt aws kms decrypt --ciphertext-blob fileb://encryptedsecret.txt --output text --query Plaintext | base64 --decode > decryptedsecret.txt cat decryptedsecret.txt aws kms re-encrypt --destination-key-id YOURKEYIDHERE --ciphertext-blob fileb://encryptedsecret.txt | base64 > newencryption.txt cat newencryption.txt # will rotate the key every year aws kms enable-key-rotation --key-id YOURKEYIDHERE Note: * Need to disable key & scheduled key deletion (min 1 week) Exam Tips aws kms encrypt aws kms decrypt aws kms re-encrypt aws kms enable-key-rotation KMS Envelope Encryption The process of encrypting your Envelop (Data) key (the key used to decrypt your data) Customer Master Key used to decrypt the data key (envelope key) Envelope key is used to decrypt the data Envelope Encryption Decryption Process Exam Tips The Customer Master key: Used to decrypt the data key (envelope key) Envelope key is used to decrypt the data Note: Cloud HSM is dedicated hardware.","title":"KMS"},{"location":"aws/kms/#aws-kms","text":"AWS Key Management Service is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. AWS KMS is integrated with other services including EBS, S3, Redshift, Elastic Transcoder, WorkMail, RDS & others to make it simple to encrypt your data with encryption keys that you manage. Encryption keys in AWS are regional (see IAM -> Encryption Keys to set region)","title":"AWS KMS"},{"location":"aws/kms/#the-customer-master-key-cmk","text":"alias creation dataset description key state key material (either customer provided or AWS provided) Can never be exported","title":"The Customer Master Key (CMK):"},{"location":"aws/kms/#setup-a-cmk","text":"Create Alias & description Choose material option\u2026 Define key Administrative permissions IAM users/roles that can administer (but not use) the key through the KMS API Define key Usage permissions IAM users/roles that can use the key to encrypt and decrypt data","title":"Setup a CMK:"},{"location":"aws/kms/#key-material-options","text":"Use KMS generated key material Your own key material","title":"Key material options:"},{"location":"aws/kms/#kms-api-calls","text":"Launch an EC2 instance (Create Key/Pair) Click \u2018Encryption Keys\u2019 (IAM) -> Key Users Users -> Encryptors Name -> Access Key -> Create access key (show secret access Key) -> copy/pasta Grab public IP of EC2 To the CLI! ssh ec2-user@public-ip-address -i MyKeyName.pem sudo su echo \"hello world\" > secret.txt ls cat secret.txt aws configure [pass in AWS Access Key ID] [pass in AWS secret access key] [default region name - for KMS] [default output format - default] aws kms encrypt --key-id YOURKEYIDHERE --plaintext fileb://secret.txt --output text --query CiphertextBlob | base64 --decode > encryptedsecret.txt cat encryptedsecret.txt aws kms decrypt --ciphertext-blob fileb://encryptedsecret.txt --output text --query Plaintext | base64 --decode > decryptedsecret.txt cat decryptedsecret.txt aws kms re-encrypt --destination-key-id YOURKEYIDHERE --ciphertext-blob fileb://encryptedsecret.txt | base64 > newencryption.txt cat newencryption.txt # will rotate the key every year aws kms enable-key-rotation --key-id YOURKEYIDHERE Note: * Need to disable key & scheduled key deletion (min 1 week)","title":"KMS API Calls"},{"location":"aws/kms/#exam-tips","text":"aws kms encrypt aws kms decrypt aws kms re-encrypt aws kms enable-key-rotation","title":"Exam Tips"},{"location":"aws/kms/#kms-envelope-encryption","text":"The process of encrypting your Envelop (Data) key (the key used to decrypt your data) Customer Master Key used to decrypt the data key (envelope key) Envelope key is used to decrypt the data","title":"KMS Envelope Encryption"},{"location":"aws/kms/#envelope-encryption","text":"","title":"Envelope Encryption"},{"location":"aws/kms/#decryption-process","text":"","title":"Decryption Process"},{"location":"aws/kms/#exam-tips_1","text":"The Customer Master key: Used to decrypt the data key (envelope key) Envelope key is used to decrypt the data Note: Cloud HSM is dedicated hardware.","title":"Exam Tips"},{"location":"aws/lambda/","text":"AWS Lambda Data Centres Hardware Assembly Code/Protocols High Level Languages Operating Systems Application Layer/AWS APIs AWS Lambda AWS Lambda is a compute service where you can upload your code and create a Lambda function. AWS Lambda takes care of provisioning and managing the servers that you use to run the code. You don\u2019t have to worry about operating systems, patching, scaling etc. You can use lambda in the following ways: As an event-driven compute service where AWS Lambda runs your code in response to events. These events could be changes to data in an Amazon S3 bucket or an Amazon DynamoDB table. As a compute service to run your code in response to HTTP requests using Amazon API Gateway or API calls made using AWS SDKs What Languages? Node.js Java Python C# Go How Priced? Number of requests First 1 million requests are free. $0.20 per 1 million requests thereafter. Duration Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. The price depends on the amount of memory you allocate to your function. You are charged $0.00001667 for every GB-second used., Why is Lambda Cool? No servers Continuous Scaling Super cheap Alexa (Amazon echo) uses Lambda Exam Tips Lambda scales out (not up) automatically Lambda functions are independent, 1 event = 1 function Lambda is serverless / a compute service Know what services are serverless (S3 / DynamoDB / API Gateway / Lambda) RDS & EC2 are not serverless (except potentially Aurora) Lambada functions can trigger other lambda functions, 1 event can = x functions if functions trigger other functions Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening. Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets etc. Know your triggers Version Control with Lambda When you use versioning in AWS Lambda, you can publish one or more versions of your Lambda function. As a result, you can work with different variations of your Lambda function in your dev workflow, such a dev, beta, prod. Each Lambda function version has a unique Amazon Resource Name (ARN). After you publish a version, it is immutable. AWS Lambda maintains your latest function code in the $LATEST version. When you update your function code, AWS Lambda replaces the code in the $LATEST version of the Lambda function. Qualified/Unqualified ARNs You can refer to this function using its Amazon Resource Name. There are two ARNs associated with this initial version: Qualified ARN - The function ARN with the version suffix. arn:aws:lambda:aws-region:acct-id:function:helloworld:$LATEST Unqualified ARN - The function ARN without the version suffix. arn:aws:lambda:aws-region:acct-id:function:helloworld Alias After initially creating a Lambda function (the $LATEST version), you can publish a version 1 of it. By creating an alias named PROD that points to version 1, you can now use the PROD alias to invoke version 1 of the Lambda function. Now, you can update the code (the $LATEST version) with all of your improvements, and then publish another stable and improved version (version 2). You can promote version 2 to production by remapping the PROD alias so that it points to version 2. If you find something wrong, you can easily roll back the production version to version 1 by remapping the PROD alias so that it points to version 1. Note: you can shift traffic between Alias using UI and defining the % weights. (can\u2019t use this with $LATEST) Exam Tips Can have multiple versions of lambda functions Latest version will use $LATEST Qualified version will use $latest, unqualified will not have it Versions are immutable Can split traffic using aliases to different versions Cannot split traffic with $latest, instead create an alias to latest","title":"Lambda"},{"location":"aws/lambda/#aws-lambda","text":"Data Centres Hardware Assembly Code/Protocols High Level Languages Operating Systems Application Layer/AWS APIs AWS Lambda AWS Lambda is a compute service where you can upload your code and create a Lambda function. AWS Lambda takes care of provisioning and managing the servers that you use to run the code. You don\u2019t have to worry about operating systems, patching, scaling etc. You can use lambda in the following ways: As an event-driven compute service where AWS Lambda runs your code in response to events. These events could be changes to data in an Amazon S3 bucket or an Amazon DynamoDB table. As a compute service to run your code in response to HTTP requests using Amazon API Gateway or API calls made using AWS SDKs","title":"AWS Lambda"},{"location":"aws/lambda/#what-languages","text":"Node.js Java Python C# Go","title":"What Languages?"},{"location":"aws/lambda/#how-priced","text":"Number of requests First 1 million requests are free. $0.20 per 1 million requests thereafter. Duration Duration is calculated from the time your code begins executing until it returns or otherwise terminates, rounded up to the nearest 100ms. The price depends on the amount of memory you allocate to your function. You are charged $0.00001667 for every GB-second used.,","title":"How Priced?"},{"location":"aws/lambda/#why-is-lambda-cool","text":"No servers Continuous Scaling Super cheap Alexa (Amazon echo) uses Lambda","title":"Why is Lambda Cool?"},{"location":"aws/lambda/#exam-tips","text":"Lambda scales out (not up) automatically Lambda functions are independent, 1 event = 1 function Lambda is serverless / a compute service Know what services are serverless (S3 / DynamoDB / API Gateway / Lambda) RDS & EC2 are not serverless (except potentially Aurora) Lambada functions can trigger other lambda functions, 1 event can = x functions if functions trigger other functions Architectures can get extremely complicated, AWS X-ray allows you to debug what is happening. Lambda can do things globally, you can use it to back up S3 buckets to other S3 buckets etc. Know your triggers","title":"Exam Tips"},{"location":"aws/lambda/#version-control-with-lambda","text":"When you use versioning in AWS Lambda, you can publish one or more versions of your Lambda function. As a result, you can work with different variations of your Lambda function in your dev workflow, such a dev, beta, prod. Each Lambda function version has a unique Amazon Resource Name (ARN). After you publish a version, it is immutable. AWS Lambda maintains your latest function code in the $LATEST version. When you update your function code, AWS Lambda replaces the code in the $LATEST version of the Lambda function.","title":"Version Control with Lambda"},{"location":"aws/lambda/#qualifiedunqualified-arns","text":"You can refer to this function using its Amazon Resource Name. There are two ARNs associated with this initial version: Qualified ARN - The function ARN with the version suffix. arn:aws:lambda:aws-region:acct-id:function:helloworld:$LATEST Unqualified ARN - The function ARN without the version suffix. arn:aws:lambda:aws-region:acct-id:function:helloworld","title":"Qualified/Unqualified ARNs"},{"location":"aws/lambda/#alias","text":"After initially creating a Lambda function (the $LATEST version), you can publish a version 1 of it. By creating an alias named PROD that points to version 1, you can now use the PROD alias to invoke version 1 of the Lambda function. Now, you can update the code (the $LATEST version) with all of your improvements, and then publish another stable and improved version (version 2). You can promote version 2 to production by remapping the PROD alias so that it points to version 2. If you find something wrong, you can easily roll back the production version to version 1 by remapping the PROD alias so that it points to version 1. Note: you can shift traffic between Alias using UI and defining the % weights. (can\u2019t use this with $LATEST)","title":"Alias"},{"location":"aws/lambda/#exam-tips_1","text":"Can have multiple versions of lambda functions Latest version will use $LATEST Qualified version will use $latest, unqualified will not have it Versions are immutable Can split traffic using aliases to different versions Cannot split traffic with $latest, instead create an alias to latest","title":"Exam Tips"},{"location":"aws/r53/","text":"AWS Route 53 Amazons DNS Service Allows you to map domain names to: EC2 instances Load balancers S3 buckets Lab Notes Register Domain on Route 53 (using namecheap use prefix.domain.com and match the namespace values) Create an A record set for the route 53 value. Use alias - clicking on Alias Target will provide dropdown where any pre-defined ELB/ALBs will appear You can setup a Load Balancer while in the EC2 dashboard - along left hand side you will see \u2018Load Balancing\u2019 Once setup you can go to the domain and should be directed to your server","title":"Route 53"},{"location":"aws/r53/#aws-route-53","text":"","title":"AWS Route 53"},{"location":"aws/r53/#amazons-dns-service","text":"Allows you to map domain names to: EC2 instances Load balancers S3 buckets","title":"Amazons DNS Service"},{"location":"aws/r53/#lab-notes","text":"Register Domain on Route 53 (using namecheap use prefix.domain.com and match the namespace values) Create an A record set for the route 53 value. Use alias - clicking on Alias Target will provide dropdown where any pre-defined ELB/ALBs will appear You can setup a Load Balancer while in the EC2 dashboard - along left hand side you will see \u2018Load Balancing\u2019 Once setup you can go to the domain and should be directed to your server","title":"Lab Notes"},{"location":"aws/rds/","text":"AWS Relational Database Service (RDS) Think traditional spreadsheet: Database Tables Row Fields (Columns) Relational Database Types (with RDS) SQL Server Oracle MySQL Server PostgresSQL Aurora (amazons DB) MariaDB Non-Relational Databases Database Collection - Table Document - Row Key Value Pairs - Fields Data Warehousing? Used for business intelligence, tools like Cognos, Jaspersoft, SQL Server, Reporting Services, Oracle Hyperion, and SAP NetWeaver Used to pull in very large and complex data sets. Usually used by Mgmt to do queries on data (such as current performance v targets, etc.) OLTP -v- OLAP Online Transaction Porcessing (OLTP) differs from Online Analytical Processing in terms of the types of queries you will run OLTP Example : Order number 2120121 - pulls up a row of data such as name, date, address to devlier to, delivery status etc. OLAP Example : Net profit for EMEA and Pacific for the Digital Radio Product. Pulls in large number of records Data warehousing DBs use different types of architecture both from a DB perspective and infrastructure layer. Elasticache A web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud Service improves the performance of web apps by allowign you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based DBs Elasticache supports two open-source in-memory caching engines: Memcached Redis Database Types - Summary: RDS - OLTP SQL MySQL PostgreSQL Oracle Aurora MariaDB No SQL DynamoDB OLAP Redshift Elasticache - In Memory Caching Memcached Redis RDS Lab Setup RDS through UI (MySQL - free tier) Create EC2 instance - Step 3 - Configure Instance Details - Include shell bootstrap script #!/bin/bash yum install httpd php php-mysql -y yum update -y chkconfig httpd on service httpd start echo \"<?php phpinfo();?>\" > /var/www/html/index.php cd var/www/html wget https://s3.eu-west-1.amazonaws.com/experimenting-aws/connect.php In s3 instance file will look like this (copy pasta the relevant value in here beforehand): <?php $username = \"sampledb\" ; $password = \"sampledb\" ; $hostname = \"\" ; // extracted from RDS dashboard connect name $dbname = \"sampledb\" ; //connection to the database $dbhandle = mysql_connect ( $hostname , $username , $password ) or die ( \"Unable to connect to MySQL\" ); echo \"Connected to MySQL using username - $username , password - $password , host - $hostname <br>\" ; $selected = mysql_select_db ( \" $dbname \" , $dbhandle ) or die ( \"Unable to connect to MySQL DB - check the database name and try again.\" ); ?> Always given a DNS address (not given a public IPv4 Address) Note once EC2 starts running you can go to the public IP address and should see the PHP server running. SSH into EC2 instance (can use public ip address) sudo su cd /var/www/html ls (should see 2 files from bootstrap in here) Go to http://34.247.73.41/connect.php Next - need to allow RDS access\u2026. Go to RDS via console Go to security groups (click first option) Click Inbound -> Add Rule MySQL/Aurora, TCP, 3306 - use security group of your EC2 instance (type sg it will auto complete available options) Go back to http://34.247.73.41/connect.php and you should see a \u2018connected to mysql\u2026\u2026.\u2019 message - success! Common Troubleshooting - RDS & EC2 in different security groups. Open port 3306 to the security group the RDS instance is in, open up mysql and allow to the security group your EC2 instance is in so the two can talk. Automated Backups Automated will allow you to recover any point within a \u2018retention period\u2019 Recover will choose most recent daily backup and apply those transaction logs (point in time recovery down to a second within the retention period (1-365 days)) Enabled by default - stored in S3 & free storage space equal to size of DB. Taken within a defined window, during window storage I/O may be suspended while data is being backup -> may experience some low-latency Snapshots User initiated -> stored even after deletion of original RDS instance. Whenever restored, there will be a new RDS instance with a new DNS endpoint. Encryption Encryption at rest is supported for MySQL, Oracle, SQL Server, PostgreSQL, MariaDB & Aurora. Done using AWS KMS service. Once RDS is encrypted data stored arest in underlying storage is encrypted as well as its automated backups, read replicas & snapshots. Currently encrypting existing DB instance isn\u2019t supported. Multi-AZ (disaster recovery) synchronous Allows exact copy of prod DB in another availability zone. AWS handles replication, so when your prod DB is written to, the write will auto sync to the stand by DB. In event of planned DB maintenance, DB failure or Availability one failure, RDS will automatically failover to the standby so DB operations can resume without admin intervention. Used only for disaster recovery -> not used for improving performance. Available on: SQL Server Oracle MySQL Server PostgreSQL MariaDB Aurora (enabled by default) Read Replicas (performance improvements) asynchronous Used for scaling not disaster recovery. Must have auto backups turned on in order to deploy a read replicas You can have up to 5 read replica copies of any DB You can have read replicas of replicas (watch latency) Each read replica will have its own DNS endpoint You can have read replicas that have Multi-AZ You can create read replicas of Multi-AZ source DBs Read replicas can be promoted to be their own DBs, this breaks the replication. You can have a read replica in a second region. Available on: MySQL PostgreSQL MariaDB Aurora","title":"RDS"},{"location":"aws/rds/#aws-relational-database-service-rds","text":"Think traditional spreadsheet: Database Tables Row Fields (Columns)","title":"AWS Relational Database Service (RDS)"},{"location":"aws/rds/#relational-database-types-with-rds","text":"SQL Server Oracle MySQL Server PostgresSQL Aurora (amazons DB) MariaDB","title":"Relational Database Types (with RDS)"},{"location":"aws/rds/#non-relational-databases","text":"Database Collection - Table Document - Row Key Value Pairs - Fields","title":"Non-Relational Databases"},{"location":"aws/rds/#data-warehousing","text":"Used for business intelligence, tools like Cognos, Jaspersoft, SQL Server, Reporting Services, Oracle Hyperion, and SAP NetWeaver Used to pull in very large and complex data sets. Usually used by Mgmt to do queries on data (such as current performance v targets, etc.) OLTP -v- OLAP Online Transaction Porcessing (OLTP) differs from Online Analytical Processing in terms of the types of queries you will run OLTP Example : Order number 2120121 - pulls up a row of data such as name, date, address to devlier to, delivery status etc. OLAP Example : Net profit for EMEA and Pacific for the Digital Radio Product. Pulls in large number of records Data warehousing DBs use different types of architecture both from a DB perspective and infrastructure layer.","title":"Data Warehousing?"},{"location":"aws/rds/#elasticache","text":"A web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud Service improves the performance of web apps by allowign you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based DBs Elasticache supports two open-source in-memory caching engines: Memcached Redis","title":"Elasticache"},{"location":"aws/rds/#database-types-summary","text":"RDS - OLTP SQL MySQL PostgreSQL Oracle Aurora MariaDB No SQL DynamoDB OLAP Redshift Elasticache - In Memory Caching Memcached Redis","title":"Database Types - Summary:"},{"location":"aws/rds/#rds-lab","text":"Setup RDS through UI (MySQL - free tier) Create EC2 instance - Step 3 - Configure Instance Details - Include shell bootstrap script #!/bin/bash yum install httpd php php-mysql -y yum update -y chkconfig httpd on service httpd start echo \"<?php phpinfo();?>\" > /var/www/html/index.php cd var/www/html wget https://s3.eu-west-1.amazonaws.com/experimenting-aws/connect.php In s3 instance file will look like this (copy pasta the relevant value in here beforehand): <?php $username = \"sampledb\" ; $password = \"sampledb\" ; $hostname = \"\" ; // extracted from RDS dashboard connect name $dbname = \"sampledb\" ; //connection to the database $dbhandle = mysql_connect ( $hostname , $username , $password ) or die ( \"Unable to connect to MySQL\" ); echo \"Connected to MySQL using username - $username , password - $password , host - $hostname <br>\" ; $selected = mysql_select_db ( \" $dbname \" , $dbhandle ) or die ( \"Unable to connect to MySQL DB - check the database name and try again.\" ); ?> Always given a DNS address (not given a public IPv4 Address) Note once EC2 starts running you can go to the public IP address and should see the PHP server running. SSH into EC2 instance (can use public ip address) sudo su cd /var/www/html ls (should see 2 files from bootstrap in here) Go to http://34.247.73.41/connect.php Next - need to allow RDS access\u2026. Go to RDS via console Go to security groups (click first option) Click Inbound -> Add Rule MySQL/Aurora, TCP, 3306 - use security group of your EC2 instance (type sg it will auto complete available options) Go back to http://34.247.73.41/connect.php and you should see a \u2018connected to mysql\u2026\u2026.\u2019 message - success! Common Troubleshooting - RDS & EC2 in different security groups. Open port 3306 to the security group the RDS instance is in, open up mysql and allow to the security group your EC2 instance is in so the two can talk.","title":"RDS Lab"},{"location":"aws/rds/#automated-backups","text":"Automated will allow you to recover any point within a \u2018retention period\u2019 Recover will choose most recent daily backup and apply those transaction logs (point in time recovery down to a second within the retention period (1-365 days)) Enabled by default - stored in S3 & free storage space equal to size of DB. Taken within a defined window, during window storage I/O may be suspended while data is being backup -> may experience some low-latency","title":"Automated Backups"},{"location":"aws/rds/#snapshots","text":"User initiated -> stored even after deletion of original RDS instance. Whenever restored, there will be a new RDS instance with a new DNS endpoint.","title":"Snapshots"},{"location":"aws/rds/#encryption","text":"Encryption at rest is supported for MySQL, Oracle, SQL Server, PostgreSQL, MariaDB & Aurora. Done using AWS KMS service. Once RDS is encrypted data stored arest in underlying storage is encrypted as well as its automated backups, read replicas & snapshots. Currently encrypting existing DB instance isn\u2019t supported.","title":"Encryption"},{"location":"aws/rds/#multi-az-disaster-recovery","text":"synchronous Allows exact copy of prod DB in another availability zone. AWS handles replication, so when your prod DB is written to, the write will auto sync to the stand by DB. In event of planned DB maintenance, DB failure or Availability one failure, RDS will automatically failover to the standby so DB operations can resume without admin intervention. Used only for disaster recovery -> not used for improving performance. Available on: SQL Server Oracle MySQL Server PostgreSQL MariaDB Aurora (enabled by default)","title":"Multi-AZ (disaster recovery)"},{"location":"aws/rds/#read-replicas-performance-improvements","text":"asynchronous Used for scaling not disaster recovery. Must have auto backups turned on in order to deploy a read replicas You can have up to 5 read replica copies of any DB You can have read replicas of replicas (watch latency) Each read replica will have its own DNS endpoint You can have read replicas that have Multi-AZ You can create read replicas of Multi-AZ source DBs Read replicas can be promoted to be their own DBs, this breaks the replication. You can have a read replica in a second region. Available on: MySQL PostgreSQL MariaDB Aurora","title":"Read Replicas (performance improvements)"},{"location":"aws/s3/","text":"AWS S3 Simple. Storage. Service. S3 provides developers and IT teams with secure, durable, highly-scalable object storage . Amazon S3 is easy to use with a simple web services interface to store and retrieve any amount of data from anywhere on the web. S3 is a safe place to store your files. It is Object-based storage. The data is spread across multiple devices and facilities. (highly available) Basics S3 is Object-based - allows you to upload files. Files can be from 0 Bytes to 5 TB. There is unlimited storage. Files are stored in buckets (similar to a folder) S3 is a universal namespace. That is, names must be unique globally. Example: https://s3-eu-west-1.amazonaws.com/acloudguru When you upload a file to S3, you will receive a HTTP 200 code if upload was successful. Data Consistency Model for S3 Read after Write consistency for PUTS of new Objects (once object added to S3, the file is available to read) Eventual Consistent to overwrite PUTs and DELETEs (can take some time to propagate) S3 is a Simple Key-Value Store * S3 is Object based. Objects consist of the following: * Key (This is simply the name of the Object) * Value (This is simply the data, which is made up of a sequence of bytes). * Version ID (important for versioning) * Metadata (Data about data you are storing) * Subresources - bucket-specific configuration: * Bucket Policies, Access Control Lists * Cross Origin Resource Sharing (CORS) * Transfer Acceleration S3 - The Basics Built for 99.99% availability for the S3 platform. Amazon Guarantee 99.9% availability Amazon guarantees 99.999999999% (11x9s) durability for S3 information. Tiered Storage Available Lifecycle Management Versioning Encryption Secure your data - Access Control Lists & Bucket Policies S3 - Storage Tiers/Classes S3 99.99% availability, 11x9s durability, stored redundantly across multiple devices in multiple facilities, and is designed to sustain the loss of 2 facilities concurrently. S3 - IA (Infrequently Accessed): For data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee. S3 - One Zone IA: Same as IA however data is stored in a single Availability Zone only, still 11x9s durability, but only 99.5% availability. Cost is 20% less than regular S3-IA. Reduced Redundancy Storage: Designed to provide 99.99% durability and 99.99% availability of objects over a given year. Used for data that can be recreated if lost, e.g. thumbnails (Starting to disappear from AWS docs but may still feature in exam) - Standard cost now more effective than using this option. Glacier: Very cheap, but used for archival only. Optimised for data that is infrequently accessed and it takes 3-5 hours to restore from Glacier. S3 - Intelligent Tiering (re-invent 2018) Unknown or unpredictable access patterns 2 tiers - frequent & infrequent access Automatically moves your data to most cost-effective tier based on how frequently you access each object - If object is not accessed for 30 consecutive days moved to infrequent access, but if it is used moved to frequent access. 11x9s durability 99,9% availability over a given year. Optimizes Cost No fees for accessing your data but a small monthly fee for monitoring/automation $0.0025 per 1,000 objects. S3 - Charges Storage per GB Requests (Get, Put, Copy, etc.) Storage Management Pricing Inventory, Analytics, Object Tags Data Management Pricing Data Transferred out of S3 Transfer Acceleration Use of CloudFront to optimize transfers S3 Exam Tips Remember that S3 is Object-based i.e. allows you to upload files. Object-based storage only (for files.) Not suitable to install an OS or running a DB on. Files can be from 0 Bytes to 5 TB. There is unlimited storage. Files are stored in Buckets. S3 is a universal namespace. That is, names must be unique globally. Example: https://s3-eu-west-1.amazonaws.com/acloudguru Read after Write consistency for PUTS of new Objects Eventual Consistency for overwrite PUTS and DELETES (can take some time to propagate) S3 Storage Classes/Tiers S3 [durable, immediately available, frequency accessed] S3 - IA [durable, immediately available, infrequency accessed] S3 - One Zone IA [same as IA however stored in single Availability Zone] S3 Reduced Redundancy Storage [data that is easily reproducible i.e. thumbnails] Glacier [Archived data, where you can wait 3-5 hours before accessing] Core fundamentals of S3 Object: Key (name) Value (data) Version ID Metadata Subresources - bucket-specific config: Bucket Policies, Access Control Lists Cross Origin Resource Sharing (CORS) Transfer Acceleration Successful uploads will generate HTTP 200 status code - when you use CLI/API Make sure you read the S3 FAQ: https://aws.amazon.com/s3/faqs/ S3 Security Securing Your Buckets By default, all newly created buckets are PRIVATE. You can set up access control to your buckets using: Bucket Policies - Applied at a bucket level. (written in JSON) Access Control Lists - Applied at an object level. S3 buckets can be configured to create access logs, which log all requests made to the s3 bucket. These logs can be written to another bucket. S3 Encryption Types of Encryption In Transit: SSL/TLS At Rest: Server Side Encryption: S3 Managed Keys - SSE-S3 [AES-256] AWS Key Management Service, Managed Keys, SSE-KMS Server Side Encryption with Customer Provided Keys - SSE-C Client Side Encryption Enforcing Encryption on S3 Buckets Every time a file is uploaded to S3, a PUT request is initiated. This is what a PUT request looks like: PUT /myFileHTTP/1.1 Host: myBucket.s3.amazonaws.com Date: Wed, 25 Apr 2018 09 :50:00 GMT Authorization: auth string Content-Type: text/plain Content-Length: 26880 x-amz-meta-author: Dan Expect: 100 -continue [ 26880 bytes of object data ] If the file is to be encrypted at upload time, the x-amz-server-side-encryption-parameter will be included in the request header Two options are currently available: x-amz-server-side-encryption: AES256 (SSE-S3 - S3 managed keys) x-amz-server-side-encryption:ams:kms (SSE-KMS - KMS managed keys) When this parameter is included in the header of the PUT request, it tells S3 to encrypt the object at the time of upload, using the specified encryption method. You can enforce the use of Server Side Encryption by using a Bucket Policy which denies any S3 PUT request which doesn\u2019t include the x-amz-server-side-encryption parameter in the request header. The following request tells S3 to encrypt the file using SSE-S3 (AES 256) at the time of upload PUT /myFileHTTP/1.1 Host: myBucket.s3.amazonaws.com Date: Wed, 25 Apr 2018 09 :50:00 GMT Authorization: auth string Content-Type: text/plain Content-Length: 26880 x-amz-meta-author: Dan Expect: 100 -continue x-amz-server-side-encrpytion: AES256 [ 26880 bytes of object data ] Encryption Exam Tips Encryption In-Transit SSL/TLS (HTTPS) Encryption At Rest Server Side Encryption SSE-S3 SSE-KMS SSE-C Client Side Encryption (encrypt locally before uploading) If you want to enforce use of encryption for files stored in S3, use an S3 Bucket Policy to deny all PUT requests that don\u2019t include the x-amz-server-side-encryption parameter in the request header. CORS (Cross Origin Request Sharing) A way of allowing code that is in one S3 bucket to access/reference code in another S3 bucket. Allowing 1 resource to access another resource - hence CORS. Useful for the \u2018static website hosting\u2019 property of S3. Think - images referenced from another bucket on your site (when both s3 are public accessible). Go to the bucket containing site HTML -> Properties -> Endpoint. (copy) Go to external s3 Bucket -> Permissions -> CORS Configuration This will provide the below default config - you will need to include your copied endpoint instead of \u2018asterisk\u2019 character\u2019: < CORSConfiguration > < CORSRule > < AllowedOrigin > * </ AllowedOrigin > < AllowedMethod > GET </ AllowedMethod > < MaxAgeSeconds > 3000 </ MaxAgeSeconds > < AllowedHeader > Authorization </ AllowedHeader > </ CORSRule > </ CORSConfiguration > S3 Performance Optimization S3 is designed to support very high request rates. however if your S3 buckets are routinely receiving > 100 PUT/LIST/DELETE or >300 GET requests per second, then there are best practice guidelines to optimize. The guidance is based on the type of workload you are running: GET-Intensive Workloads - use CloudFront content delivery service to get best performance. CloudFront will cache your most frequently accessed objects and will reduce latency for your GET requests. Note: see update 2018 below - this is no longer a major concern. * Mixed Request Type Workloads - a mix of GET, PUT, DELETE, GET Bucket - they key names you use for your objects can impact performance for intensive workloads. * S3 uses the key name to determine which partition an object will be stored in. * The use of sequential key names e.g. names prefixed with a time stamp or alphabetical sequence increases the likelihood of having multiple objects stored on the same partition * For heavy workloads this can cause I/O issues and contention * By using a random prefix to key names, you can force S3 to distribute your keys across multiple partitions, distributing the I/O workload. Key Name Example The following Sequential Key Names are not optimal (likely stored on same partition): mybucket/date/custnum/photo1.jpg mybucket/date/custnum/photo2.jpg * mybucket/date/custnum/photo3.jpg Note: see update 2018 below - this is no longer a major concern. For optimal performance, introduce some randomness into the key name e.g. prefix with 4-character hexadecimal hash. mybucket/6ef8-date/custnum/photo1.jpg mybucket/h35d-date/custnum/photo2.jpg * mybucket/7eg4-date/custnum/photo3.jpg Update (2018) In July 2018, Amazon Announced massive increase in S3 performance that they can support 3,500 PUT requests per second 5,500 GET requests per second This new increased performances negates previous guidance to randomize your object key names to achieve faster performance. This means logical and sequential naming patterns can now be used without any performance implication. S3 Optimization Exam Tips Remember 2 main approaches to Performance Optimization for S3: Get-Intensive -> CloudFront Mixed-Workloads -> Avoid sequential key names (maybe prefix with hex hash - avoids same partition) Read S3 FAQ -> https://aws.amazon.com/s3/faqs/","title":"S3"},{"location":"aws/s3/#aws-s3","text":"Simple. Storage. Service. S3 provides developers and IT teams with secure, durable, highly-scalable object storage . Amazon S3 is easy to use with a simple web services interface to store and retrieve any amount of data from anywhere on the web. S3 is a safe place to store your files. It is Object-based storage. The data is spread across multiple devices and facilities. (highly available)","title":"AWS S3"},{"location":"aws/s3/#basics","text":"S3 is Object-based - allows you to upload files. Files can be from 0 Bytes to 5 TB. There is unlimited storage. Files are stored in buckets (similar to a folder) S3 is a universal namespace. That is, names must be unique globally. Example: https://s3-eu-west-1.amazonaws.com/acloudguru When you upload a file to S3, you will receive a HTTP 200 code if upload was successful.","title":"Basics"},{"location":"aws/s3/#data-consistency-model-for-s3","text":"Read after Write consistency for PUTS of new Objects (once object added to S3, the file is available to read) Eventual Consistent to overwrite PUTs and DELETEs (can take some time to propagate) S3 is a Simple Key-Value Store * S3 is Object based. Objects consist of the following: * Key (This is simply the name of the Object) * Value (This is simply the data, which is made up of a sequence of bytes). * Version ID (important for versioning) * Metadata (Data about data you are storing) * Subresources - bucket-specific configuration: * Bucket Policies, Access Control Lists * Cross Origin Resource Sharing (CORS) * Transfer Acceleration","title":"Data Consistency Model for S3"},{"location":"aws/s3/#s3-the-basics","text":"Built for 99.99% availability for the S3 platform. Amazon Guarantee 99.9% availability Amazon guarantees 99.999999999% (11x9s) durability for S3 information. Tiered Storage Available Lifecycle Management Versioning Encryption Secure your data - Access Control Lists & Bucket Policies","title":"S3 - The Basics"},{"location":"aws/s3/#s3-storage-tiersclasses","text":"S3 99.99% availability, 11x9s durability, stored redundantly across multiple devices in multiple facilities, and is designed to sustain the loss of 2 facilities concurrently. S3 - IA (Infrequently Accessed): For data that is accessed less frequently, but requires rapid access when needed. Lower fee than S3, but you are charged a retrieval fee. S3 - One Zone IA: Same as IA however data is stored in a single Availability Zone only, still 11x9s durability, but only 99.5% availability. Cost is 20% less than regular S3-IA. Reduced Redundancy Storage: Designed to provide 99.99% durability and 99.99% availability of objects over a given year. Used for data that can be recreated if lost, e.g. thumbnails (Starting to disappear from AWS docs but may still feature in exam) - Standard cost now more effective than using this option. Glacier: Very cheap, but used for archival only. Optimised for data that is infrequently accessed and it takes 3-5 hours to restore from Glacier.","title":"S3 - Storage Tiers/Classes"},{"location":"aws/s3/#s3-intelligent-tiering-re-invent-2018","text":"Unknown or unpredictable access patterns 2 tiers - frequent & infrequent access Automatically moves your data to most cost-effective tier based on how frequently you access each object - If object is not accessed for 30 consecutive days moved to infrequent access, but if it is used moved to frequent access. 11x9s durability 99,9% availability over a given year. Optimizes Cost No fees for accessing your data but a small monthly fee for monitoring/automation $0.0025 per 1,000 objects.","title":"S3 - Intelligent Tiering (re-invent 2018)"},{"location":"aws/s3/#s3-charges","text":"Storage per GB Requests (Get, Put, Copy, etc.) Storage Management Pricing Inventory, Analytics, Object Tags Data Management Pricing Data Transferred out of S3 Transfer Acceleration Use of CloudFront to optimize transfers","title":"S3 - Charges"},{"location":"aws/s3/#s3-exam-tips","text":"Remember that S3 is Object-based i.e. allows you to upload files. Object-based storage only (for files.) Not suitable to install an OS or running a DB on. Files can be from 0 Bytes to 5 TB. There is unlimited storage. Files are stored in Buckets. S3 is a universal namespace. That is, names must be unique globally. Example: https://s3-eu-west-1.amazonaws.com/acloudguru Read after Write consistency for PUTS of new Objects Eventual Consistency for overwrite PUTS and DELETES (can take some time to propagate) S3 Storage Classes/Tiers S3 [durable, immediately available, frequency accessed] S3 - IA [durable, immediately available, infrequency accessed] S3 - One Zone IA [same as IA however stored in single Availability Zone] S3 Reduced Redundancy Storage [data that is easily reproducible i.e. thumbnails] Glacier [Archived data, where you can wait 3-5 hours before accessing] Core fundamentals of S3 Object: Key (name) Value (data) Version ID Metadata Subresources - bucket-specific config: Bucket Policies, Access Control Lists Cross Origin Resource Sharing (CORS) Transfer Acceleration Successful uploads will generate HTTP 200 status code - when you use CLI/API Make sure you read the S3 FAQ: https://aws.amazon.com/s3/faqs/","title":"S3 Exam Tips"},{"location":"aws/s3/#s3-security","text":"","title":"S3 Security"},{"location":"aws/s3/#securing-your-buckets","text":"By default, all newly created buckets are PRIVATE. You can set up access control to your buckets using: Bucket Policies - Applied at a bucket level. (written in JSON) Access Control Lists - Applied at an object level. S3 buckets can be configured to create access logs, which log all requests made to the s3 bucket. These logs can be written to another bucket.","title":"Securing Your Buckets"},{"location":"aws/s3/#s3-encryption","text":"","title":"S3 Encryption"},{"location":"aws/s3/#types-of-encryption","text":"In Transit: SSL/TLS At Rest: Server Side Encryption: S3 Managed Keys - SSE-S3 [AES-256] AWS Key Management Service, Managed Keys, SSE-KMS Server Side Encryption with Customer Provided Keys - SSE-C Client Side Encryption","title":"Types of Encryption"},{"location":"aws/s3/#enforcing-encryption-on-s3-buckets","text":"Every time a file is uploaded to S3, a PUT request is initiated. This is what a PUT request looks like: PUT /myFileHTTP/1.1 Host: myBucket.s3.amazonaws.com Date: Wed, 25 Apr 2018 09 :50:00 GMT Authorization: auth string Content-Type: text/plain Content-Length: 26880 x-amz-meta-author: Dan Expect: 100 -continue [ 26880 bytes of object data ] If the file is to be encrypted at upload time, the x-amz-server-side-encryption-parameter will be included in the request header Two options are currently available: x-amz-server-side-encryption: AES256 (SSE-S3 - S3 managed keys) x-amz-server-side-encryption:ams:kms (SSE-KMS - KMS managed keys) When this parameter is included in the header of the PUT request, it tells S3 to encrypt the object at the time of upload, using the specified encryption method. You can enforce the use of Server Side Encryption by using a Bucket Policy which denies any S3 PUT request which doesn\u2019t include the x-amz-server-side-encryption parameter in the request header. The following request tells S3 to encrypt the file using SSE-S3 (AES 256) at the time of upload PUT /myFileHTTP/1.1 Host: myBucket.s3.amazonaws.com Date: Wed, 25 Apr 2018 09 :50:00 GMT Authorization: auth string Content-Type: text/plain Content-Length: 26880 x-amz-meta-author: Dan Expect: 100 -continue x-amz-server-side-encrpytion: AES256 [ 26880 bytes of object data ]","title":"Enforcing Encryption on S3 Buckets"},{"location":"aws/s3/#encryption-exam-tips","text":"Encryption In-Transit SSL/TLS (HTTPS) Encryption At Rest Server Side Encryption SSE-S3 SSE-KMS SSE-C Client Side Encryption (encrypt locally before uploading) If you want to enforce use of encryption for files stored in S3, use an S3 Bucket Policy to deny all PUT requests that don\u2019t include the x-amz-server-side-encryption parameter in the request header.","title":"Encryption Exam Tips"},{"location":"aws/s3/#cors-cross-origin-request-sharing","text":"A way of allowing code that is in one S3 bucket to access/reference code in another S3 bucket. Allowing 1 resource to access another resource - hence CORS. Useful for the \u2018static website hosting\u2019 property of S3. Think - images referenced from another bucket on your site (when both s3 are public accessible). Go to the bucket containing site HTML -> Properties -> Endpoint. (copy) Go to external s3 Bucket -> Permissions -> CORS Configuration This will provide the below default config - you will need to include your copied endpoint instead of \u2018asterisk\u2019 character\u2019: < CORSConfiguration > < CORSRule > < AllowedOrigin > * </ AllowedOrigin > < AllowedMethod > GET </ AllowedMethod > < MaxAgeSeconds > 3000 </ MaxAgeSeconds > < AllowedHeader > Authorization </ AllowedHeader > </ CORSRule > </ CORSConfiguration >","title":"CORS (Cross Origin Request Sharing)"},{"location":"aws/s3/#s3-performance-optimization","text":"S3 is designed to support very high request rates. however if your S3 buckets are routinely receiving > 100 PUT/LIST/DELETE or >300 GET requests per second, then there are best practice guidelines to optimize. The guidance is based on the type of workload you are running: GET-Intensive Workloads - use CloudFront content delivery service to get best performance. CloudFront will cache your most frequently accessed objects and will reduce latency for your GET requests. Note: see update 2018 below - this is no longer a major concern. * Mixed Request Type Workloads - a mix of GET, PUT, DELETE, GET Bucket - they key names you use for your objects can impact performance for intensive workloads. * S3 uses the key name to determine which partition an object will be stored in. * The use of sequential key names e.g. names prefixed with a time stamp or alphabetical sequence increases the likelihood of having multiple objects stored on the same partition * For heavy workloads this can cause I/O issues and contention * By using a random prefix to key names, you can force S3 to distribute your keys across multiple partitions, distributing the I/O workload.","title":"S3 Performance Optimization"},{"location":"aws/s3/#key-name-example","text":"The following Sequential Key Names are not optimal (likely stored on same partition): mybucket/date/custnum/photo1.jpg mybucket/date/custnum/photo2.jpg * mybucket/date/custnum/photo3.jpg Note: see update 2018 below - this is no longer a major concern. For optimal performance, introduce some randomness into the key name e.g. prefix with 4-character hexadecimal hash. mybucket/6ef8-date/custnum/photo1.jpg mybucket/h35d-date/custnum/photo2.jpg * mybucket/7eg4-date/custnum/photo3.jpg","title":"Key Name Example"},{"location":"aws/s3/#update-2018","text":"In July 2018, Amazon Announced massive increase in S3 performance that they can support 3,500 PUT requests per second 5,500 GET requests per second This new increased performances negates previous guidance to randomize your object key names to achieve faster performance. This means logical and sequential naming patterns can now be used without any performance implication.","title":"Update (2018)"},{"location":"aws/s3/#s3-optimization-exam-tips","text":"Remember 2 main approaches to Performance Optimization for S3: Get-Intensive -> CloudFront Mixed-Workloads -> Avoid sequential key names (maybe prefix with hex hash - avoids same partition) Read S3 FAQ -> https://aws.amazon.com/s3/faqs/","title":"S3 Optimization Exam Tips"},{"location":"aws/serverless/","text":"Serverless 101 EC2 Launches 2006 (provision machines from CLI -> IAAS born!) Data Centre -> IAAS (EC2) -> PAAS (Azure/ElasticBeanstalk) -> Containers (Docker) -> Serverless (Lambda) Lab Example Setup S3 Bucket with Public Site Hosting Setup Upload index.html & error.html Register Route53 (need bucket & domain name to be same) Create Lambda Function - multiple options (select author from scratch) Author from Scratch Blueprints Serverless App Repository (CF Templates) Provide Name, Create Role for Lambda (policy template: simple microservice permissions) -> Create Function. Scroll to Function code (used to be \u2018Cloud 9\u2019 IDE) copy pasta in code sample below Select \u2018Add Trigger\u2019 -> [popular exam - which of the following is not a trigger] -> select API Gateway Configure triggers -> pick an existing API, or create a new one (create new one -> filling in UI) Need to configure GET endpoint in Amazon API Gateway & then deploy API Sample Lambda Function (python) def lambda_handler ( event , context ): print ( \"In lambda handler\" ) resp = { \"statusCode\" : 200 , \"headers\" : { \"Access-Control-Allow-Origin\" : \"*\" , }, \"body\" : \"Dan McM\" } return resp","title":"Serverless"},{"location":"aws/serverless/#serverless-101","text":"EC2 Launches 2006 (provision machines from CLI -> IAAS born!) Data Centre -> IAAS (EC2) -> PAAS (Azure/ElasticBeanstalk) -> Containers (Docker) -> Serverless (Lambda)","title":"Serverless 101"},{"location":"aws/serverless/#lab-example","text":"Setup S3 Bucket with Public Site Hosting Setup Upload index.html & error.html Register Route53 (need bucket & domain name to be same) Create Lambda Function - multiple options (select author from scratch) Author from Scratch Blueprints Serverless App Repository (CF Templates) Provide Name, Create Role for Lambda (policy template: simple microservice permissions) -> Create Function. Scroll to Function code (used to be \u2018Cloud 9\u2019 IDE) copy pasta in code sample below Select \u2018Add Trigger\u2019 -> [popular exam - which of the following is not a trigger] -> select API Gateway Configure triggers -> pick an existing API, or create a new one (create new one -> filling in UI) Need to configure GET endpoint in Amazon API Gateway & then deploy API Sample Lambda Function (python) def lambda_handler ( event , context ): print ( \"In lambda handler\" ) resp = { \"statusCode\" : 200 , \"headers\" : { \"Access-Control-Allow-Origin\" : \"*\" , }, \"body\" : \"Dan McM\" } return resp","title":"Lab Example"},{"location":"aws/ses/","text":"Amazon SES (Simple Email Service) This is a scalable and highly available email service designed to help marketing teams and application developers send marketing, notification, and transactional emails to their customers using a pay as you go model. Can also be used to receive emails: incoming mails can be delivered automatically to an S3 bucket. Incoming mails can be used to trigger Lambda functions and SNS notifications. Use Cases Automated emails Purchase Confirmations, shipping notifications, order status updates e.g. mobile phone company needs to send automated confirmation email every time a customer purchases pre-paid mobile phone minutes. marketing communications, advertisements, newsletters, special offers e.g. online retail business that needs to let customers know about sales promotions and discounts SES -v- SNS SES SNS Email messaging service Pub/Sub messaging service, formats include SMS, HTTP, SQS, email Can trigger Lambda function or SNS notification Can be used to trigger Lambda function Can be used for both incoming and outgoing email Can fan out messages to large number of recipients (replicate and push messages to multiple endpoints and formats) An email address is all that is required to start sending messages to a user Consumers must subscribe to a topic to receive the notifications Exam Tips Remember that SES is email only Can be used for incoming and outgoing mail No subscription based, you only need to know the email address SNS supports multiple formats (SMS, SQS, HTTP, email) Push notifications only Pub/Sub model: consumers must subscribe to a topic You can fan-out messages to large number of recipients (e.g. multiple clients each with their own SQS queue)","title":"SES"},{"location":"aws/ses/#amazon-ses-simple-email-service","text":"This is a scalable and highly available email service designed to help marketing teams and application developers send marketing, notification, and transactional emails to their customers using a pay as you go model. Can also be used to receive emails: incoming mails can be delivered automatically to an S3 bucket. Incoming mails can be used to trigger Lambda functions and SNS notifications.","title":"Amazon SES (Simple Email Service)"},{"location":"aws/ses/#use-cases","text":"Automated emails Purchase Confirmations, shipping notifications, order status updates e.g. mobile phone company needs to send automated confirmation email every time a customer purchases pre-paid mobile phone minutes. marketing communications, advertisements, newsletters, special offers e.g. online retail business that needs to let customers know about sales promotions and discounts","title":"Use Cases"},{"location":"aws/ses/#ses-v-sns","text":"SES SNS Email messaging service Pub/Sub messaging service, formats include SMS, HTTP, SQS, email Can trigger Lambda function or SNS notification Can be used to trigger Lambda function Can be used for both incoming and outgoing email Can fan out messages to large number of recipients (replicate and push messages to multiple endpoints and formats) An email address is all that is required to start sending messages to a user Consumers must subscribe to a topic to receive the notifications","title":"SES -v- SNS"},{"location":"aws/ses/#exam-tips","text":"Remember that SES is email only Can be used for incoming and outgoing mail No subscription based, you only need to know the email address SNS supports multiple formats (SMS, SQS, HTTP, email) Push notifications only Pub/Sub model: consumers must subscribe to a topic You can fan-out messages to large number of recipients (e.g. multiple clients each with their own SQS queue)","title":"Exam Tips"},{"location":"aws/sns/","text":"AWS SNS (Simple Notification Service) Amazon SNS is a web service that makes it easy to set up, operate, and send notifications from the cloud. It provides developers with a highly scalable, flexible, and cost-effective capability to publish messages from an application and immediately deliver them to subscribers or other applications. Examples: push notifications to Apple, google, Fire OS, and Windows, as well as Android devices in China with Baidu Cloud Push. Besides pushing cloud notifications directly to mobile, SNS can also deliver notifications by SMS text message or email to Amazon Simple Queue Service (SQS) queues, or to any HTTP endpoint SNS notifications can also trigger Lambda functions: When a message is published to an SNS topic that has a Lambda function subscribed to it, the Lambda function is invoked with the payload of the published message. The Lambda function receives the message payload as an input parameter and can manipulate the information in the message, publish the message to another SNS topic, or send the message to another AWS service. SNS allows you to group multiple recipients using topics. A topic is an \u201caccess point\u201d for allowing recipients to dynamically subscribe for identical copies of the same notification. One topic can support delivers to multiple endpoint types - for example, you can group together iOS, Android and SMS recipients. When you publish once to a topic, SNS delivers appropriately formatted copies of your message to each subscriber. SNS - Topics To prevent messages from being lost, all messages published to Amazon SNS are stored redundantly across multiple availability zones. SNS Benefits Instantaneous, push-based delivery (no polling) Simple APIs and easy integration with applications Flexible message delivery over multiple transport protocols Inexpensive, pay-as-you-go model with no up-front costs Web-based AWS management console offers the simplicity of a point-and-click interface SNS -v- SQS Both messaging Services in AWS SNS - push SQS - Polls (Pulls) SNS Pricing Users pay $0.50 per 1 million SNS requests $0.06 per 100,000 Notification delivers over HTTP $0.75 per 100 Notification delivers over SMS $2.00 per 100,000 Notification deliveries over Email SNS follows the \u201cpublish-subscribe\u201d (pub-sub) messaging paradigm, with notifications being delivered to clients using a \u201cpush\u201d mechanism that eliminates the need to periodically check or \u201cpoll\u201d for new information and updates. With simple APIs requiring minimal up-front development effort, no maintenance or management overhead and pay-as-you-go pricing, Amazon SNS gives developers an easy mechanism to incorporate a powerful notification system with their applications. SNS Exam Tips SNS is a scalable and highly available notification service which allows you to send push notifications from the cloud. Variety of message formats supported: SMS text message, email, Amazon Simple Queue Service (SQS) queues, any HTTP endpoint. Pub-sub model whereby users subscribe to topics It is a push mechanism, rather than a pull (poll) mechanism","title":"SNS"},{"location":"aws/sns/#aws-sns-simple-notification-service","text":"Amazon SNS is a web service that makes it easy to set up, operate, and send notifications from the cloud. It provides developers with a highly scalable, flexible, and cost-effective capability to publish messages from an application and immediately deliver them to subscribers or other applications. Examples: push notifications to Apple, google, Fire OS, and Windows, as well as Android devices in China with Baidu Cloud Push. Besides pushing cloud notifications directly to mobile, SNS can also deliver notifications by SMS text message or email to Amazon Simple Queue Service (SQS) queues, or to any HTTP endpoint SNS notifications can also trigger Lambda functions: When a message is published to an SNS topic that has a Lambda function subscribed to it, the Lambda function is invoked with the payload of the published message. The Lambda function receives the message payload as an input parameter and can manipulate the information in the message, publish the message to another SNS topic, or send the message to another AWS service. SNS allows you to group multiple recipients using topics. A topic is an \u201caccess point\u201d for allowing recipients to dynamically subscribe for identical copies of the same notification. One topic can support delivers to multiple endpoint types - for example, you can group together iOS, Android and SMS recipients. When you publish once to a topic, SNS delivers appropriately formatted copies of your message to each subscriber.","title":"AWS SNS (Simple Notification Service)"},{"location":"aws/sns/#sns-topics","text":"To prevent messages from being lost, all messages published to Amazon SNS are stored redundantly across multiple availability zones.","title":"SNS - Topics"},{"location":"aws/sns/#sns-benefits","text":"Instantaneous, push-based delivery (no polling) Simple APIs and easy integration with applications Flexible message delivery over multiple transport protocols Inexpensive, pay-as-you-go model with no up-front costs Web-based AWS management console offers the simplicity of a point-and-click interface","title":"SNS Benefits"},{"location":"aws/sns/#sns-v-sqs","text":"Both messaging Services in AWS SNS - push SQS - Polls (Pulls)","title":"SNS -v- SQS"},{"location":"aws/sns/#sns-pricing","text":"Users pay $0.50 per 1 million SNS requests $0.06 per 100,000 Notification delivers over HTTP $0.75 per 100 Notification delivers over SMS $2.00 per 100,000 Notification deliveries over Email SNS follows the \u201cpublish-subscribe\u201d (pub-sub) messaging paradigm, with notifications being delivered to clients using a \u201cpush\u201d mechanism that eliminates the need to periodically check or \u201cpoll\u201d for new information and updates. With simple APIs requiring minimal up-front development effort, no maintenance or management overhead and pay-as-you-go pricing, Amazon SNS gives developers an easy mechanism to incorporate a powerful notification system with their applications.","title":"SNS Pricing"},{"location":"aws/sns/#sns-exam-tips","text":"SNS is a scalable and highly available notification service which allows you to send push notifications from the cloud. Variety of message formats supported: SMS text message, email, Amazon Simple Queue Service (SQS) queues, any HTTP endpoint. Pub-sub model whereby users subscribe to topics It is a push mechanism, rather than a pull (poll) mechanism","title":"SNS Exam Tips"},{"location":"aws/sqs/","text":"AWS SQS Amazon SQS is a web services that gives you access to a message queue that can be used to store messages while waiting for a computer to process them. Amazon SQS is a distributed queue system that enables web service applications to quickly and reliably queue messages that one component in the application generates to be consumed by another component. A queue is a temporary repository for messages that are awaiting processing. Using Amazon SQS, you can decouple the components of an application so they run independently, easing message management between components. Any component of a distributed application can store messages in the queue, messages can contain up to 256KB of text in any format. Any component can later retrieve the messages programmatically using the Amazon SQS API. The queue acts as a buffer between the component producing and saving data, and the component receiving the data for processing. This means the queue resolves issues that arise if the producer is producing work faster than the consumer can process it, or if the producer or consumer are only intermittently connected to the network. Queue Types Standard Queues (default) FIFO Queues (First-In-First-Out) Standard Queue This lets you have a nearly unlimited number of transactions per second. Standard queues guarantee that a message is delivered at least once. However, occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order. Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent. FIFO Queues The FIFO queue complements the standard queue. The most important features of this queue type are FIFO delivery and exactly-once processing: The order in which messages are sent and received is strictly preserved and a message is delivered once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue. FIFO queues also support message groups that allow multiple ordered message groups within a single queue. FIFO queues are limited to 300 transactions per second (TPS), but have all the capabilities of standard queues. SQS Visibility Timeout The visibility timeout is the amount of time that the message is invisible in the SQS queue after a reader picks up that message. Provided the job is processed before the visibility time out expires, the message will then be deleted from the queue. If the job is not processed within that time, the message will become visible again and another reader will process it. This could result in the same message being delivered twice. Default Visibility Timeout is 30 seconds Increase it if your task takes > 30 seconds Maximum is 12 hours SQS Key Facts SQS is pull-based, not pushed-based Messages are 256 KB in size Messages can be kept in the queue from 1 minute to 14 days Default retention period is 4 days SQS guarantees that your messages will be processed at least once SQS Long Polling Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues While the regular short polling returns immediately (even if the message queue being polled is empty), long polling doesn\u2019t return a response until a message arrives in the message queue, or the long poll times out As such, long polling can save you money. SQS Exam Tips SQS is a distributed message queueing system Allows you to decouple the components of an application so that they are independent Pull based, not push based Standard Queues (default) - best effort ordering; message delivered at least one. FIFO Queues (First In First Out) - ordering strictly preserved, message delivered once, no duplicates e.g. good for banking transactions which need to happen in strict order. Visibility Timeout Default is 30 seconds - increase if task takes > 30 seconds to complete Max 12 hours Short Polling - returned immediately even if no messages are in the queue Long Polling - polls the queue periodically and only returns a response when a message in the queue or the timeout is reached","title":"SQS"},{"location":"aws/sqs/#aws-sqs","text":"Amazon SQS is a web services that gives you access to a message queue that can be used to store messages while waiting for a computer to process them. Amazon SQS is a distributed queue system that enables web service applications to quickly and reliably queue messages that one component in the application generates to be consumed by another component. A queue is a temporary repository for messages that are awaiting processing. Using Amazon SQS, you can decouple the components of an application so they run independently, easing message management between components. Any component of a distributed application can store messages in the queue, messages can contain up to 256KB of text in any format. Any component can later retrieve the messages programmatically using the Amazon SQS API. The queue acts as a buffer between the component producing and saving data, and the component receiving the data for processing. This means the queue resolves issues that arise if the producer is producing work faster than the consumer can process it, or if the producer or consumer are only intermittently connected to the network.","title":"AWS SQS"},{"location":"aws/sqs/#queue-types","text":"Standard Queues (default) FIFO Queues (First-In-First-Out)","title":"Queue Types"},{"location":"aws/sqs/#standard-queue","text":"This lets you have a nearly unlimited number of transactions per second. Standard queues guarantee that a message is delivered at least once. However, occasionally (because of the highly-distributed architecture that allows high throughput), more than one copy of a message might be delivered out of order. Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they are sent.","title":"Standard Queue"},{"location":"aws/sqs/#fifo-queues","text":"The FIFO queue complements the standard queue. The most important features of this queue type are FIFO delivery and exactly-once processing: The order in which messages are sent and received is strictly preserved and a message is delivered once and remains available until a consumer processes and deletes it; duplicates are not introduced into the queue. FIFO queues also support message groups that allow multiple ordered message groups within a single queue. FIFO queues are limited to 300 transactions per second (TPS), but have all the capabilities of standard queues.","title":"FIFO Queues"},{"location":"aws/sqs/#sqs-visibility-timeout","text":"The visibility timeout is the amount of time that the message is invisible in the SQS queue after a reader picks up that message. Provided the job is processed before the visibility time out expires, the message will then be deleted from the queue. If the job is not processed within that time, the message will become visible again and another reader will process it. This could result in the same message being delivered twice. Default Visibility Timeout is 30 seconds Increase it if your task takes > 30 seconds Maximum is 12 hours","title":"SQS Visibility Timeout"},{"location":"aws/sqs/#sqs-key-facts","text":"SQS is pull-based, not pushed-based Messages are 256 KB in size Messages can be kept in the queue from 1 minute to 14 days Default retention period is 4 days SQS guarantees that your messages will be processed at least once","title":"SQS Key Facts"},{"location":"aws/sqs/#sqs-long-polling","text":"Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues While the regular short polling returns immediately (even if the message queue being polled is empty), long polling doesn\u2019t return a response until a message arrives in the message queue, or the long poll times out As such, long polling can save you money.","title":"SQS Long Polling"},{"location":"aws/sqs/#sqs-exam-tips","text":"SQS is a distributed message queueing system Allows you to decouple the components of an application so that they are independent Pull based, not push based Standard Queues (default) - best effort ordering; message delivered at least one. FIFO Queues (First In First Out) - ordering strictly preserved, message delivered once, no duplicates e.g. good for banking transactions which need to happen in strict order. Visibility Timeout Default is 30 seconds - increase if task takes > 30 seconds to complete Max 12 hours Short Polling - returned immediately even if no messages are in the queue Long Polling - polls the queue periodically and only returns a response when a message in the queue or the timeout is reached","title":"SQS Exam Tips"},{"location":"aws/step-functions/","text":"Step Functions Step functions allow you to visualize and test your serverless applications. Step Functions provides a graphical console to arrange and visualize components of your applicaiton as a series of steps. This makes it simple to build and run multistep applications. Step Functions automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected. Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems quickly. Sequential Steps Branching Steps (Choice of Path) Parallel Steps Note: Under Application Integrations on AWS (not Lambda) Uses JSON-based Amazon State Language. Exam Tips Great way to visualize your serverless application Step Functions automatically triggers and tracks each step Step Functions logs the state of each step so if something goes wrong you can track what went wrong and where","title":"Step Functions"},{"location":"aws/step-functions/#step-functions","text":"Step functions allow you to visualize and test your serverless applications. Step Functions provides a graphical console to arrange and visualize components of your applicaiton as a series of steps. This makes it simple to build and run multistep applications. Step Functions automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected. Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems quickly. Sequential Steps Branching Steps (Choice of Path) Parallel Steps Note: Under Application Integrations on AWS (not Lambda) Uses JSON-based Amazon State Language.","title":"Step Functions"},{"location":"aws/step-functions/#exam-tips","text":"Great way to visualize your serverless application Step Functions automatically triggers and tracks each step Step Functions logs the state of each step so if something goes wrong you can track what went wrong and where","title":"Exam Tips"},{"location":"aws/x-ray/","text":"AWS X-Ray AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases and HTTP web APIs. X-Ray SDK installed within your application SDK sends JSON to X-Ray Daemon (installed on linux/win/osx) - listening on UDP -> sends to X-Ray API X-Ray Console stores visualization AWS SDK/CLI communicate with Daemon or API directly. X-Ray SDK Provides: Interceptors to add to your code to trace incoming HTTP requests Client handlers to instrument AWS SDK clients that your application uses to call other AWS services An HTTP client to use to instrument calls to other internal and external HTTP web services X-Ray Integrations X-Ray integrates with the following AWS services: Elastic Load Balancing AWS Lambda AWS API Gateway Amazon EC2 AWS Elastic Beanstalk X-Ray Languages X-Ray Supported Languages: Java Go Node.js Python Ruby .Net Exam Tips What X-Ray SDK Provides What X-Ray Integrated with What X-Ray languages are supported","title":"X Ray"},{"location":"aws/x-ray/#aws-x-ray","text":"AWS X-Ray is a service that collects data about requests that your application serves, and provides tools you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization. For any traced request to your application, you can see detailed information not only about the request and response, but also about calls that your application makes to downstream AWS resources, microservices, databases and HTTP web APIs. X-Ray SDK installed within your application SDK sends JSON to X-Ray Daemon (installed on linux/win/osx) - listening on UDP -> sends to X-Ray API X-Ray Console stores visualization AWS SDK/CLI communicate with Daemon or API directly.","title":"AWS X-Ray"},{"location":"aws/x-ray/#x-ray-sdk","text":"Provides: Interceptors to add to your code to trace incoming HTTP requests Client handlers to instrument AWS SDK clients that your application uses to call other AWS services An HTTP client to use to instrument calls to other internal and external HTTP web services","title":"X-Ray SDK"},{"location":"aws/x-ray/#x-ray-integrations","text":"X-Ray integrates with the following AWS services: Elastic Load Balancing AWS Lambda AWS API Gateway Amazon EC2 AWS Elastic Beanstalk","title":"X-Ray Integrations"},{"location":"aws/x-ray/#x-ray-languages","text":"X-Ray Supported Languages: Java Go Node.js Python Ruby .Net","title":"X-Ray Languages"},{"location":"aws/x-ray/#exam-tips","text":"What X-Ray SDK Provides What X-Ray Integrated with What X-Ray languages are supported","title":"Exam Tips"},{"location":"containerisation/docker/","text":"Docker Docker Commands The -it flag will allow you to ctrl+c to close the running image docker run -p 3000:3000 -it 0319d6e6a683 The -d flag will run service in the background allowing continued terminal access docker run -p 3000:3000 -d 0319d6e6a683 To push an image to Docker Hub (required for Kubernetes access) docker login docker tag imageid your-login/docker-demo docker push your-login/docker-demo To tag an image during the build process docker build -t your-login/docker-demo . docker push your-login/docker-demo","title":"Docker"},{"location":"containerisation/docker/#docker","text":"","title":"Docker"},{"location":"containerisation/docker/#docker-commands","text":"The -it flag will allow you to ctrl+c to close the running image docker run -p 3000:3000 -it 0319d6e6a683 The -d flag will run service in the background allowing continued terminal access docker run -p 3000:3000 -d 0319d6e6a683 To push an image to Docker Hub (required for Kubernetes access) docker login docker tag imageid your-login/docker-demo docker push your-login/docker-demo To tag an image during the build process docker build -t your-login/docker-demo . docker push your-login/docker-demo","title":"Docker Commands"},{"location":"containerisation/kubernetes/","text":"Kubernetes Notes based on practice following the Udemy course: Learn DevOps: The Complete Kubernetes Course . See related GitHub code files here . Vagrant Commands To build a plain ubuntu box mkdir ubuntu vagrant init ubuntu/xenial64 vagrant up Running Docker Container on Kubernetes Before launching container based on Docker image need to create pod definition Pod: describes an application running on Kubernetes Service: contain one or more tightly coupled containers (that make up the app - easily communicate with local port numbers) Setup Application on Kubernetes kubectl create -f k8s/demopod-helloworld.yml kubectl describe pod nodehelloworld.example.com kubectl port-forward nodehelloworld.example.com 8081:3000 kubectl expose pod nodehelloworld.example.com --type=NodePort --name=helloworld-service minikube service helloworld-service --url returns url of the service running = kubectl get service Useful Kubectl Commands (tie in with above example) kubectl attach helloworld.example.com kubectl exec helloworld.example.com -- ls /app lists file running inside container kubectl exec helloworld.example.com -- touch /app/test.txt creates file inside container (will disappear if the container is killed - non-persistent data) kubectl describe service helloworld-service kubectl run -i -tty busybox --image=busybox --restart=Never -- sh Kops Cluster Deployment AFTER SSHING INTO LINUX INSTANCE (vagrant via putty): setup awscli sudo apt-get install python-pip sudo pip install awscli aws configure ( use IAM role details - with admin policy ) ls -ahl ~/.aws/ ( verify credentials ) setup kubectl wget https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl sudo mv kubectl /usr/local/bin/ sudo chmod +x /usr/local/bin/kubectl kubectl setup keygen ssh-keygen -f .ssh/id_rsa cat .ssh/id_rsa.pub rename kops installation sudo mv /usr/local/bin/kops-linux-amd64 /usr/local/bin/kops generate cluster with your s3 bucket with route53 dns zone kops create cluster --name=kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.yourowndomain.com kops update cluster kubernetes.yourowndomain.com --yes --state=s3://kops-state-randomhash check your cert and password to log into new cluster cat .kube/config check nodes are running kubectl get node test own program on nodes kubectl run hello-minikube --image = gcr.io/google_containers/echoserver:1.4 --port = 8080 kubectl expose deployment hello-minikube --type = NodePort kubectl get service at this point add custom rule to aws- using VPC dashbaord - select master node - modify inbound traffic - all allowed on the port of your app in this case: 31956 Test it out: http://api.kubernetes.yourowndomain.com:31956/ to delete and avoid payments kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash to agree to delete kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --yes Horizontal -v- Vertical Scaling You can only horizontally scale when your pod is stateless. (i.e. kubectl scale options). Stateful pods cannot be horizontally scaled. Useful Pod Commands kubectl get pod Get info about all running pods kubectl describe pod <pod> Describe one pod kubectl expose pod <pod> --port=444 --name=frontend Expose port of a pod (creates new service) kubectl port-forward <pod> 8080 Port forward teh exposed pod port to your local machine kubectl attach <podname> -i Attach to the pod kubectl exec <pod> --command Execute a command on the pod kubectl label pods <pod> mylabel=awesome Add a new label to a pod kubectl run -i --tty busybox --image=busybox --restart=Never -- sh Run a shell in a pod - very useful for debugging. Once in you can run telnet 172.17.0.5 3000 to hit service and then command GET / kubectl scale --replicas=4 -f file.yaml Scales the number of pods replicated kubectl get rc Gets replica controllers kubectl scale --replicas=4 -f rc/helloworld-controller Scales the number of pods replicated kubectl delete rc/helloworld-controller Deletes controller Useful Deployment Commands kubectl get deployments kubectl get rs kubectl get pods --show-labels kubectl rollout status deployment/helloworld-deployment kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 can be used to update app to latest version kubectl edit deployment/helloworld-deployment kubectl rollout history deployment/helloworld-deployment kubectl rollout undo deployment/helloworld-deployment --to-revision=n Useful Service Commands ClusterIP - virtual IP address only reachable from within the Cluster (default) NodePort - same on each node also reachable externally LoadBalancer - routes external traffic to every node on the NodePort (ELB on AWS) ExternalName can provide a DNS name for service (e.g. service discovery using DNS) - only works when DNS add-on is enabled By default service can only run between ports 30000-32767 but can be changed by adding \u2013service-node-port-range= argument to kube-apiserver minikube service service-name --url kubectl describe svc helloworld-service kubectl get svc svc short for service Labels key/value pairs attached to objects labels are not unique, multiple can be added to one objects Label selectors - you can use matching expressions to match labels kubectl get nodes --show-labels Node labels You can also use labels to tag nodes - once tagged you can use label selectors to let pods only run on specific nodes 2 steps to run a pod on a specific set of nodes: First you tag the node Then you add a nodeSelector to your pod configuration First step: add a label or multiple to your nodes: kubectl label nodes node1 hardware = high-spec kubectl label nodes node2 hardware = low = spec Second step: add a pod that uses those labels: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 nodeSelector : hardware : high-spec Health Checks To detect and resolve problems with you app, you can run health checks 2 types of healthchecks: Running a command in the container periodically Periodic checks on a URL (HTTP) apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 livenessProbe : httpGet : path : / port : 3000 initialDelaySeconds : 15 timeoutSeconds : 30 You can describe pods to see the liveness in effect and success/failure counts Readiness Probes livenessProbes: indicates whether container is Running readinessProbes: indicates whether container is ready to serve make sure at startup the pod will onyl receive traffic when test succeeds you can use them in conjunction Pod State Pod Status kubectl get pods [Pending/Succeeded/Failed/Unknown] Pod Condition kubectl describe pod PODNAME [PodScheduled/Ready/Initialized/Unschedulable/ContainersReady] Container State kubectl get pod <podname> [Running/Terminated/Waiting] Secrets A way to distribute credentials, keys, passwords or data to pods Use in the following ways: Use secrets as env varaiables Use secrets as a file in a pod Uses volumes to be mounted in a container with files, can be used for dotenv files or your app can just read this file. Use an external image to pull secrets from a private image registry To generate secrets using files: echo -n \"root\" > ./username.txt echo -n \"password\" > ./password.txt kubectl create secret generic db-user-pass --from-file = ./username.txt --from-file = ./password.txt Can also be an SSH key or SSL cert: kubectl create secret generic ssl-certificate --from-file = ssh-privatekey = ~/.sshid_rsa --ssl-cert = mysslcert.crt To generate secrets using yaml apiVersion : v1 kind : Secret metadata : name : db-secret type : Opaque data : password : blergh # base64 encoded username : blergh2 # base64 encoded echo -n \"blergh\" | base64 Using Secrets Create a pod that exposes secrets as env variables: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : db-secret key : username - name : SECRET_PASSWORD [ ... ] Alternatively you can provide the secrets in a file: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 volumeMounts : - name : credvolume mountPath : /etc/creds readOnly : true volumes : - name : credvolume secret : secretName : db-secrets Web UI Start dashboard Create dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml Create user Create sample user (if using RBAC - on by default on new installs with kops / kubeadm): kubectl create -f sample-user.yaml Get login token: kubectl -n kube-system get secret | grep admin-user kubectl -n kube-system describe secret admin-user-token-<id displayed by previous command> Login to dashboard Go to http://api.yourdomain.com:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login Login: admin Password: the password that is listed in ~/.kube/config (open file in editor and look for \u201cpassword: \u2026\u201d Choose for login token and enter the login token from the previous ste","title":"Kubernetes"},{"location":"containerisation/kubernetes/#kubernetes","text":"Notes based on practice following the Udemy course: Learn DevOps: The Complete Kubernetes Course . See related GitHub code files here .","title":"Kubernetes"},{"location":"containerisation/kubernetes/#vagrant-commands","text":"To build a plain ubuntu box mkdir ubuntu vagrant init ubuntu/xenial64 vagrant up","title":"Vagrant Commands"},{"location":"containerisation/kubernetes/#running-docker-container-on-kubernetes","text":"Before launching container based on Docker image need to create pod definition Pod: describes an application running on Kubernetes Service: contain one or more tightly coupled containers (that make up the app - easily communicate with local port numbers)","title":"Running Docker Container on Kubernetes"},{"location":"containerisation/kubernetes/#setup-application-on-kubernetes","text":"kubectl create -f k8s/demopod-helloworld.yml kubectl describe pod nodehelloworld.example.com kubectl port-forward nodehelloworld.example.com 8081:3000 kubectl expose pod nodehelloworld.example.com --type=NodePort --name=helloworld-service minikube service helloworld-service --url returns url of the service running = kubectl get service","title":"Setup Application on Kubernetes"},{"location":"containerisation/kubernetes/#useful-kubectl-commands-tie-in-with-above-example","text":"kubectl attach helloworld.example.com kubectl exec helloworld.example.com -- ls /app lists file running inside container kubectl exec helloworld.example.com -- touch /app/test.txt creates file inside container (will disappear if the container is killed - non-persistent data) kubectl describe service helloworld-service kubectl run -i -tty busybox --image=busybox --restart=Never -- sh","title":"Useful Kubectl Commands (tie in with above example)"},{"location":"containerisation/kubernetes/#kops-cluster-deployment","text":"AFTER SSHING INTO LINUX INSTANCE (vagrant via putty): setup awscli sudo apt-get install python-pip sudo pip install awscli aws configure ( use IAM role details - with admin policy ) ls -ahl ~/.aws/ ( verify credentials ) setup kubectl wget https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl sudo mv kubectl /usr/local/bin/ sudo chmod +x /usr/local/bin/kubectl kubectl setup keygen ssh-keygen -f .ssh/id_rsa cat .ssh/id_rsa.pub rename kops installation sudo mv /usr/local/bin/kops-linux-amd64 /usr/local/bin/kops generate cluster with your s3 bucket with route53 dns zone kops create cluster --name=kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.yourowndomain.com kops update cluster kubernetes.yourowndomain.com --yes --state=s3://kops-state-randomhash check your cert and password to log into new cluster cat .kube/config check nodes are running kubectl get node test own program on nodes kubectl run hello-minikube --image = gcr.io/google_containers/echoserver:1.4 --port = 8080 kubectl expose deployment hello-minikube --type = NodePort kubectl get service at this point add custom rule to aws- using VPC dashbaord - select master node - modify inbound traffic - all allowed on the port of your app in this case: 31956 Test it out: http://api.kubernetes.yourowndomain.com:31956/ to delete and avoid payments kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash to agree to delete kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --yes","title":"Kops Cluster Deployment"},{"location":"containerisation/kubernetes/#horizontal-v-vertical-scaling","text":"You can only horizontally scale when your pod is stateless. (i.e. kubectl scale options). Stateful pods cannot be horizontally scaled.","title":"Horizontal -v- Vertical Scaling"},{"location":"containerisation/kubernetes/#useful-pod-commands","text":"kubectl get pod Get info about all running pods kubectl describe pod <pod> Describe one pod kubectl expose pod <pod> --port=444 --name=frontend Expose port of a pod (creates new service) kubectl port-forward <pod> 8080 Port forward teh exposed pod port to your local machine kubectl attach <podname> -i Attach to the pod kubectl exec <pod> --command Execute a command on the pod kubectl label pods <pod> mylabel=awesome Add a new label to a pod kubectl run -i --tty busybox --image=busybox --restart=Never -- sh Run a shell in a pod - very useful for debugging. Once in you can run telnet 172.17.0.5 3000 to hit service and then command GET / kubectl scale --replicas=4 -f file.yaml Scales the number of pods replicated kubectl get rc Gets replica controllers kubectl scale --replicas=4 -f rc/helloworld-controller Scales the number of pods replicated kubectl delete rc/helloworld-controller Deletes controller","title":"Useful Pod Commands"},{"location":"containerisation/kubernetes/#useful-deployment-commands","text":"kubectl get deployments kubectl get rs kubectl get pods --show-labels kubectl rollout status deployment/helloworld-deployment kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 can be used to update app to latest version kubectl edit deployment/helloworld-deployment kubectl rollout history deployment/helloworld-deployment kubectl rollout undo deployment/helloworld-deployment --to-revision=n","title":"Useful Deployment Commands"},{"location":"containerisation/kubernetes/#useful-service-commands","text":"ClusterIP - virtual IP address only reachable from within the Cluster (default) NodePort - same on each node also reachable externally LoadBalancer - routes external traffic to every node on the NodePort (ELB on AWS) ExternalName can provide a DNS name for service (e.g. service discovery using DNS) - only works when DNS add-on is enabled By default service can only run between ports 30000-32767 but can be changed by adding \u2013service-node-port-range= argument to kube-apiserver minikube service service-name --url kubectl describe svc helloworld-service kubectl get svc svc short for service","title":"Useful Service Commands"},{"location":"containerisation/kubernetes/#labels","text":"key/value pairs attached to objects labels are not unique, multiple can be added to one objects Label selectors - you can use matching expressions to match labels kubectl get nodes --show-labels","title":"Labels"},{"location":"containerisation/kubernetes/#node-labels","text":"You can also use labels to tag nodes - once tagged you can use label selectors to let pods only run on specific nodes 2 steps to run a pod on a specific set of nodes: First you tag the node Then you add a nodeSelector to your pod configuration First step: add a label or multiple to your nodes: kubectl label nodes node1 hardware = high-spec kubectl label nodes node2 hardware = low = spec Second step: add a pod that uses those labels: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 nodeSelector : hardware : high-spec","title":"Node labels"},{"location":"containerisation/kubernetes/#health-checks","text":"To detect and resolve problems with you app, you can run health checks 2 types of healthchecks: Running a command in the container periodically Periodic checks on a URL (HTTP) apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 livenessProbe : httpGet : path : / port : 3000 initialDelaySeconds : 15 timeoutSeconds : 30 You can describe pods to see the liveness in effect and success/failure counts","title":"Health Checks"},{"location":"containerisation/kubernetes/#readiness-probes","text":"livenessProbes: indicates whether container is Running readinessProbes: indicates whether container is ready to serve make sure at startup the pod will onyl receive traffic when test succeeds you can use them in conjunction","title":"Readiness Probes"},{"location":"containerisation/kubernetes/#pod-state","text":"Pod Status kubectl get pods [Pending/Succeeded/Failed/Unknown] Pod Condition kubectl describe pod PODNAME [PodScheduled/Ready/Initialized/Unschedulable/ContainersReady] Container State kubectl get pod <podname> [Running/Terminated/Waiting]","title":"Pod State"},{"location":"containerisation/kubernetes/#secrets","text":"A way to distribute credentials, keys, passwords or data to pods Use in the following ways: Use secrets as env varaiables Use secrets as a file in a pod Uses volumes to be mounted in a container with files, can be used for dotenv files or your app can just read this file. Use an external image to pull secrets from a private image registry To generate secrets using files: echo -n \"root\" > ./username.txt echo -n \"password\" > ./password.txt kubectl create secret generic db-user-pass --from-file = ./username.txt --from-file = ./password.txt Can also be an SSH key or SSL cert: kubectl create secret generic ssl-certificate --from-file = ssh-privatekey = ~/.sshid_rsa --ssl-cert = mysslcert.crt To generate secrets using yaml apiVersion : v1 kind : Secret metadata : name : db-secret type : Opaque data : password : blergh # base64 encoded username : blergh2 # base64 encoded echo -n \"blergh\" | base64","title":"Secrets"},{"location":"containerisation/kubernetes/#using-secrets","text":"Create a pod that exposes secrets as env variables: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : db-secret key : username - name : SECRET_PASSWORD [ ... ] Alternatively you can provide the secrets in a file: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 volumeMounts : - name : credvolume mountPath : /etc/creds readOnly : true volumes : - name : credvolume secret : secretName : db-secrets","title":"Using Secrets"},{"location":"containerisation/kubernetes/#web-ui","text":"","title":"Web UI"},{"location":"containerisation/kubernetes/#start-dashboard","text":"Create dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml","title":"Start dashboard"},{"location":"containerisation/kubernetes/#create-user","text":"Create sample user (if using RBAC - on by default on new installs with kops / kubeadm): kubectl create -f sample-user.yaml","title":"Create user"},{"location":"containerisation/kubernetes/#get-login-token","text":"kubectl -n kube-system get secret | grep admin-user kubectl -n kube-system describe secret admin-user-token-<id displayed by previous command>","title":"Get login token:"},{"location":"containerisation/kubernetes/#login-to-dashboard","text":"Go to http://api.yourdomain.com:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login Login: admin Password: the password that is listed in ~/.kube/config (open file in editor and look for \u201cpassword: \u2026\u201d Choose for login token and enter the login token from the previous ste","title":"Login to dashboard"},{"location":"languages/go/","text":"Go","title":"Go"},{"location":"languages/go/#go","text":"","title":"Go"},{"location":"languages/java/","text":"Java","title":"Java"},{"location":"languages/java/#java","text":"","title":"Java"},{"location":"languages/javascript/","text":"JavaScript","title":"JavaScript"},{"location":"languages/javascript/#javascript","text":"","title":"JavaScript"},{"location":"languages/python/","text":"Python x = \"always look on\" ; y = \"the bright side\" ; z = x + y + \"of life\" ;","title":"Python"},{"location":"languages/python/#python","text":"x = \"always look on\" ; y = \"the bright side\" ; z = x + y + \"of life\" ;","title":"Python"},{"location":"languages/scala/","text":"Scala Variable Declaration // immutable val donutsToBuy : Int = 5 // mutable var favoriteDonut : String = \"Glazed Donut\" favoriteDonut = \"Vanilla Donut\" // lazy (delay the initialization until consumed by application) // this also uses type inference - note the lack of : String! lazy val donutService = \"initialize some donut service\" // declare a variable with no initialization var leastFavoriteDonut : String = _ leastFavoriteDonut = \"Plain Donut\" Supported Types val donutsBought : Int = 5 val bigNumberOfDonuts : Long = 100000000L val smallNumberOfDonuts : Short = 1 val priceOfDonut : Double = 2.50 val donutPrice : Float = 2.50f val donutStoreName : String = \"allaboutscala Donut Store\" val donutByte : Byte = 0xa val donutFirstLetter : Char = 'D' val nothing : Unit = () Collections Lists // List of Strings val fruit : List [ String ] = List ( \"apples\" , \"oranges\" , \"pears\" ) // List of Integers val nums : List [ Int ] = List ( 1 , 2 , 3 , 4 ) // Empty List. val empty : List [ Nothing ] = List () // Two dimensional list val dim : List [ List [ Int ]] = List ( List ( 1 , 0 , 0 ), List ( 0 , 1 , 0 ), List ( 0 , 0 , 1 ) ) // Basic Examples of common methods // another way of defining the fruit List above using cons (::) val fruit = \"apples\" :: ( \"oranges\" :: ( \"pears\" :: Nil )) val nums = Nil println ( \"Head of fruit : \" + fruit . head ) println ( \"Tail of fruit : \" + fruit . tail ) println ( \"Check if fruit is empty : \" + fruit . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty ) Sets // Empty set of integer type var s : Set [ Int ] = Set () // Set of integer type var s : Set [ Int ] = Set ( 1 , 3 , 5 , 7 ) or var s = Set ( 1 , 3 , 5 , 7 ) // find common elements between two sets val num1 = Set ( 5 , 6 , 9 , 20 , 30 , 45 ) val num2 = Set ( 50 , 60 , 9 , 20 , 35 , 55 ) println ( \"num1.&(num2) : \" + num1 .&( num2 ) ) println ( \"num1.intersect(num2) : \" + num1 . intersect ( num2 ) ) Maps // Empty hash table whose keys are strings and values are integers: var A : Map [ Char , Int ] = Map () // A map with keys and values. val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" ) // Basic examples of common methods val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" , \"peru\" -> \"#CD853F\" ) val nums : Map [ Int , Int ] = Map () println ( \"Keys in colors : \" + colors . keys ) println ( \"Values in colors : \" + colors . values ) println ( \"Check if colors is empty : \" + colors . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty ) Tuples // Short version val t = ( 1 , \"hello\" , Console ) // Long version val t = new Tuple3 ( 1 , \"hello\" , Console ) // Example Usage val t = ( 4 , 3 , 2 , 1 ) val sum = t . _1 + t . _2 + t . _3 + t . _4 println ( \"Sum of elements: \" + sum ) val t = new Tuple3 ( 1 , \"hello\" , Console ) println ( \"Concatenated String: \" + t . toString () ) Options // An Option[T] can be either Some[T] or None object, which represents a missing value. // Example object Demo { def main ( args : Array [ String ]) { val capitals = Map ( \"France\" -> \"Paris\" , \"Japan\" -> \"Tokyo\" ) println ( \"capitals.get( \\\"France\\\" ) : \" + capitals . get ( \"France\" )) println ( \"capitals.get( \\\"India\\\" ) : \" + capitals . get ( \"India\" )) } } // Example Output capitals . get ( \"France\" ) : Some ( Paris ) capitals.get ( \" India \" ) : None Iterators val it = Iterator ( \"a\" , \"number\" , \"of\" , \"words\" ) while ( it . hasNext ){ println ( it . next ()) } val ita = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) val itb = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) println ( \"Maximum valued element \" + ita . max ) println ( \"Minimum valued element \" + itb . min ) println ( \"Value of ita.size : \" + ita . size ) println ( \"Value of itb.length : \" + itb . length ) Conditionals/Matching If Statements var x = 10 ; if ( x < 20 ) 100 else if ( x > 20 ) 200 else 0 Pattern Matching import scala.util.Random val x : Int = Random . nextInt ( 10 ) x match { case 0 => \"zero\" case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } def matchTest ( x : Int ) : String = x match { case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } matchTest ( 3 ) // many matchTest ( 1 ) // one Loops For var a = 0 ; // for loop execution with a single range for ( a <- 1 to 10 ){ println ( \"Value of a: \" + a ); } var a = 0 ; var b = 0 ; // for loop execution with multiple ranges for ( a <- 1 to 3 ; b <- 1 to 3 ){ println ( \"Value of a: \" + a ); println ( \"Value of b: \" + b ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 ); // for loop execution with a collection for ( a <- numList ){ println ( \"Value of a: \" + a ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ); // for loop execution with multiple filters for ( a <- numList if a != 3 ; if a < 8 ){ println ( \"Value of a: \" + a ); } While var a = 10 ; // while loop execution while ( a < 20 ){ println ( \"Value of a: \" + a ); a = a + 1 ; }","title":"Scala"},{"location":"languages/scala/#scala","text":"","title":"Scala"},{"location":"languages/scala/#variable-declaration","text":"// immutable val donutsToBuy : Int = 5 // mutable var favoriteDonut : String = \"Glazed Donut\" favoriteDonut = \"Vanilla Donut\" // lazy (delay the initialization until consumed by application) // this also uses type inference - note the lack of : String! lazy val donutService = \"initialize some donut service\" // declare a variable with no initialization var leastFavoriteDonut : String = _ leastFavoriteDonut = \"Plain Donut\"","title":"Variable Declaration"},{"location":"languages/scala/#supported-types","text":"val donutsBought : Int = 5 val bigNumberOfDonuts : Long = 100000000L val smallNumberOfDonuts : Short = 1 val priceOfDonut : Double = 2.50 val donutPrice : Float = 2.50f val donutStoreName : String = \"allaboutscala Donut Store\" val donutByte : Byte = 0xa val donutFirstLetter : Char = 'D' val nothing : Unit = ()","title":"Supported Types"},{"location":"languages/scala/#collections","text":"","title":"Collections"},{"location":"languages/scala/#lists","text":"// List of Strings val fruit : List [ String ] = List ( \"apples\" , \"oranges\" , \"pears\" ) // List of Integers val nums : List [ Int ] = List ( 1 , 2 , 3 , 4 ) // Empty List. val empty : List [ Nothing ] = List () // Two dimensional list val dim : List [ List [ Int ]] = List ( List ( 1 , 0 , 0 ), List ( 0 , 1 , 0 ), List ( 0 , 0 , 1 ) ) // Basic Examples of common methods // another way of defining the fruit List above using cons (::) val fruit = \"apples\" :: ( \"oranges\" :: ( \"pears\" :: Nil )) val nums = Nil println ( \"Head of fruit : \" + fruit . head ) println ( \"Tail of fruit : \" + fruit . tail ) println ( \"Check if fruit is empty : \" + fruit . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty )","title":"Lists"},{"location":"languages/scala/#sets","text":"// Empty set of integer type var s : Set [ Int ] = Set () // Set of integer type var s : Set [ Int ] = Set ( 1 , 3 , 5 , 7 ) or var s = Set ( 1 , 3 , 5 , 7 ) // find common elements between two sets val num1 = Set ( 5 , 6 , 9 , 20 , 30 , 45 ) val num2 = Set ( 50 , 60 , 9 , 20 , 35 , 55 ) println ( \"num1.&(num2) : \" + num1 .&( num2 ) ) println ( \"num1.intersect(num2) : \" + num1 . intersect ( num2 ) )","title":"Sets"},{"location":"languages/scala/#maps","text":"// Empty hash table whose keys are strings and values are integers: var A : Map [ Char , Int ] = Map () // A map with keys and values. val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" ) // Basic examples of common methods val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" , \"peru\" -> \"#CD853F\" ) val nums : Map [ Int , Int ] = Map () println ( \"Keys in colors : \" + colors . keys ) println ( \"Values in colors : \" + colors . values ) println ( \"Check if colors is empty : \" + colors . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty )","title":"Maps"},{"location":"languages/scala/#tuples","text":"// Short version val t = ( 1 , \"hello\" , Console ) // Long version val t = new Tuple3 ( 1 , \"hello\" , Console ) // Example Usage val t = ( 4 , 3 , 2 , 1 ) val sum = t . _1 + t . _2 + t . _3 + t . _4 println ( \"Sum of elements: \" + sum ) val t = new Tuple3 ( 1 , \"hello\" , Console ) println ( \"Concatenated String: \" + t . toString () )","title":"Tuples"},{"location":"languages/scala/#options","text":"// An Option[T] can be either Some[T] or None object, which represents a missing value. // Example object Demo { def main ( args : Array [ String ]) { val capitals = Map ( \"France\" -> \"Paris\" , \"Japan\" -> \"Tokyo\" ) println ( \"capitals.get( \\\"France\\\" ) : \" + capitals . get ( \"France\" )) println ( \"capitals.get( \\\"India\\\" ) : \" + capitals . get ( \"India\" )) } } // Example Output capitals . get ( \"France\" ) : Some ( Paris ) capitals.get ( \" India \" ) : None","title":"Options"},{"location":"languages/scala/#iterators","text":"val it = Iterator ( \"a\" , \"number\" , \"of\" , \"words\" ) while ( it . hasNext ){ println ( it . next ()) } val ita = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) val itb = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) println ( \"Maximum valued element \" + ita . max ) println ( \"Minimum valued element \" + itb . min ) println ( \"Value of ita.size : \" + ita . size ) println ( \"Value of itb.length : \" + itb . length )","title":"Iterators"},{"location":"languages/scala/#conditionalsmatching","text":"","title":"Conditionals/Matching"},{"location":"languages/scala/#if-statements","text":"var x = 10 ; if ( x < 20 ) 100 else if ( x > 20 ) 200 else 0","title":"If Statements"},{"location":"languages/scala/#pattern-matching","text":"import scala.util.Random val x : Int = Random . nextInt ( 10 ) x match { case 0 => \"zero\" case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } def matchTest ( x : Int ) : String = x match { case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } matchTest ( 3 ) // many matchTest ( 1 ) // one","title":"Pattern Matching"},{"location":"languages/scala/#loops","text":"","title":"Loops"},{"location":"languages/scala/#for","text":"var a = 0 ; // for loop execution with a single range for ( a <- 1 to 10 ){ println ( \"Value of a: \" + a ); } var a = 0 ; var b = 0 ; // for loop execution with multiple ranges for ( a <- 1 to 3 ; b <- 1 to 3 ){ println ( \"Value of a: \" + a ); println ( \"Value of b: \" + b ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 ); // for loop execution with a collection for ( a <- numList ){ println ( \"Value of a: \" + a ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ); // for loop execution with multiple filters for ( a <- numList if a != 3 ; if a < 8 ){ println ( \"Value of a: \" + a ); }","title":"For"},{"location":"languages/scala/#while","text":"var a = 10 ; // while loop execution while ( a < 20 ){ println ( \"Value of a: \" + a ); a = a + 1 ; }","title":"While"},{"location":"publications/dev/","text":"Publications My primary publications are posted over at dev.together . You can access my main account for an up to date list of my recent activity/publications or review the ocassionally updated list below. 2018 - DEV.to Publications Visualizing Fibonacci: For the Music Lover in You! Leveling Up: From Create-React-App to Express (& some neat styling libraries) Building A Portfolio: The Painful Way The Inspiration API: A project built with Scala & Play Framework A year in Professional Software Development Integrating Docker with your Personal Projects Lucas - A Webscraper in Go","title":"Publications"},{"location":"publications/dev/#publications","text":"My primary publications are posted over at dev.together . You can access my main account for an up to date list of my recent activity/publications or review the ocassionally updated list below.","title":"Publications"},{"location":"publications/dev/#2018-devto-publications","text":"Visualizing Fibonacci: For the Music Lover in You! Leveling Up: From Create-React-App to Express (& some neat styling libraries) Building A Portfolio: The Painful Way The Inspiration API: A project built with Scala & Play Framework A year in Professional Software Development Integrating Docker with your Personal Projects Lucas - A Webscraper in Go","title":"2018 - DEV.to Publications"}]}