{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dan\u2019s Docs Who am I? Daniel McMahon . Connect Github LinkedIn Education Higher Diploma in Computer Science (Conversion) 2016-17 Professional Diploma in Education 2012-2013 BA Music & Geography 2009-2012 Work Software Engineer 2017-Present Education Officer 2013-2016 Student Teacher 2012-2013 Various Retail Locations 2007-2008 What are these docs and who are they for? (Me) => { Making notes & resources helps me concrete my own knowledge. Docs provide a useful and quick reference for day to day work. An excuse to experiment with a new documentation library. } (You) => { Maybe there is some useful knowledge in here relating to a language or tech concept that will help Maybe you would like to adopt something similar for your own purposes and will use this as a template } Why am I using mkdocs? It\u2019s quick and easy. I wanted to have a docs page I can quickly update when the mood strikes. Being able to update the documentation using markdown and not worrying about custom styling and css is crucial to achieve this. Once I have my knowledge solidified in a markdown structure I may consider switching to something like Gatsby or Hugo. Having previously designed page search functionality, it\u2019s also so much easier to use a pre-made solution like mkdocs that has search integrated. There\u2019s no point having all these resources if I can\u2019t search for what I need quickly and efficiently now is there?","title":"Home"},{"location":"#dans-docs","text":"","title":"Dan's Docs"},{"location":"#who-am-i","text":"Daniel McMahon . Connect Github LinkedIn Education Higher Diploma in Computer Science (Conversion) 2016-17 Professional Diploma in Education 2012-2013 BA Music & Geography 2009-2012 Work Software Engineer 2017-Present Education Officer 2013-2016 Student Teacher 2012-2013 Various Retail Locations 2007-2008","title":"Who am I?"},{"location":"#what-are-these-docs-and-who-are-they-for","text":"(Me) => { Making notes & resources helps me concrete my own knowledge. Docs provide a useful and quick reference for day to day work. An excuse to experiment with a new documentation library. } (You) => { Maybe there is some useful knowledge in here relating to a language or tech concept that will help Maybe you would like to adopt something similar for your own purposes and will use this as a template }","title":"What are these docs and who are they for?"},{"location":"#why-am-i-using-mkdocs","text":"It\u2019s quick and easy. I wanted to have a docs page I can quickly update when the mood strikes. Being able to update the documentation using markdown and not worrying about custom styling and css is crucial to achieve this. Once I have my knowledge solidified in a markdown structure I may consider switching to something like Gatsby or Hugo. Having previously designed page search functionality, it\u2019s also so much easier to use a pre-made solution like mkdocs that has search integrated. There\u2019s no point having all these resources if I can\u2019t search for what I need quickly and efficiently now is there?","title":"Why am I using mkdocs?"},{"location":"aws/aws/","text":"Amazon Web Services The following section outlines notes taken during the AWS Certified Developer - Associate course run via Udemy.","title":"AWS"},{"location":"aws/aws/#amazon-web-services","text":"The following section outlines notes taken during the AWS Certified Developer - Associate course run via Udemy.","title":"Amazon Web Services"},{"location":"aws/cli/","text":"AWS CLI SSH/Putty into EC2 instance aws s3 ls aws configure [ copy paste details from programmatic IAM user ] aws s3 mb s3://experimenting-aws [ make bucket ] echo \"Hello world\" > hello.txt aws s3 cp ./hello.txt s3://experimenting-aws aws s3 ls s3://experimenting-aws AWS CLI reference: https://docs.aws.amazon.com/cli/latest/index.html Exam Tips: Least Privilege - always give your users the minimum amount of access required Create Groups - assign users to groups. Your users with auto inherit the permissions of the group - the groups permissions are assigned using policy documents. Secret Access Key - you will see this only once - if you do not save it you can delete the key pair (access key id and secret access key) and regenerate it. You will need to run aws configure again. Do not use just one access key - do not create just one access key and share that with all your developers. If someone leaves the company on bad terms, then you will need to delete the key and create a new one and every developer would then need to update their keys. Instead create on key pair per developer. You can use the CLI on your PC [Mac, Linux or Windows]","title":"CLI"},{"location":"aws/cli/#aws-cli","text":"","title":"AWS CLI"},{"location":"aws/cli/#sshputty-into-ec2-instance","text":"aws s3 ls aws configure [ copy paste details from programmatic IAM user ] aws s3 mb s3://experimenting-aws [ make bucket ] echo \"Hello world\" > hello.txt aws s3 cp ./hello.txt s3://experimenting-aws aws s3 ls s3://experimenting-aws AWS CLI reference: https://docs.aws.amazon.com/cli/latest/index.html","title":"SSH/Putty into EC2 instance"},{"location":"aws/cli/#exam-tips","text":"Least Privilege - always give your users the minimum amount of access required Create Groups - assign users to groups. Your users with auto inherit the permissions of the group - the groups permissions are assigned using policy documents. Secret Access Key - you will see this only once - if you do not save it you can delete the key pair (access key id and secret access key) and regenerate it. You will need to run aws configure again. Do not use just one access key - do not create just one access key and share that with all your developers. If someone leaves the company on bad terms, then you will need to delete the key and create a new one and every developer would then need to update their keys. Instead create on key pair per developer. You can use the CLI on your PC [Mac, Linux or Windows]","title":"Exam Tips:"},{"location":"aws/dynamo/","text":"Dynamo DB A fast and flexible noSQL DB for applications that need consistent, single-digit millisecond latency at any scale. Fully managed DB, can be configured to autoscale, integrates well with Lambda. Supports both document and key-value data models Flexible data model and reliable performance. Stored on SSD Spread across 3 geographically distinct dat centres Choice of 2 consistency models: Eventual Consistent Reads (Default) Strongly Consistent Reads Reads Eventually Consistent Reads Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance) Strongly Consistent Reads A strongly consistent read returns a result that reflects all writes that received a successful response prior to the end Structure Tables Items Attributes Supports key-value and document data structures Key = Name of the data, Value = Data itself Documents can be written in JSON, HTML or XML Primary Keys Dynamo DB Stores and retrieves data based on Primary Key - 2 Types: Partition Key - unique attribute (e.g. user ID): Value of partition key is input to an internal hash function which determines the partition or physical location on which the data is Stored If you are using the partition key as your primary key, then no two items can have the same partition key. Composite Key (Partition key + Sort Key) in combination: Primary key would be a composite key consisting of Partition Key - User ID Sort Key - Timestamp of Post 2 items may have the same Partition key, but they must have a different sort key. All items with the same Partition Key are stored together, then sorted according to the Sort key Value Allows you to store multiple items with the same Partition Key Access Control Authentication and Access Control is managed using AWS IAM You can create an IAM user within your AWS account which has specific permissions to access and create DynamoDB tables. You can create an IAM role which enables you to obtain temporary access keys which can be used to access DynamoDB You can also use a special IAM Condition to restrict user access to only their own records IAM Conditions Example Imagine a mobile gaming app with millions of users: Users need to access high scores for each game they are playing Access must be restricted to ensure they cannot view anyone else\u2019s data This can be done by adding a Condition to an IAM Policy to allow access only to items where the Partition Key value matches their User ID. Exam Tips - Dynamo DB DynamoDB is a low latency noSQL DB Consists of Tables Items and Attributes Supports both document and key-value data models Supported document formats are JSON, HTML, XML 2 types of Primary key - Partition Key and combination of Partition Key + Sort Key (Composite Key) 2 consistency models: Strongly Consistent / Eventually Consistent Access is controlled using IAM policies Fine grained access control using IAM Condition parameter: dynamodb:LeadingKeys to allow users to access only the items where the partition key value matches their user ID Useful Example Reference with PHP related scripts. Sample CLI Commands aws dynamodb get-item --table-name ProductCatalog --key '{\"Id\": {\"N\":\"205\"}}' Index In SQL DBs an index is a data structure which allows you to perform fast queries on specific columns in a table. You select the columns that you want included in the index and run your searches on the index - rather than the entire dataset. Dynamo DB has two types of Index (even though its NoSQL): 1. Local Secondary Index 2. Global Secondary Index Local Secondary Index Can only be created when you are creating your table You cannot add, remove or modify it later It has the same partition key as your original table But a different Sort Key Gives you a different view of your data, organized according to an alternative Sort Key Any queries based on this Sort Key are much faster using the index than the main table e.g. Partition Key: User ID, Sort Key: account creation date Global Secondary Index You can create when you create your table, or add it later Different Partition Key as well as Sort Key Gives a completely different view of the data Speeds up any queries related to this alternative partition and sort key e.g. Partition Key: email address, Sort Key: last log-in date Exam Tips - Indexes Enable fast queries on specific data columns Give you a different view of your data, based on alternative Partition / Sort Keys Important to understand difference Local Secondary Index Global Secondary Index Must be created at same time table is created Can create at any time (including table creation) Same Partition Key as your Table Different Partition Key Different Sort Key Different Sort Key Query & Scan Query A query operation finds item in a table based on the Primary Key attribute and a distinct value to search for e.g. select and item where the user ID is equal to 212, will select all attributes for that name e.g. first name, surname, email etc. Use an optional Sort Key name and value to refine the results e.g. if Sort Key is a timestamp, you can refine query to only select items from last 7 days By default a query returns all the attributes for the items but you can use the ProjectionExpression parameter if you want the query to only return the specific attributes you want e.g. if you only want to see the email address rather than all the attributes Results are always sorted by the Sort Key Numeric order - by default in ascending order (1,2,3,4) ASCII character code values You can reverse the order by setting the ScanIndexForward parameter to false - (not this param is only related to queries no scan) By default, Queries are Eventually Consistent You need to explicitly set the query to be Strongly Consistent Scan A scan operation examines every item in the table. By default returns all data Attributes Use the ProjectionExpression parameter to refine the scan to return only the attributes you want Query or Scan? Query is more efficient than a Scan Scan dumps the entire table, then filters out the values to provide the desired result - removing the unwanted data This adds an extra step of removing the data you don\u2019t want As the table grows, the scan operation takes longer Scan operation on a larger table can use up the provisioned throughput for a large table in just a single operation How to Improve Performance You can reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations e.g. set the page size to return 40 Items Larger number of smaller operations will allow other requests to succeed without throttling Avoid using scan operations if you can: design tables in a way that you can use Query, Get, or BatchGetItem APIs By default, a scan operation processes data sequentially in returning 1MB increments before moving on to retrieve the next 1MB of data. It can only scan one partition at a time. You can configure DynamoDB to use Parallel scans instead by logically dividing a table or index into segments and scanning each segment in parallel Best to avoid parallel scans if your table or index is already incurring heavy read / write activity from other applications Exam Tips - Scan -v- Query A Query operation finds items in a table using only the Primary Key attribute -> You provide the primary key and a distinct value to search for A scan operation examines every item in the table -> By default returns all data attributes Use the ProjectionExpression parameter to refine the results Query results always sorted by Sort Key (if there is one) Sorted in ascending order Set ScanIndexForward parameter to false to reverse the order - queries only Query operation is generally more efficient than a Scan Reduce the impact of a query of scan by setting a smaller page size which uses fewer read operations Isolate scan operations to specific tables and segregate them from your mission-critical traffic Try Parallel scans, rather than the default sequential scan Avoid using scan operations if you can: design tables in a way that you can use the Query, Get, or BatchGetItem APIs Provisioned Throughput Dynamo DB Provisioned Throughput is measure in Capacity Units. When you create your table, you specify your requirements in terms of Read Capacity Units and Write Capacity Units 1x Write Capacity Unit = 1x 1KB write per second 1x Read Capacity Unit = 1x Strongly Consistent Read of 4KB per second OR 2x Eventually ConsistentReads of 4KB per second (Default) Strongly Consistent Reads Calculation Your application needs to read 80 items (table rows) per second. Each item is 3KB in size. You need Strongly consistent reads. First: Calculate how many Read Capacity Units needed for each read: size of each item / 4KB i.e. 3KB/4Kb = 0.75. Rounded up to the nearest whole number, each read will need 1x Read Capacity Unit per read operation. Multiplied by the number of reads per second = 80 Read Capacity Units required Eventually Consistent Reads Calculation Same as above BUT 2x 4KB reads per second - double the throughput of Strongly Consistent Reads 3KB/4KB = 0.75, round to nearest whole number = 1 Multiply by number of reads per second = 80 Divided 80 by 2, only need 40 read capacity units for Eventually Consistent Reads Write Capacity Unit Calculation You want to write 100 items per second. Each item 512 bytes in size First: Calculate how many Capacity Units for each write: Size of each item /1KB (for Write CU) 512 bytes / 1KB = 0.5 Round to nearest whole number = 1 Write Capacity Unit per write operation operation Multiplied by number of writes per second = 100 Write Capacity Units required Exam Tips - Provisioned throughput Measured in Capacity Units 1x Write Capacity Unit = 1x 1KB Write per Second 1x Read Capacity Unit = 1x 4KB Strongly Consistent Read OR 2x 4KB Eventually Consistent Read On Demand Capacity Charges apply for: Reading, Writing, Storing data Don\u2019t need to specify your requirements DynamoDB instantly scales up and down based on the activity of your application Great for unpredictable workloads You want to pay for only what you use (pay per request) Which Pricing Model to Use On-Demand Capacity Provisioned Capacity Unknown Workloads Forecast read & write capacity requirements Unpredictable Application Traffic Predictable Application Traffic Want Pay-Per-Use Model App traffic is consistent or increases gradually Spikey/Short-lived Projects DAX (Dynamo DB Accellorator) A fully managed, clustered in-memory cache for DynamoDB Delivers up to 10x read performance improvement Microsecond performance for millions of requests per second * Ideal for Read-Heavy and bursty workloads e.g. auction applications, gaming and retail sites during black Friday promotions How DAX Works Dax is a write-through caching service - data is written to the Cache as well as the back end store at the same time Allows you to point your DynamoDB API calls at the DAX cluster If the item you are querying is in the cache (cache hit), DAX returns the result to the application If the item is not available (cache miss) then DAX performs an Eventually Consistent GetItem operation against DynamoDB Retrieval of data from DAX reduces the read load on DynamoDB tables May be able to reduce Provisioned Read Capacity What DAX is not suitable for Caters for Eventually Consistent reads only - no not suitable for applications that require Strongly Consistent Reads Write intensive applications Applications that do not perform many read operations Applications that do not require microsecond response times Exam Tips - DAX Provides in-memory caching for DynamoDB tables Improves response times for Eventually Consistent reads only You point your API calls to the DAX cluster instead of your table If item you are querying is on the cache, DAX with return it; otherwise it will perform an Eventually Consistent GetItem operation to your DynamoDB table Not suitable for write-intensive applications or applications that require Strongly Consistent Reads Elasticache In memory cache in the cloud Improves performance of web apps, allowing retrieval of info from fast in-memory caches rather than slower disk based DBs. Sits between your app and the DB e.g. application frequently requesting specific product information for your best selling products Takes the load off your DBs Good if DB is particularly read-heavy and the data is not changing frequently Benefits & Use Cases Improves performance for read-heavy workloads e.g. Social Networking, gaming, Q&A Portals Frequently-accessed dat is store din memory for low-latency access, improving overall performance of your app. Also good for compute heavy workloads e.g. recommendation engines Can be used to store results of I/O intensive DB queries or output of compute-intensive calculations. Types Memcached Wiedly adopted memory object cachign system Multi threaded No Multi-AZ capability Redis Open-source in-memory key-value store Supports more complex data structures: sorted sets and lists Supports Master / SLave replicatoin and Multi-AZ for cross AZ redundancy Caching Strategies 2 strategies available: Lazy Loading Loads data into the cache only when necessary If requested data is in the cache, Elasticache returns the data to the application If the data is not in the cache or has expired, Elasticache returns a null Your application then fetches the data from the DB and writes the data received into the cache so that it is available next time. Advantages Disadvantages Only requested data cached: Avoids filling up cache with useless data Cache miss penalty: initial request, query to DB, writing of data to the cache Node failures are not fatal a new empty node will just have a lot of cache misses initially Stale data - if data is only updated when there is a cache miss, it can become stale. Doesn\u2019t automatically update if the data in the database changes Lazy Loading TTL (Time To Live) Specifies number of seconds until the key (data) expires to avoid keeping stale data in the cache Lazy loading treats an expired key as a cache miss and causes the application to retrieve the data from the DB and subsequently write the data into the cache with a new TTL Does not eliminate stale data - but helps to avoid it Write-Through Adds or updates data to the cache whenever data is written to the database Advantages Disadvantages Data in the cache never stale Write Penalty: every write involves a write to the cache as well as a write to the DB Users are generally more tolerant of additional latency when updating data than when retrieving it If a node fails and a new one is spun up, data is missing until added or updated in the DB (mitigate by implementing Lazy Loading in conjunction with write-through) Wasted resources if most of the data is never read Exam Tips - Elasticache In-memory cache sites between your application and DB 2 different caching strats - Lazy Loading & Write Through Lazy loading only caches data when it is requested Elasticache Node failures not fatal, just lots of cache misses Cache miss penalty: Initial request, query DB, writing to cache Avoid stale data by implementing a TTL Write through strategy writes data into the cache whenever there is a change to the DB Data is never Stale Write penalty: each write involves a write to the cache Elasticache node failure means that data is missing until added or updated in the DB Wasted resources if most of the data is never used DynamoDB Transactions ACID Transactions (Atomic, Consistent, Isolated, Durable) Read or write multiple items across multiple tables as an all or nothing operations Check for a pre-requisite condition before writing to a table DynamoDB TTL TTL attribute defines an expiry time for your data Expired items marked for deletion Great for removing irrelevant or old data: Session data Event logs Temporary data Reduces cost by automatically removing data which is no longer relevant TTL expressed as epoch time i.e. when current time > TTL item expired and marked for deletion Sample Commands # ensure you have right IAM role & access permissions aws iam get-user # create sessiondata table aws dynamodb create-table --table-name SessionData --attribute-defintiions \\ AttributeName = UserID,AttributeType = N --key-schema \\ AttributeName = UserID,KeyType = HASH \\ --provisioned-throughput ReadCapacityUnits = 5 ,WriteCapacityUnits = 5 # populate SessionData table aws dynamodb bach-write-item --request-items file://items.json DynamoDB Streams Time-ordered sequence of item level modifications (insert, update, delete) Logs are encrypted at rest and stored for 24hrs Accessed using a dedicated endpoint By default the Primary Key is recorded Before and After images can be captured Processing Streams Events are recorded in near real-time Apps can take actions based on contents of stream Event source for Lambda Lambda polls the DynamoDB Stream Executes Lambda code based on a DynamoDB Streams event Exam Tips - DynamoDB Streams Time-ordered sequence of item level modifications in your DynamoDB Tables Data is stored for 24 hours only Can be used as an event source for Lambda so you can create applications which take actions based on events in your DynamoDB table Provisioned Throughput Exceeded & Exponential Backoff ProvisionedThroughputExceededException Your request rate is too high for the read/write capacity provisioned on your DynamoDB table SDK will automatically retries the requests until successful If you are not using the SDK you can: Reduce request frequency Use Exponential Backoff Exponential Backoff Many components in a network can generate errors due to being overloaded In addition to simple retries all AWS SDKs use Exponential Backoff Progressively longer waits between consecutive retries e.g. 50ms, 100ms, 200ms for improved flow control If after 1 minute this doesn\u2019t work, your request size may be exceeding the throughput for your read/write capacity Exam Tips - Provisioned Throughput & Exponential Backoff If you see a ProvisionedThoguhputExceeded Error, this means the number of requests is too high Exponential Backoff improves flow by retrying requests using progressively longer waits This is not just true for DynamoDB, Exponential Backoff is a feature of every AWS SDK and applies to many services within AWS e.g. S#, CloudFormation, SES","title":"Dynamo DB"},{"location":"aws/dynamo/#dynamo-db","text":"A fast and flexible noSQL DB for applications that need consistent, single-digit millisecond latency at any scale. Fully managed DB, can be configured to autoscale, integrates well with Lambda. Supports both document and key-value data models Flexible data model and reliable performance. Stored on SSD Spread across 3 geographically distinct dat centres Choice of 2 consistency models: Eventual Consistent Reads (Default) Strongly Consistent Reads","title":"Dynamo DB"},{"location":"aws/dynamo/#reads","text":"","title":"Reads"},{"location":"aws/dynamo/#eventually-consistent-reads","text":"Consistency across all copies of data is usually reached within a second. Repeating a read after a short time should return the updated data. (Best Read Performance)","title":"Eventually Consistent Reads"},{"location":"aws/dynamo/#strongly-consistent-reads","text":"A strongly consistent read returns a result that reflects all writes that received a successful response prior to the end","title":"Strongly Consistent Reads"},{"location":"aws/dynamo/#structure","text":"Tables Items Attributes Supports key-value and document data structures Key = Name of the data, Value = Data itself Documents can be written in JSON, HTML or XML","title":"Structure"},{"location":"aws/dynamo/#primary-keys","text":"Dynamo DB Stores and retrieves data based on Primary Key - 2 Types: Partition Key - unique attribute (e.g. user ID): Value of partition key is input to an internal hash function which determines the partition or physical location on which the data is Stored If you are using the partition key as your primary key, then no two items can have the same partition key. Composite Key (Partition key + Sort Key) in combination: Primary key would be a composite key consisting of Partition Key - User ID Sort Key - Timestamp of Post 2 items may have the same Partition key, but they must have a different sort key. All items with the same Partition Key are stored together, then sorted according to the Sort key Value Allows you to store multiple items with the same Partition Key","title":"Primary Keys"},{"location":"aws/dynamo/#access-control","text":"Authentication and Access Control is managed using AWS IAM You can create an IAM user within your AWS account which has specific permissions to access and create DynamoDB tables. You can create an IAM role which enables you to obtain temporary access keys which can be used to access DynamoDB You can also use a special IAM Condition to restrict user access to only their own records","title":"Access Control"},{"location":"aws/dynamo/#iam-conditions-example","text":"Imagine a mobile gaming app with millions of users: Users need to access high scores for each game they are playing Access must be restricted to ensure they cannot view anyone else\u2019s data This can be done by adding a Condition to an IAM Policy to allow access only to items where the Partition Key value matches their User ID.","title":"IAM Conditions Example"},{"location":"aws/dynamo/#exam-tips-dynamo-db","text":"DynamoDB is a low latency noSQL DB Consists of Tables Items and Attributes Supports both document and key-value data models Supported document formats are JSON, HTML, XML 2 types of Primary key - Partition Key and combination of Partition Key + Sort Key (Composite Key) 2 consistency models: Strongly Consistent / Eventually Consistent Access is controlled using IAM policies Fine grained access control using IAM Condition parameter: dynamodb:LeadingKeys to allow users to access only the items where the partition key value matches their user ID Useful Example Reference with PHP related scripts.","title":"Exam Tips - Dynamo DB"},{"location":"aws/dynamo/#sample-cli-commands","text":"aws dynamodb get-item --table-name ProductCatalog --key '{\"Id\": {\"N\":\"205\"}}'","title":"Sample CLI Commands"},{"location":"aws/dynamo/#index","text":"In SQL DBs an index is a data structure which allows you to perform fast queries on specific columns in a table. You select the columns that you want included in the index and run your searches on the index - rather than the entire dataset. Dynamo DB has two types of Index (even though its NoSQL): 1. Local Secondary Index 2. Global Secondary Index","title":"Index"},{"location":"aws/dynamo/#local-secondary-index","text":"Can only be created when you are creating your table You cannot add, remove or modify it later It has the same partition key as your original table But a different Sort Key Gives you a different view of your data, organized according to an alternative Sort Key Any queries based on this Sort Key are much faster using the index than the main table e.g. Partition Key: User ID, Sort Key: account creation date","title":"Local Secondary Index"},{"location":"aws/dynamo/#global-secondary-index","text":"You can create when you create your table, or add it later Different Partition Key as well as Sort Key Gives a completely different view of the data Speeds up any queries related to this alternative partition and sort key e.g. Partition Key: email address, Sort Key: last log-in date","title":"Global Secondary Index"},{"location":"aws/dynamo/#exam-tips-indexes","text":"Enable fast queries on specific data columns Give you a different view of your data, based on alternative Partition / Sort Keys Important to understand difference Local Secondary Index Global Secondary Index Must be created at same time table is created Can create at any time (including table creation) Same Partition Key as your Table Different Partition Key Different Sort Key Different Sort Key","title":"Exam Tips - Indexes"},{"location":"aws/dynamo/#query-scan","text":"","title":"Query &amp; Scan"},{"location":"aws/dynamo/#query","text":"A query operation finds item in a table based on the Primary Key attribute and a distinct value to search for e.g. select and item where the user ID is equal to 212, will select all attributes for that name e.g. first name, surname, email etc. Use an optional Sort Key name and value to refine the results e.g. if Sort Key is a timestamp, you can refine query to only select items from last 7 days By default a query returns all the attributes for the items but you can use the ProjectionExpression parameter if you want the query to only return the specific attributes you want e.g. if you only want to see the email address rather than all the attributes Results are always sorted by the Sort Key Numeric order - by default in ascending order (1,2,3,4) ASCII character code values You can reverse the order by setting the ScanIndexForward parameter to false - (not this param is only related to queries no scan) By default, Queries are Eventually Consistent You need to explicitly set the query to be Strongly Consistent","title":"Query"},{"location":"aws/dynamo/#scan","text":"A scan operation examines every item in the table. By default returns all data Attributes Use the ProjectionExpression parameter to refine the scan to return only the attributes you want","title":"Scan"},{"location":"aws/dynamo/#query-or-scan","text":"Query is more efficient than a Scan Scan dumps the entire table, then filters out the values to provide the desired result - removing the unwanted data This adds an extra step of removing the data you don\u2019t want As the table grows, the scan operation takes longer Scan operation on a larger table can use up the provisioned throughput for a large table in just a single operation","title":"Query or Scan?"},{"location":"aws/dynamo/#how-to-improve-performance","text":"You can reduce the impact of a query or scan by setting a smaller page size which uses fewer read operations e.g. set the page size to return 40 Items Larger number of smaller operations will allow other requests to succeed without throttling Avoid using scan operations if you can: design tables in a way that you can use Query, Get, or BatchGetItem APIs By default, a scan operation processes data sequentially in returning 1MB increments before moving on to retrieve the next 1MB of data. It can only scan one partition at a time. You can configure DynamoDB to use Parallel scans instead by logically dividing a table or index into segments and scanning each segment in parallel Best to avoid parallel scans if your table or index is already incurring heavy read / write activity from other applications","title":"How to Improve Performance"},{"location":"aws/dynamo/#exam-tips-scan-v-query","text":"A Query operation finds items in a table using only the Primary Key attribute -> You provide the primary key and a distinct value to search for A scan operation examines every item in the table -> By default returns all data attributes Use the ProjectionExpression parameter to refine the results Query results always sorted by Sort Key (if there is one) Sorted in ascending order Set ScanIndexForward parameter to false to reverse the order - queries only Query operation is generally more efficient than a Scan Reduce the impact of a query of scan by setting a smaller page size which uses fewer read operations Isolate scan operations to specific tables and segregate them from your mission-critical traffic Try Parallel scans, rather than the default sequential scan Avoid using scan operations if you can: design tables in a way that you can use the Query, Get, or BatchGetItem APIs","title":"Exam Tips - Scan -v- Query"},{"location":"aws/dynamo/#provisioned-throughput","text":"Dynamo DB Provisioned Throughput is measure in Capacity Units. When you create your table, you specify your requirements in terms of Read Capacity Units and Write Capacity Units 1x Write Capacity Unit = 1x 1KB write per second 1x Read Capacity Unit = 1x Strongly Consistent Read of 4KB per second OR 2x Eventually ConsistentReads of 4KB per second (Default)","title":"Provisioned Throughput"},{"location":"aws/dynamo/#strongly-consistent-reads-calculation","text":"Your application needs to read 80 items (table rows) per second. Each item is 3KB in size. You need Strongly consistent reads. First: Calculate how many Read Capacity Units needed for each read: size of each item / 4KB i.e. 3KB/4Kb = 0.75. Rounded up to the nearest whole number, each read will need 1x Read Capacity Unit per read operation. Multiplied by the number of reads per second = 80 Read Capacity Units required","title":"Strongly Consistent Reads Calculation"},{"location":"aws/dynamo/#eventually-consistent-reads-calculation","text":"Same as above BUT 2x 4KB reads per second - double the throughput of Strongly Consistent Reads 3KB/4KB = 0.75, round to nearest whole number = 1 Multiply by number of reads per second = 80 Divided 80 by 2, only need 40 read capacity units for Eventually Consistent Reads","title":"Eventually Consistent Reads Calculation"},{"location":"aws/dynamo/#write-capacity-unit-calculation","text":"You want to write 100 items per second. Each item 512 bytes in size First: Calculate how many Capacity Units for each write: Size of each item /1KB (for Write CU) 512 bytes / 1KB = 0.5 Round to nearest whole number = 1 Write Capacity Unit per write operation operation Multiplied by number of writes per second = 100 Write Capacity Units required","title":"Write Capacity Unit Calculation"},{"location":"aws/dynamo/#exam-tips-provisioned-throughput","text":"Measured in Capacity Units 1x Write Capacity Unit = 1x 1KB Write per Second 1x Read Capacity Unit = 1x 4KB Strongly Consistent Read OR 2x 4KB Eventually Consistent Read","title":"Exam Tips - Provisioned throughput"},{"location":"aws/dynamo/#on-demand-capacity","text":"Charges apply for: Reading, Writing, Storing data Don\u2019t need to specify your requirements DynamoDB instantly scales up and down based on the activity of your application Great for unpredictable workloads You want to pay for only what you use (pay per request)","title":"On Demand Capacity"},{"location":"aws/dynamo/#which-pricing-model-to-use","text":"On-Demand Capacity Provisioned Capacity Unknown Workloads Forecast read & write capacity requirements Unpredictable Application Traffic Predictable Application Traffic Want Pay-Per-Use Model App traffic is consistent or increases gradually Spikey/Short-lived Projects","title":"Which Pricing Model to Use"},{"location":"aws/dynamo/#dax-dynamo-db-accellorator","text":"A fully managed, clustered in-memory cache for DynamoDB Delivers up to 10x read performance improvement Microsecond performance for millions of requests per second * Ideal for Read-Heavy and bursty workloads e.g. auction applications, gaming and retail sites during black Friday promotions","title":"DAX (Dynamo DB Accellorator)"},{"location":"aws/dynamo/#how-dax-works","text":"Dax is a write-through caching service - data is written to the Cache as well as the back end store at the same time Allows you to point your DynamoDB API calls at the DAX cluster If the item you are querying is in the cache (cache hit), DAX returns the result to the application If the item is not available (cache miss) then DAX performs an Eventually Consistent GetItem operation against DynamoDB Retrieval of data from DAX reduces the read load on DynamoDB tables May be able to reduce Provisioned Read Capacity","title":"How DAX Works"},{"location":"aws/dynamo/#what-dax-is-not-suitable-for","text":"Caters for Eventually Consistent reads only - no not suitable for applications that require Strongly Consistent Reads Write intensive applications Applications that do not perform many read operations Applications that do not require microsecond response times","title":"What DAX is not suitable for"},{"location":"aws/dynamo/#exam-tips-dax","text":"Provides in-memory caching for DynamoDB tables Improves response times for Eventually Consistent reads only You point your API calls to the DAX cluster instead of your table If item you are querying is on the cache, DAX with return it; otherwise it will perform an Eventually Consistent GetItem operation to your DynamoDB table Not suitable for write-intensive applications or applications that require Strongly Consistent Reads","title":"Exam Tips - DAX"},{"location":"aws/dynamo/#elasticache","text":"In memory cache in the cloud Improves performance of web apps, allowing retrieval of info from fast in-memory caches rather than slower disk based DBs. Sits between your app and the DB e.g. application frequently requesting specific product information for your best selling products Takes the load off your DBs Good if DB is particularly read-heavy and the data is not changing frequently","title":"Elasticache"},{"location":"aws/dynamo/#benefits-use-cases","text":"Improves performance for read-heavy workloads e.g. Social Networking, gaming, Q&A Portals Frequently-accessed dat is store din memory for low-latency access, improving overall performance of your app. Also good for compute heavy workloads e.g. recommendation engines Can be used to store results of I/O intensive DB queries or output of compute-intensive calculations.","title":"Benefits &amp; Use Cases"},{"location":"aws/dynamo/#types","text":"","title":"Types"},{"location":"aws/dynamo/#memcached","text":"Wiedly adopted memory object cachign system Multi threaded No Multi-AZ capability","title":"Memcached"},{"location":"aws/dynamo/#redis","text":"Open-source in-memory key-value store Supports more complex data structures: sorted sets and lists Supports Master / SLave replicatoin and Multi-AZ for cross AZ redundancy","title":"Redis"},{"location":"aws/dynamo/#caching-strategies","text":"2 strategies available:","title":"Caching Strategies"},{"location":"aws/dynamo/#lazy-loading","text":"Loads data into the cache only when necessary If requested data is in the cache, Elasticache returns the data to the application If the data is not in the cache or has expired, Elasticache returns a null Your application then fetches the data from the DB and writes the data received into the cache so that it is available next time. Advantages Disadvantages Only requested data cached: Avoids filling up cache with useless data Cache miss penalty: initial request, query to DB, writing of data to the cache Node failures are not fatal a new empty node will just have a lot of cache misses initially Stale data - if data is only updated when there is a cache miss, it can become stale. Doesn\u2019t automatically update if the data in the database changes","title":"Lazy Loading"},{"location":"aws/dynamo/#lazy-loading-ttl-time-to-live","text":"Specifies number of seconds until the key (data) expires to avoid keeping stale data in the cache Lazy loading treats an expired key as a cache miss and causes the application to retrieve the data from the DB and subsequently write the data into the cache with a new TTL Does not eliminate stale data - but helps to avoid it","title":"Lazy Loading TTL (Time To Live)"},{"location":"aws/dynamo/#write-through","text":"Adds or updates data to the cache whenever data is written to the database Advantages Disadvantages Data in the cache never stale Write Penalty: every write involves a write to the cache as well as a write to the DB Users are generally more tolerant of additional latency when updating data than when retrieving it If a node fails and a new one is spun up, data is missing until added or updated in the DB (mitigate by implementing Lazy Loading in conjunction with write-through) Wasted resources if most of the data is never read","title":"Write-Through"},{"location":"aws/dynamo/#exam-tips-elasticache","text":"In-memory cache sites between your application and DB 2 different caching strats - Lazy Loading & Write Through Lazy loading only caches data when it is requested Elasticache Node failures not fatal, just lots of cache misses Cache miss penalty: Initial request, query DB, writing to cache Avoid stale data by implementing a TTL Write through strategy writes data into the cache whenever there is a change to the DB Data is never Stale Write penalty: each write involves a write to the cache Elasticache node failure means that data is missing until added or updated in the DB Wasted resources if most of the data is never used","title":"Exam Tips - Elasticache"},{"location":"aws/dynamo/#dynamodb-transactions","text":"ACID Transactions (Atomic, Consistent, Isolated, Durable) Read or write multiple items across multiple tables as an all or nothing operations Check for a pre-requisite condition before writing to a table","title":"DynamoDB Transactions"},{"location":"aws/dynamo/#dynamodb-ttl","text":"TTL attribute defines an expiry time for your data Expired items marked for deletion Great for removing irrelevant or old data: Session data Event logs Temporary data Reduces cost by automatically removing data which is no longer relevant TTL expressed as epoch time i.e. when current time > TTL item expired and marked for deletion","title":"DynamoDB TTL"},{"location":"aws/dynamo/#sample-commands","text":"# ensure you have right IAM role & access permissions aws iam get-user # create sessiondata table aws dynamodb create-table --table-name SessionData --attribute-defintiions \\ AttributeName = UserID,AttributeType = N --key-schema \\ AttributeName = UserID,KeyType = HASH \\ --provisioned-throughput ReadCapacityUnits = 5 ,WriteCapacityUnits = 5 # populate SessionData table aws dynamodb bach-write-item --request-items file://items.json","title":"Sample Commands"},{"location":"aws/dynamo/#dynamodb-streams","text":"Time-ordered sequence of item level modifications (insert, update, delete) Logs are encrypted at rest and stored for 24hrs Accessed using a dedicated endpoint By default the Primary Key is recorded Before and After images can be captured","title":"DynamoDB Streams"},{"location":"aws/dynamo/#processing-streams","text":"Events are recorded in near real-time Apps can take actions based on contents of stream Event source for Lambda Lambda polls the DynamoDB Stream Executes Lambda code based on a DynamoDB Streams event","title":"Processing Streams"},{"location":"aws/dynamo/#exam-tips-dynamodb-streams","text":"Time-ordered sequence of item level modifications in your DynamoDB Tables Data is stored for 24 hours only Can be used as an event source for Lambda so you can create applications which take actions based on events in your DynamoDB table","title":"Exam Tips - DynamoDB Streams"},{"location":"aws/dynamo/#provisioned-throughput-exceeded-exponential-backoff","text":"ProvisionedThroughputExceededException Your request rate is too high for the read/write capacity provisioned on your DynamoDB table SDK will automatically retries the requests until successful If you are not using the SDK you can: Reduce request frequency Use Exponential Backoff","title":"Provisioned Throughput Exceeded &amp; Exponential Backoff"},{"location":"aws/dynamo/#exponential-backoff","text":"Many components in a network can generate errors due to being overloaded In addition to simple retries all AWS SDKs use Exponential Backoff Progressively longer waits between consecutive retries e.g. 50ms, 100ms, 200ms for improved flow control If after 1 minute this doesn\u2019t work, your request size may be exceeding the throughput for your read/write capacity","title":"Exponential Backoff"},{"location":"aws/dynamo/#exam-tips-provisioned-throughput-exponential-backoff","text":"If you see a ProvisionedThoguhputExceeded Error, this means the number of requests is too high Exponential Backoff improves flow by retrying requests using progressively longer waits This is not just true for DynamoDB, Exponential Backoff is a feature of every AWS SDK and applies to many services within AWS e.g. S#, CloudFormation, SES","title":"Exam Tips - Provisioned Throughput &amp; Exponential Backoff"},{"location":"aws/ec2/","text":"AWS EC2 EC2 Types On Demand - pay fixed rate by hour (or second) with no commitment Reserved - provides capacity reservation with significant discount on hourly charge - 1 Year or 3 Year terms Spot - enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexibile start and end times Dedicated Hosts - physical EC2 server dedicated for your use. Can help reduce costs by allowing you to use existing server-bound software licenses On Spot Instances : If they are terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged forthe complete hour in which the instance ran. EC2 Instance Types FIGHT DR MC PX: EBS Volume Types SSD General Purpose SSD (<10,000 IOPS) - balances price and performance for wide variety of workloads Provisioned IOPS SSD (>10,000 IOPS) - highest-performance SSD volume for mission-critical low-latency or high-throughput workloads MAGNETIC Throughput Optimized HDD - Low cost HDD volume designed for frequently accessed, throughput-intensive workloads Cold HDD - Lowest cost HDD volume designed for less frequently accessed workloads Magnetic - Previous Generation. Can be a boot volume. EC2 Lab Commands Basic WebServer sudo su [ superuser access ] yum update -y [ update ec2 instance ] uum install httpd -y [ install apache server ] service httpd start [ start apache server ] chkconfig httpd on [ on ec2 restart server will auto restart ] service httpd status [ check apache server status ] cd /var/www/html [ change to server root directory ] vim index.html [ create basic web page index file ]","title":"EC2"},{"location":"aws/ec2/#aws-ec2","text":"","title":"AWS EC2"},{"location":"aws/ec2/#ec2-types","text":"On Demand - pay fixed rate by hour (or second) with no commitment Reserved - provides capacity reservation with significant discount on hourly charge - 1 Year or 3 Year terms Spot - enables you to bid whatever price you want for instance capacity, providing for even greater savings if your applications have flexibile start and end times Dedicated Hosts - physical EC2 server dedicated for your use. Can help reduce costs by allowing you to use existing server-bound software licenses On Spot Instances : If they are terminated by Amazon EC2, you will not be charged for a partial hour of usage. However, if you terminate the instance yourself, you will be charged forthe complete hour in which the instance ran.","title":"EC2 Types"},{"location":"aws/ec2/#ec2-instance-types","text":"FIGHT DR MC PX:","title":"EC2 Instance Types"},{"location":"aws/ec2/#ebs-volume-types","text":"SSD General Purpose SSD (<10,000 IOPS) - balances price and performance for wide variety of workloads Provisioned IOPS SSD (>10,000 IOPS) - highest-performance SSD volume for mission-critical low-latency or high-throughput workloads MAGNETIC Throughput Optimized HDD - Low cost HDD volume designed for frequently accessed, throughput-intensive workloads Cold HDD - Lowest cost HDD volume designed for less frequently accessed workloads Magnetic - Previous Generation. Can be a boot volume.","title":"EBS Volume Types"},{"location":"aws/ec2/#ec2-lab-commands","text":"Basic WebServer sudo su [ superuser access ] yum update -y [ update ec2 instance ] uum install httpd -y [ install apache server ] service httpd start [ start apache server ] chkconfig httpd on [ on ec2 restart server will auto restart ] service httpd status [ check apache server status ] cd /var/www/html [ change to server root directory ] vim index.html [ create basic web page index file ]","title":"EC2 Lab Commands"},{"location":"aws/elasticache/","text":"AWS Elasticache Elasticache is a web services that makes it easy to deploy, operate and scale an in-memory cache in cloud. Improve performance of web apps by allowing retrieval of info from fast managed in memory caches instead of relying on slower disk-based DBs. Can be used to significantly improve latency for read-heavy app workloads (social networking/gaming) or compute-intensive workloads (recommendation engine) Caching improves app performance by storing critical piece of data in memory for low-latency access. Cached information may include the results of I/O-intensive DB queries or the results of computation intense calculations. Types Memcached Widely adopted memory object caching system. Protocol compliant with Memcached, so popular tools with existing Memcached environment will work with the services Pure caching solution with no persistence, ElastiCache manages Memcached nodes as a pool that can grow and shrink similar to an EC2 auto scaling group. individual nodes are expendable, and ElastiCache provides additional capabilities here, such as automatic node replacement and auto discovery. Use Cases: Object caching your goal - to offland DB. Simple caching model as possible Running large cache nodes & multithreaded performance with use of multiple cores Grow & scale cache horizontally Redis Popular open source in-memory key-value store supports data structs such as sorted sets and lists. Supports master/slave replication and Multi-AZ which can be used to achieve cross AZ redundancy. ElastiCache manages Redis more as a relational DB. Redis ElastiCache clusters are managed as stateful entities that include failover, similar to how Amazon RDS manages DB failover. Use Cases: More adv data types such as lists hashes & sets Sorting and ranking datasets in memory i.e. leaderboards Persistence of key store important Run multiple AZs with failover Exam Tips Typically scenario where DB is under stress/load -> may be asked which service you should use to alleviate this. Elasticache is a good choice if DB is read-heavy and not prone to frequent changing Redshift good answer if reason the DB is feeling stress is because management keep running OLAP transactions on it etc.","title":"Elasticache"},{"location":"aws/elasticache/#aws-elasticache","text":"Elasticache is a web services that makes it easy to deploy, operate and scale an in-memory cache in cloud. Improve performance of web apps by allowing retrieval of info from fast managed in memory caches instead of relying on slower disk-based DBs. Can be used to significantly improve latency for read-heavy app workloads (social networking/gaming) or compute-intensive workloads (recommendation engine) Caching improves app performance by storing critical piece of data in memory for low-latency access. Cached information may include the results of I/O-intensive DB queries or the results of computation intense calculations.","title":"AWS Elasticache"},{"location":"aws/elasticache/#types","text":"","title":"Types"},{"location":"aws/elasticache/#memcached","text":"Widely adopted memory object caching system. Protocol compliant with Memcached, so popular tools with existing Memcached environment will work with the services Pure caching solution with no persistence, ElastiCache manages Memcached nodes as a pool that can grow and shrink similar to an EC2 auto scaling group. individual nodes are expendable, and ElastiCache provides additional capabilities here, such as automatic node replacement and auto discovery. Use Cases: Object caching your goal - to offland DB. Simple caching model as possible Running large cache nodes & multithreaded performance with use of multiple cores Grow & scale cache horizontally","title":"Memcached"},{"location":"aws/elasticache/#redis","text":"Popular open source in-memory key-value store supports data structs such as sorted sets and lists. Supports master/slave replication and Multi-AZ which can be used to achieve cross AZ redundancy. ElastiCache manages Redis more as a relational DB. Redis ElastiCache clusters are managed as stateful entities that include failover, similar to how Amazon RDS manages DB failover. Use Cases: More adv data types such as lists hashes & sets Sorting and ranking datasets in memory i.e. leaderboards Persistence of key store important Run multiple AZs with failover","title":"Redis"},{"location":"aws/elasticache/#exam-tips","text":"Typically scenario where DB is under stress/load -> may be asked which service you should use to alleviate this. Elasticache is a good choice if DB is read-heavy and not prone to frequent changing Redshift good answer if reason the DB is feeling stress is because management keep running OLAP transactions on it etc.","title":"Exam Tips"},{"location":"aws/elb/","text":"AWS ELB Types of Load Balancer Application Load Balancer [Layer 7] Network Load Balancer [Layer 4] - AWS most expensive Classic Load Balancer - legacy purposes - non-recommended Application Load Balancers Best suited for load balancing of http and https traffic Operate at layer 7 and are application-aware Intelligent - you can create advanced request routing, sending specified requests to specific web servers Network Load Balancers Best suited for load balancing of TCP traffic where extremer performance is required Operating at the connection level (Layer 4) Capable of handling millions of requests per second while maintaining ultra-low latencies Use for extreme performance. [most expensive to buy] Classic Load Balancers Legacy Elastic Load Balancers Load balance http/https applications and use Layer 7 specific features such as X-Forwarded and sticky sessions Can also use strict Layer 4 load balancing for applications that rely purely on the TCP protocol If application stops responding the ELB responds with a 504 error. This means app is having issues - at either Web Server or Database Layer Identify where the application is failing, and scale it up or out where possible (504 err) X-Forwarded-For Header How to get IPv4 public address when load balancer sends EC2 the private address? Uses X-Forwarded-For header","title":"ELB"},{"location":"aws/elb/#aws-elb","text":"","title":"AWS ELB"},{"location":"aws/elb/#types-of-load-balancer","text":"Application Load Balancer [Layer 7] Network Load Balancer [Layer 4] - AWS most expensive Classic Load Balancer - legacy purposes - non-recommended","title":"Types of Load Balancer"},{"location":"aws/elb/#application-load-balancers","text":"Best suited for load balancing of http and https traffic Operate at layer 7 and are application-aware Intelligent - you can create advanced request routing, sending specified requests to specific web servers","title":"Application Load Balancers"},{"location":"aws/elb/#network-load-balancers","text":"Best suited for load balancing of TCP traffic where extremer performance is required Operating at the connection level (Layer 4) Capable of handling millions of requests per second while maintaining ultra-low latencies Use for extreme performance. [most expensive to buy]","title":"Network Load Balancers"},{"location":"aws/elb/#classic-load-balancers","text":"Legacy Elastic Load Balancers Load balance http/https applications and use Layer 7 specific features such as X-Forwarded and sticky sessions Can also use strict Layer 4 load balancing for applications that rely purely on the TCP protocol If application stops responding the ELB responds with a 504 error. This means app is having issues - at either Web Server or Database Layer Identify where the application is failing, and scale it up or out where possible (504 err) X-Forwarded-For Header How to get IPv4 public address when load balancer sends EC2 the private address? Uses X-Forwarded-For header","title":"Classic Load Balancers"},{"location":"aws/iam/","text":"AWS IAM IAM consists of: Users Groups [Group users and apply policies collectively] Roles Policy Documents [JSON] Basics: IAM is universal - does not apply to regions at this time The \u2018root account\u2019 is account created when first setup your AWS account (complete admin access) New Users have no permissions when first created New Users are assigned Access Key ID & Secret Access Keys when first created These are not same as a password, you cannot use the key ID and secret access key to login to the AWS management console You can use this to access AWS via the APIs and CLI Account Setup: You only get to view Access Key ID & Secrete Access Key once. If lost, you have to regenerate them. Always setup Multifactor Authentication (MFA) on your root account You can create and customise your own password rotation policies Practical: Applying IAM role to EC2 : Go to EC2 in UI Select Actions/Instance-Settings/ Attach/Replace IAM role Modifying S3 within EC2 : cd ~/.aws [ contains config credentials ] aws s3 ls aws s3 ls s3://experimenting-aws echo \"Hello v2 from EC2\" > hello2.txt aws s3 cp hello2.txt s3://experimenting-aws Exam Tips Roles allow you to not use Access Key ID\u2019s and Secret Access Keys Roles are preferred from a security perspective Roles are controlled by policies You can change a policy on a role and it will take immediate affect You can attach and detach roles to running EC2 instances without having to stop or terminate these instances","title":"IAM"},{"location":"aws/iam/#aws-iam","text":"IAM consists of: Users Groups [Group users and apply policies collectively] Roles Policy Documents [JSON]","title":"AWS IAM"},{"location":"aws/iam/#basics","text":"IAM is universal - does not apply to regions at this time The \u2018root account\u2019 is account created when first setup your AWS account (complete admin access) New Users have no permissions when first created New Users are assigned Access Key ID & Secret Access Keys when first created These are not same as a password, you cannot use the key ID and secret access key to login to the AWS management console You can use this to access AWS via the APIs and CLI","title":"Basics:"},{"location":"aws/iam/#account-setup","text":"You only get to view Access Key ID & Secrete Access Key once. If lost, you have to regenerate them. Always setup Multifactor Authentication (MFA) on your root account You can create and customise your own password rotation policies","title":"Account Setup:"},{"location":"aws/iam/#practical","text":"Applying IAM role to EC2 : Go to EC2 in UI Select Actions/Instance-Settings/ Attach/Replace IAM role Modifying S3 within EC2 : cd ~/.aws [ contains config credentials ] aws s3 ls aws s3 ls s3://experimenting-aws echo \"Hello v2 from EC2\" > hello2.txt aws s3 cp hello2.txt s3://experimenting-aws","title":"Practical:"},{"location":"aws/iam/#exam-tips","text":"Roles allow you to not use Access Key ID\u2019s and Secret Access Keys Roles are preferred from a security perspective Roles are controlled by policies You can change a policy on a role and it will take immediate affect You can attach and detach roles to running EC2 instances without having to stop or terminate these instances","title":"Exam Tips"},{"location":"aws/r53/","text":"AWS Route 53 Amazons DNS Service Allows you to map domain names to: EC2 instances Load balancers S3 buckets Lab Notes Register Domain on Route 53 (using namecheap use prefix.domain.com and match the namespace values) Create an A record set for the route 53 value. Use alias - clicking on Alias Target will provide dropdown where any pre-defined ELB/ALBs will appear You can setup a Load Balancer while in the EC2 dashboard - along left hand side you will see \u2018Load Balancing\u2019 Once setup you can go to the domain and should be directed to your server","title":"Route 53"},{"location":"aws/r53/#aws-route-53","text":"","title":"AWS Route 53"},{"location":"aws/r53/#amazons-dns-service","text":"Allows you to map domain names to: EC2 instances Load balancers S3 buckets","title":"Amazons DNS Service"},{"location":"aws/r53/#lab-notes","text":"Register Domain on Route 53 (using namecheap use prefix.domain.com and match the namespace values) Create an A record set for the route 53 value. Use alias - clicking on Alias Target will provide dropdown where any pre-defined ELB/ALBs will appear You can setup a Load Balancer while in the EC2 dashboard - along left hand side you will see \u2018Load Balancing\u2019 Once setup you can go to the domain and should be directed to your server","title":"Lab Notes"},{"location":"aws/rds/","text":"AWS Relational Database Service (RDS) Think traditional spreadsheet: Database Tables Row Fields (Columns) Relational Database Types (with RDS) SQL Server Oracle MySQL Server PostgresSQL Aurora (amazons DB) MariaDB Non-Relational Databases Database Collection - Table Document - Row Key Value Pairs - Fields Data Warehousing? Used for business intelligence, tools like Cognos, Jaspersoft, SQL Server, Reporting Services, Oracle Hyperion, and SAP NetWeaver Used to pull in very large and complex data sets. Usually used by Mgmt to do queries on data (such as current performance v targets, etc.) OLTP -v- OLAP Online Transaction Porcessing (OLTP) differs from Online Analytical Processing in terms of the types of queries you will run OLTP Example : Order number 2120121 - pulls up a row of data such as name, date, address to devlier to, delivery status etc. OLAP Example : Net profit for EMEA and Pacific for the Digital Radio Product. Pulls in large number of records Data warehousing DBs use different types of architecture both from a DB perspective and infrastructure layer. Elasticache A web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud Service improves the performance of web apps by allowign you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based DBs Elasticache supports two open-source in-memory caching engines: Memcached Redis Database Types - Summary: RDS - OLTP SQL MySQL PostgreSQL Oracle Aurora MariaDB No SQL DynamoDB OLAP Redshift Elasticache - In Memory Caching Memcached Redis RDS Lab Setup RDS through UI (MySQL - free tier) Create EC2 instance - Step 3 - Configure Instance Details - Include shell bootstrap script #!/bin/bash yum install httpd php php-mysql -y yum update -y chkconfig httpd on service httpd start echo \"<?php phpinfo();?>\" > /var/www/html/index.php cd var/www/html wget https://s3.eu-west-1.amazonaws.com/experimenting-aws/connect.php In s3 instance file will look like this (copy pasta the relevant value in here beforehand): <?php $username = \"sampledb\" ; $password = \"sampledb\" ; $hostname = \"\" ; // extracted from RDS dashboard connect name $dbname = \"sampledb\" ; //connection to the database $dbhandle = mysql_connect ( $hostname , $username , $password ) or die ( \"Unable to connect to MySQL\" ); echo \"Connected to MySQL using username - $username , password - $password , host - $hostname <br>\" ; $selected = mysql_select_db ( \" $dbname \" , $dbhandle ) or die ( \"Unable to connect to MySQL DB - check the database name and try again.\" ); ?> Always given a DNS address (not given a public IPv4 Address) Note once EC2 starts running you can go to the public IP address and should see the PHP server running. SSH into EC2 instance (can use public ip address) sudo su cd /var/www/html ls (should see 2 files from bootstrap in here) Go to http://34.247.73.41/connect.php Next - need to allow RDS access\u2026. Go to RDS via console Go to security groups (click first option) Click Inbound -> Add Rule MySQL/Aurora, TCP, 3306 - use security group of your EC2 instance (type sg it will auto complete available options) Go back to http://34.247.73.41/connect.php and you should see a \u2018connected to mysql\u2026\u2026.\u2019 message - success! Common Troubleshooting - RDS & EC2 in different security groups. Open port 3306 to the security group the RDS instance is in, open up mysql and allow to the security group your EC2 instance is in so the two can talk. Automated Backups Automated will allow you to recover any point within a \u2018retention period\u2019 Recover will choose most recent daily backup and apply those transaction logs (point in time recovery down to a second within the retention period (1-365 days)) Enabled by default - stored in S3 & free storage space equal to size of DB. Taken within a defined window, during window storage I/O may be suspended while data is being backup -> may experience some low-latency Snapshots User initiated -> stored even after deletion of original RDS instance. Whenever restored, there will be a new RDS instance with a new DNS endpoint. Encryption Encryption at rest is supported for MySQL, Oracle, SQL Server, PostgreSQL, MariaDB & Aurora. Done using AWS KMS service. Once RDS is encrypted data stored arest in underlying storage is encrypted as well as its automated backups, read replicas & snapshots. Currently encrypting existing DB instance isn\u2019t supported. Multi-AZ (disaster recovery) synchronous Allows exact copy of prod DB in another availability zone. AWS handles replication, so when your prod DB is written to, the write will auto sync to the stand by DB. In event of planned DB maintenance, DB failure or Availability one failure, RDS will automatically failover to the standby so DB operations can resume without admin intervention. Used only for disaster recovery -> not used for improving performance. Available on: SQL Server Oracle MySQL Server PostgreSQL MariaDB Aurora (enabled by default) Read Replicas (performance improvements) asynchronous Used for scaling not disaster recovery. Must have auto backups turned on in order to deploy a read replicas You can have up to 5 read replica copies of any DB You can have read replicas of replicas (watch latency) Each read replica will have its own DNS endpoint You can have read replicas that have Multi-AZ You can create read replicas of Multi-AZ source DBs Read replicas can be promoted to be their own DBs, this breaks the replication. You can have a read replica in a second region. Available on: MySQL PostgreSQL MariaDB Aurora","title":"RDS"},{"location":"aws/rds/#aws-relational-database-service-rds","text":"Think traditional spreadsheet: Database Tables Row Fields (Columns)","title":"AWS Relational Database Service (RDS)"},{"location":"aws/rds/#relational-database-types-with-rds","text":"SQL Server Oracle MySQL Server PostgresSQL Aurora (amazons DB) MariaDB","title":"Relational Database Types (with RDS)"},{"location":"aws/rds/#non-relational-databases","text":"Database Collection - Table Document - Row Key Value Pairs - Fields","title":"Non-Relational Databases"},{"location":"aws/rds/#data-warehousing","text":"Used for business intelligence, tools like Cognos, Jaspersoft, SQL Server, Reporting Services, Oracle Hyperion, and SAP NetWeaver Used to pull in very large and complex data sets. Usually used by Mgmt to do queries on data (such as current performance v targets, etc.) OLTP -v- OLAP Online Transaction Porcessing (OLTP) differs from Online Analytical Processing in terms of the types of queries you will run OLTP Example : Order number 2120121 - pulls up a row of data such as name, date, address to devlier to, delivery status etc. OLAP Example : Net profit for EMEA and Pacific for the Digital Radio Product. Pulls in large number of records Data warehousing DBs use different types of architecture both from a DB perspective and infrastructure layer.","title":"Data Warehousing?"},{"location":"aws/rds/#elasticache","text":"A web service that makes it easy to deploy, operate and scale an in-memory cache in the cloud Service improves the performance of web apps by allowign you to retrieve information from fast, managed, in-memory caches, instead of relying entirely on slower disk-based DBs Elasticache supports two open-source in-memory caching engines: Memcached Redis","title":"Elasticache"},{"location":"aws/rds/#database-types-summary","text":"RDS - OLTP SQL MySQL PostgreSQL Oracle Aurora MariaDB No SQL DynamoDB OLAP Redshift Elasticache - In Memory Caching Memcached Redis","title":"Database Types - Summary:"},{"location":"aws/rds/#rds-lab","text":"Setup RDS through UI (MySQL - free tier) Create EC2 instance - Step 3 - Configure Instance Details - Include shell bootstrap script #!/bin/bash yum install httpd php php-mysql -y yum update -y chkconfig httpd on service httpd start echo \"<?php phpinfo();?>\" > /var/www/html/index.php cd var/www/html wget https://s3.eu-west-1.amazonaws.com/experimenting-aws/connect.php In s3 instance file will look like this (copy pasta the relevant value in here beforehand): <?php $username = \"sampledb\" ; $password = \"sampledb\" ; $hostname = \"\" ; // extracted from RDS dashboard connect name $dbname = \"sampledb\" ; //connection to the database $dbhandle = mysql_connect ( $hostname , $username , $password ) or die ( \"Unable to connect to MySQL\" ); echo \"Connected to MySQL using username - $username , password - $password , host - $hostname <br>\" ; $selected = mysql_select_db ( \" $dbname \" , $dbhandle ) or die ( \"Unable to connect to MySQL DB - check the database name and try again.\" ); ?> Always given a DNS address (not given a public IPv4 Address) Note once EC2 starts running you can go to the public IP address and should see the PHP server running. SSH into EC2 instance (can use public ip address) sudo su cd /var/www/html ls (should see 2 files from bootstrap in here) Go to http://34.247.73.41/connect.php Next - need to allow RDS access\u2026. Go to RDS via console Go to security groups (click first option) Click Inbound -> Add Rule MySQL/Aurora, TCP, 3306 - use security group of your EC2 instance (type sg it will auto complete available options) Go back to http://34.247.73.41/connect.php and you should see a \u2018connected to mysql\u2026\u2026.\u2019 message - success! Common Troubleshooting - RDS & EC2 in different security groups. Open port 3306 to the security group the RDS instance is in, open up mysql and allow to the security group your EC2 instance is in so the two can talk.","title":"RDS Lab"},{"location":"aws/rds/#automated-backups","text":"Automated will allow you to recover any point within a \u2018retention period\u2019 Recover will choose most recent daily backup and apply those transaction logs (point in time recovery down to a second within the retention period (1-365 days)) Enabled by default - stored in S3 & free storage space equal to size of DB. Taken within a defined window, during window storage I/O may be suspended while data is being backup -> may experience some low-latency","title":"Automated Backups"},{"location":"aws/rds/#snapshots","text":"User initiated -> stored even after deletion of original RDS instance. Whenever restored, there will be a new RDS instance with a new DNS endpoint.","title":"Snapshots"},{"location":"aws/rds/#encryption","text":"Encryption at rest is supported for MySQL, Oracle, SQL Server, PostgreSQL, MariaDB & Aurora. Done using AWS KMS service. Once RDS is encrypted data stored arest in underlying storage is encrypted as well as its automated backups, read replicas & snapshots. Currently encrypting existing DB instance isn\u2019t supported.","title":"Encryption"},{"location":"aws/rds/#multi-az-disaster-recovery","text":"synchronous Allows exact copy of prod DB in another availability zone. AWS handles replication, so when your prod DB is written to, the write will auto sync to the stand by DB. In event of planned DB maintenance, DB failure or Availability one failure, RDS will automatically failover to the standby so DB operations can resume without admin intervention. Used only for disaster recovery -> not used for improving performance. Available on: SQL Server Oracle MySQL Server PostgreSQL MariaDB Aurora (enabled by default)","title":"Multi-AZ (disaster recovery)"},{"location":"aws/rds/#read-replicas-performance-improvements","text":"asynchronous Used for scaling not disaster recovery. Must have auto backups turned on in order to deploy a read replicas You can have up to 5 read replica copies of any DB You can have read replicas of replicas (watch latency) Each read replica will have its own DNS endpoint You can have read replicas that have Multi-AZ You can create read replicas of Multi-AZ source DBs Read replicas can be promoted to be their own DBs, this breaks the replication. You can have a read replica in a second region. Available on: MySQL PostgreSQL MariaDB Aurora","title":"Read Replicas (performance improvements)"},{"location":"containerisation/docker/","text":"Docker Docker Commands The -it flag will allow you to ctrl+c to close the running image docker run -p 3000:3000 -it 0319d6e6a683 The -d flag will run service in the background allowing continued terminal access docker run -p 3000:3000 -d 0319d6e6a683 To push an image to Docker Hub (required for Kubernetes access) docker login docker tag imageid your-login/docker-demo docker push your-login/docker-demo To tag an image during the build process docker build -t your-login/docker-demo . docker push your-login/docker-demo","title":"Docker"},{"location":"containerisation/docker/#docker","text":"","title":"Docker"},{"location":"containerisation/docker/#docker-commands","text":"The -it flag will allow you to ctrl+c to close the running image docker run -p 3000:3000 -it 0319d6e6a683 The -d flag will run service in the background allowing continued terminal access docker run -p 3000:3000 -d 0319d6e6a683 To push an image to Docker Hub (required for Kubernetes access) docker login docker tag imageid your-login/docker-demo docker push your-login/docker-demo To tag an image during the build process docker build -t your-login/docker-demo . docker push your-login/docker-demo","title":"Docker Commands"},{"location":"containerisation/kubernetes/","text":"Kubernetes Notes based on practice following the Udemy course: Learn DevOps: The Complete Kubernetes Course . See related GitHub code files here . Vagrant Commands To build a plain ubuntu box mkdir ubuntu vagrant init ubuntu/xenial64 vagrant up Running Docker Container on Kubernetes Before launching container based on Docker image need to create pod definition Pod: describes an application running on Kubernetes Service: contain one or more tightly coupled containers (that make up the app - easily communicate with local port numbers) Setup Application on Kubernetes kubectl create -f k8s/demopod-helloworld.yml kubectl describe pod nodehelloworld.example.com kubectl port-forward nodehelloworld.example.com 8081:3000 kubectl expose pod nodehelloworld.example.com --type=NodePort --name=helloworld-service minikube service helloworld-service --url returns url of the service running = kubectl get service Useful Kubectl Commands (tie in with above example) kubectl attach helloworld.example.com kubectl exec helloworld.example.com -- ls /app lists file running inside container kubectl exec helloworld.example.com -- touch /app/test.txt creates file inside container (will disappear if the container is killed - non-persistent data) kubectl describe service helloworld-service kubectl run -i -tty busybox --image=busybox --restart=Never -- sh Kops Cluster Deployment AFTER SSHING INTO LINUX INSTANCE (vagrant via putty): setup awscli sudo apt-get install python-pip sudo pip install awscli aws configure ( use IAM role details - with admin policy ) ls -ahl ~/.aws/ ( verify credentials ) setup kubectl wget https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl sudo mv kubectl /usr/local/bin/ sudo chmod +x /usr/local/bin/kubectl kubectl setup keygen ssh-keygen -f .ssh/id_rsa cat .ssh/id_rsa.pub rename kops installation sudo mv /usr/local/bin/kops-linux-amd64 /usr/local/bin/kops generate cluster with your s3 bucket with route53 dns zone kops create cluster --name=kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.yourowndomain.com kops update cluster kubernetes.yourowndomain.com --yes --state=s3://kops-state-randomhash check your cert and password to log into new cluster cat .kube/config check nodes are running kubectl get node test own program on nodes kubectl run hello-minikube --image = gcr.io/google_containers/echoserver:1.4 --port = 8080 kubectl expose deployment hello-minikube --type = NodePort kubectl get service at this point add custom rule to aws- using VPC dashbaord - select master node - modify inbound traffic - all allowed on the port of your app in this case: 31956 Test it out: http://api.kubernetes.yourowndomain.com:31956/ to delete and avoid payments kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash to agree to delete kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --yes Horizontal -v- Vertical Scaling You can only horizontally scale when your pod is stateless. (i.e. kubectl scale options). Stateful pods cannot be horizontally scaled. Useful Pod Commands kubectl get pod Get info about all running pods kubectl describe pod <pod> Describe one pod kubectl expose pod <pod> --port=444 --name=frontend Expose port of a pod (creates new service) kubectl port-forward <pod> 8080 Port forward teh exposed pod port to your local machine kubectl attach <podname> -i Attach to the pod kubectl exec <pod> --command Execute a command on the pod kubectl label pods <pod> mylabel=awesome Add a new label to a pod kubectl run -i --tty busybox --image=busybox --restart=Never -- sh Run a shell in a pod - very useful for debugging. Once in you can run telnet 172.17.0.5 3000 to hit service and then command GET / kubectl scale --replicas=4 -f file.yaml Scales the number of pods replicated kubectl get rc Gets replica controllers kubectl scale --replicas=4 -f rc/helloworld-controller Scales the number of pods replicated kubectl delete rc/helloworld-controller Deletes controller Useful Deployment Commands kubectl get deployments kubectl get rs kubectl get pods --show-labels kubectl rollout status deployment/helloworld-deployment kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 can be used to update app to latest version kubectl edit deployment/helloworld-deployment kubectl rollout history deployment/helloworld-deployment kubectl rollout undo deployment/helloworld-deployment --to-revision=n Useful Service Commands ClusterIP - virtual IP address only reachable from within the Cluster (default) NodePort - same on each node also reachable externally LoadBalancer - routes external traffic to every node on the NodePort (ELB on AWS) ExternalName can provide a DNS name for service (e.g. service discovery using DNS) - only works when DNS add-on is enabled By default service can only run between ports 30000-32767 but can be changed by adding \u2013service-node-port-range= argument to kube-apiserver minikube service service-name --url kubectl describe svc helloworld-service kubectl get svc svc short for service Labels key/value pairs attached to objects labels are not unique, multiple can be added to one objects Label selectors - you can use matching expressions to match labels kubectl get nodes --show-labels Node labels You can also use labels to tag nodes - once tagged you can use label selectors to let pods only run on specific nodes 2 steps to run a pod on a specific set of nodes: First you tag the node Then you add a nodeSelector to your pod configuration First step: add a label or multiple to your nodes: kubectl label nodes node1 hardware = high-spec kubectl label nodes node2 hardware = low = spec Second step: add a pod that uses those labels: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 nodeSelector : hardware : high-spec Health Checks To detect and resolve problems with you app, you can run health checks 2 types of healthchecks: Running a command in the container periodically Periodic checks on a URL (HTTP) apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 livenessProbe : httpGet : path : / port : 3000 initialDelaySeconds : 15 timeoutSeconds : 30 You can describe pods to see the liveness in effect and success/failure counts Readiness Probes livenessProbes: indicates whether container is Running readinessProbes: indicates whether container is ready to serve make sure at startup the pod will onyl receive traffic when test succeeds you can use them in conjunction Pod State Pod Status kubectl get pods [Pending/Succeeded/Failed/Unknown] Pod Condition kubectl describe pod PODNAME [PodScheduled/Ready/Initialized/Unschedulable/ContainersReady] Container State kubectl get pod <podname> [Running/Terminated/Waiting] Secrets A way to distribute credentials, keys, passwords or data to pods Use in the following ways: Use secrets as env varaiables Use secrets as a file in a pod Uses volumes to be mounted in a container with files, can be used for dotenv files or your app can just read this file. Use an external image to pull secrets from a private image registry To generate secrets using files: echo -n \"root\" > ./username.txt echo -n \"password\" > ./password.txt kubectl create secret generic db-user-pass --from-file = ./username.txt --from-file = ./password.txt Can also be an SSH key or SSL cert: kubectl create secret generic ssl-certificate --from-file = ssh-privatekey = ~/.sshid_rsa --ssl-cert = mysslcert.crt To generate secrets using yaml apiVersion : v1 kind : Secret metadata : name : db-secret type : Opaque data : password : blergh # base64 encoded username : blergh2 # base64 encoded echo -n \"blergh\" | base64 Using Secrets Create a pod that exposes secrets as env variables: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : db-secret key : username - name : SECRET_PASSWORD [ ... ] Alternatively you can provide the secrets in a file: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 volumeMounts : - name : credvolume mountPath : /etc/creds readOnly : true volumes : - name : credvolume secret : secretName : db-secrets Web UI Start dashboard Create dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml Create user Create sample user (if using RBAC - on by default on new installs with kops / kubeadm): kubectl create -f sample-user.yaml Get login token: kubectl -n kube-system get secret | grep admin-user kubectl -n kube-system describe secret admin-user-token-<id displayed by previous command> Login to dashboard Go to http://api.yourdomain.com:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login Login: admin Password: the password that is listed in ~/.kube/config (open file in editor and look for \u201cpassword: \u2026\u201d Choose for login token and enter the login token from the previous ste","title":"Kubernetes"},{"location":"containerisation/kubernetes/#kubernetes","text":"Notes based on practice following the Udemy course: Learn DevOps: The Complete Kubernetes Course . See related GitHub code files here .","title":"Kubernetes"},{"location":"containerisation/kubernetes/#vagrant-commands","text":"To build a plain ubuntu box mkdir ubuntu vagrant init ubuntu/xenial64 vagrant up","title":"Vagrant Commands"},{"location":"containerisation/kubernetes/#running-docker-container-on-kubernetes","text":"Before launching container based on Docker image need to create pod definition Pod: describes an application running on Kubernetes Service: contain one or more tightly coupled containers (that make up the app - easily communicate with local port numbers)","title":"Running Docker Container on Kubernetes"},{"location":"containerisation/kubernetes/#setup-application-on-kubernetes","text":"kubectl create -f k8s/demopod-helloworld.yml kubectl describe pod nodehelloworld.example.com kubectl port-forward nodehelloworld.example.com 8081:3000 kubectl expose pod nodehelloworld.example.com --type=NodePort --name=helloworld-service minikube service helloworld-service --url returns url of the service running = kubectl get service","title":"Setup Application on Kubernetes"},{"location":"containerisation/kubernetes/#useful-kubectl-commands-tie-in-with-above-example","text":"kubectl attach helloworld.example.com kubectl exec helloworld.example.com -- ls /app lists file running inside container kubectl exec helloworld.example.com -- touch /app/test.txt creates file inside container (will disappear if the container is killed - non-persistent data) kubectl describe service helloworld-service kubectl run -i -tty busybox --image=busybox --restart=Never -- sh","title":"Useful Kubectl Commands (tie in with above example)"},{"location":"containerisation/kubernetes/#kops-cluster-deployment","text":"AFTER SSHING INTO LINUX INSTANCE (vagrant via putty): setup awscli sudo apt-get install python-pip sudo pip install awscli aws configure ( use IAM role details - with admin policy ) ls -ahl ~/.aws/ ( verify credentials ) setup kubectl wget https://storage.googleapis.com/kubernetes-release/release/v1.6.1/bin/linux/amd64/kubectl sudo mv kubectl /usr/local/bin/ sudo chmod +x /usr/local/bin/kubectl kubectl setup keygen ssh-keygen -f .ssh/id_rsa cat .ssh/id_rsa.pub rename kops installation sudo mv /usr/local/bin/kops-linux-amd64 /usr/local/bin/kops generate cluster with your s3 bucket with route53 dns zone kops create cluster --name=kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --zones=eu-west-1a --node-count=2 --node-size=t2.micro --master-size=t2.micro --dns-zone=kubernetes.yourowndomain.com kops update cluster kubernetes.yourowndomain.com --yes --state=s3://kops-state-randomhash check your cert and password to log into new cluster cat .kube/config check nodes are running kubectl get node test own program on nodes kubectl run hello-minikube --image = gcr.io/google_containers/echoserver:1.4 --port = 8080 kubectl expose deployment hello-minikube --type = NodePort kubectl get service at this point add custom rule to aws- using VPC dashbaord - select master node - modify inbound traffic - all allowed on the port of your app in this case: 31956 Test it out: http://api.kubernetes.yourowndomain.com:31956/ to delete and avoid payments kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash to agree to delete kops delete cluster --name kubernetes.yourowndomain.com --state=s3://kops-state-randomhash --yes","title":"Kops Cluster Deployment"},{"location":"containerisation/kubernetes/#horizontal-v-vertical-scaling","text":"You can only horizontally scale when your pod is stateless. (i.e. kubectl scale options). Stateful pods cannot be horizontally scaled.","title":"Horizontal -v- Vertical Scaling"},{"location":"containerisation/kubernetes/#useful-pod-commands","text":"kubectl get pod Get info about all running pods kubectl describe pod <pod> Describe one pod kubectl expose pod <pod> --port=444 --name=frontend Expose port of a pod (creates new service) kubectl port-forward <pod> 8080 Port forward teh exposed pod port to your local machine kubectl attach <podname> -i Attach to the pod kubectl exec <pod> --command Execute a command on the pod kubectl label pods <pod> mylabel=awesome Add a new label to a pod kubectl run -i --tty busybox --image=busybox --restart=Never -- sh Run a shell in a pod - very useful for debugging. Once in you can run telnet 172.17.0.5 3000 to hit service and then command GET / kubectl scale --replicas=4 -f file.yaml Scales the number of pods replicated kubectl get rc Gets replica controllers kubectl scale --replicas=4 -f rc/helloworld-controller Scales the number of pods replicated kubectl delete rc/helloworld-controller Deletes controller","title":"Useful Pod Commands"},{"location":"containerisation/kubernetes/#useful-deployment-commands","text":"kubectl get deployments kubectl get rs kubectl get pods --show-labels kubectl rollout status deployment/helloworld-deployment kubectl set image deployment/helloworld-deployment k8s-demo=k8s-demo:2 can be used to update app to latest version kubectl edit deployment/helloworld-deployment kubectl rollout history deployment/helloworld-deployment kubectl rollout undo deployment/helloworld-deployment --to-revision=n","title":"Useful Deployment Commands"},{"location":"containerisation/kubernetes/#useful-service-commands","text":"ClusterIP - virtual IP address only reachable from within the Cluster (default) NodePort - same on each node also reachable externally LoadBalancer - routes external traffic to every node on the NodePort (ELB on AWS) ExternalName can provide a DNS name for service (e.g. service discovery using DNS) - only works when DNS add-on is enabled By default service can only run between ports 30000-32767 but can be changed by adding \u2013service-node-port-range= argument to kube-apiserver minikube service service-name --url kubectl describe svc helloworld-service kubectl get svc svc short for service","title":"Useful Service Commands"},{"location":"containerisation/kubernetes/#labels","text":"key/value pairs attached to objects labels are not unique, multiple can be added to one objects Label selectors - you can use matching expressions to match labels kubectl get nodes --show-labels","title":"Labels"},{"location":"containerisation/kubernetes/#node-labels","text":"You can also use labels to tag nodes - once tagged you can use label selectors to let pods only run on specific nodes 2 steps to run a pod on a specific set of nodes: First you tag the node Then you add a nodeSelector to your pod configuration First step: add a label or multiple to your nodes: kubectl label nodes node1 hardware = high-spec kubectl label nodes node2 hardware = low = spec Second step: add a pod that uses those labels: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 nodeSelector : hardware : high-spec","title":"Node labels"},{"location":"containerisation/kubernetes/#health-checks","text":"To detect and resolve problems with you app, you can run health checks 2 types of healthchecks: Running a command in the container periodically Periodic checks on a URL (HTTP) apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 livenessProbe : httpGet : path : / port : 3000 initialDelaySeconds : 15 timeoutSeconds : 30 You can describe pods to see the liveness in effect and success/failure counts","title":"Health Checks"},{"location":"containerisation/kubernetes/#readiness-probes","text":"livenessProbes: indicates whether container is Running readinessProbes: indicates whether container is ready to serve make sure at startup the pod will onyl receive traffic when test succeeds you can use them in conjunction","title":"Readiness Probes"},{"location":"containerisation/kubernetes/#pod-state","text":"Pod Status kubectl get pods [Pending/Succeeded/Failed/Unknown] Pod Condition kubectl describe pod PODNAME [PodScheduled/Ready/Initialized/Unschedulable/ContainersReady] Container State kubectl get pod <podname> [Running/Terminated/Waiting]","title":"Pod State"},{"location":"containerisation/kubernetes/#secrets","text":"A way to distribute credentials, keys, passwords or data to pods Use in the following ways: Use secrets as env varaiables Use secrets as a file in a pod Uses volumes to be mounted in a container with files, can be used for dotenv files or your app can just read this file. Use an external image to pull secrets from a private image registry To generate secrets using files: echo -n \"root\" > ./username.txt echo -n \"password\" > ./password.txt kubectl create secret generic db-user-pass --from-file = ./username.txt --from-file = ./password.txt Can also be an SSH key or SSL cert: kubectl create secret generic ssl-certificate --from-file = ssh-privatekey = ~/.sshid_rsa --ssl-cert = mysslcert.crt To generate secrets using yaml apiVersion : v1 kind : Secret metadata : name : db-secret type : Opaque data : password : blergh # base64 encoded username : blergh2 # base64 encoded echo -n \"blergh\" | base64","title":"Secrets"},{"location":"containerisation/kubernetes/#using-secrets","text":"Create a pod that exposes secrets as env variables: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 env : - name : SECRET_USERNAME valueFrom : secretKeyRef : name : db-secret key : username - name : SECRET_PASSWORD [ ... ] Alternatively you can provide the secrets in a file: apiVersion : v1 kind : pod metadata : name : nodehelloworld.example.com labels : app : helloworld spec : containers : - name : k8s-demo image : daniel40392/k8s-demo ports : - containerPort : 3000 volumeMounts : - name : credvolume mountPath : /etc/creds readOnly : true volumes : - name : credvolume secret : secretName : db-secrets","title":"Using Secrets"},{"location":"containerisation/kubernetes/#web-ui","text":"","title":"Web UI"},{"location":"containerisation/kubernetes/#start-dashboard","text":"Create dashboard: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml","title":"Start dashboard"},{"location":"containerisation/kubernetes/#create-user","text":"Create sample user (if using RBAC - on by default on new installs with kops / kubeadm): kubectl create -f sample-user.yaml","title":"Create user"},{"location":"containerisation/kubernetes/#get-login-token","text":"kubectl -n kube-system get secret | grep admin-user kubectl -n kube-system describe secret admin-user-token-<id displayed by previous command>","title":"Get login token:"},{"location":"containerisation/kubernetes/#login-to-dashboard","text":"Go to http://api.yourdomain.com:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login Login: admin Password: the password that is listed in ~/.kube/config (open file in editor and look for \u201cpassword: \u2026\u201d Choose for login token and enter the login token from the previous ste","title":"Login to dashboard"},{"location":"languages/go/","text":"Go","title":"Go"},{"location":"languages/go/#go","text":"","title":"Go"},{"location":"languages/java/","text":"Java","title":"Java"},{"location":"languages/java/#java","text":"","title":"Java"},{"location":"languages/javascript/","text":"JavaScript","title":"JavaScript"},{"location":"languages/javascript/#javascript","text":"","title":"JavaScript"},{"location":"languages/python/","text":"Python x = \"always look on\" ; y = \"the bright side\" ; z = x + y + \"of life\" ;","title":"Python"},{"location":"languages/python/#python","text":"x = \"always look on\" ; y = \"the bright side\" ; z = x + y + \"of life\" ;","title":"Python"},{"location":"languages/scala/","text":"Scala Variable Declaration // immutable val donutsToBuy : Int = 5 // mutable var favoriteDonut : String = \"Glazed Donut\" favoriteDonut = \"Vanilla Donut\" // lazy (delay the initialization until consumed by application) // this also uses type inference - note the lack of : String! lazy val donutService = \"initialize some donut service\" // declare a variable with no initialization var leastFavoriteDonut : String = _ leastFavoriteDonut = \"Plain Donut\" Supported Types val donutsBought : Int = 5 val bigNumberOfDonuts : Long = 100000000L val smallNumberOfDonuts : Short = 1 val priceOfDonut : Double = 2.50 val donutPrice : Float = 2.50f val donutStoreName : String = \"allaboutscala Donut Store\" val donutByte : Byte = 0xa val donutFirstLetter : Char = 'D' val nothing : Unit = () Collections Lists // List of Strings val fruit : List [ String ] = List ( \"apples\" , \"oranges\" , \"pears\" ) // List of Integers val nums : List [ Int ] = List ( 1 , 2 , 3 , 4 ) // Empty List. val empty : List [ Nothing ] = List () // Two dimensional list val dim : List [ List [ Int ]] = List ( List ( 1 , 0 , 0 ), List ( 0 , 1 , 0 ), List ( 0 , 0 , 1 ) ) // Basic Examples of common methods // another way of defining the fruit List above using cons (::) val fruit = \"apples\" :: ( \"oranges\" :: ( \"pears\" :: Nil )) val nums = Nil println ( \"Head of fruit : \" + fruit . head ) println ( \"Tail of fruit : \" + fruit . tail ) println ( \"Check if fruit is empty : \" + fruit . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty ) Sets // Empty set of integer type var s : Set [ Int ] = Set () // Set of integer type var s : Set [ Int ] = Set ( 1 , 3 , 5 , 7 ) or var s = Set ( 1 , 3 , 5 , 7 ) // find common elements between two sets val num1 = Set ( 5 , 6 , 9 , 20 , 30 , 45 ) val num2 = Set ( 50 , 60 , 9 , 20 , 35 , 55 ) println ( \"num1.&(num2) : \" + num1 .&( num2 ) ) println ( \"num1.intersect(num2) : \" + num1 . intersect ( num2 ) ) Maps // Empty hash table whose keys are strings and values are integers: var A : Map [ Char , Int ] = Map () // A map with keys and values. val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" ) // Basic examples of common methods val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" , \"peru\" -> \"#CD853F\" ) val nums : Map [ Int , Int ] = Map () println ( \"Keys in colors : \" + colors . keys ) println ( \"Values in colors : \" + colors . values ) println ( \"Check if colors is empty : \" + colors . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty ) Tuples // Short version val t = ( 1 , \"hello\" , Console ) // Long version val t = new Tuple3 ( 1 , \"hello\" , Console ) // Example Usage val t = ( 4 , 3 , 2 , 1 ) val sum = t . _1 + t . _2 + t . _3 + t . _4 println ( \"Sum of elements: \" + sum ) val t = new Tuple3 ( 1 , \"hello\" , Console ) println ( \"Concatenated String: \" + t . toString () ) Options // An Option[T] can be either Some[T] or None object, which represents a missing value. // Example object Demo { def main ( args : Array [ String ]) { val capitals = Map ( \"France\" -> \"Paris\" , \"Japan\" -> \"Tokyo\" ) println ( \"capitals.get( \\\"France\\\" ) : \" + capitals . get ( \"France\" )) println ( \"capitals.get( \\\"India\\\" ) : \" + capitals . get ( \"India\" )) } } // Example Output capitals . get ( \"France\" ) : Some ( Paris ) capitals.get ( \" India \" ) : None Iterators val it = Iterator ( \"a\" , \"number\" , \"of\" , \"words\" ) while ( it . hasNext ){ println ( it . next ()) } val ita = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) val itb = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) println ( \"Maximum valued element \" + ita . max ) println ( \"Minimum valued element \" + itb . min ) println ( \"Value of ita.size : \" + ita . size ) println ( \"Value of itb.length : \" + itb . length ) Conditionals/Matching If Statements var x = 10 ; if ( x < 20 ) 100 else if ( x > 20 ) 200 else 0 Pattern Matching import scala.util.Random val x : Int = Random . nextInt ( 10 ) x match { case 0 => \"zero\" case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } def matchTest ( x : Int ) : String = x match { case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } matchTest ( 3 ) // many matchTest ( 1 ) // one Loops For var a = 0 ; // for loop execution with a single range for ( a <- 1 to 10 ){ println ( \"Value of a: \" + a ); } var a = 0 ; var b = 0 ; // for loop execution with multiple ranges for ( a <- 1 to 3 ; b <- 1 to 3 ){ println ( \"Value of a: \" + a ); println ( \"Value of b: \" + b ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 ); // for loop execution with a collection for ( a <- numList ){ println ( \"Value of a: \" + a ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ); // for loop execution with multiple filters for ( a <- numList if a != 3 ; if a < 8 ){ println ( \"Value of a: \" + a ); } While var a = 10 ; // while loop execution while ( a < 20 ){ println ( \"Value of a: \" + a ); a = a + 1 ; }","title":"Scala"},{"location":"languages/scala/#scala","text":"","title":"Scala"},{"location":"languages/scala/#variable-declaration","text":"// immutable val donutsToBuy : Int = 5 // mutable var favoriteDonut : String = \"Glazed Donut\" favoriteDonut = \"Vanilla Donut\" // lazy (delay the initialization until consumed by application) // this also uses type inference - note the lack of : String! lazy val donutService = \"initialize some donut service\" // declare a variable with no initialization var leastFavoriteDonut : String = _ leastFavoriteDonut = \"Plain Donut\"","title":"Variable Declaration"},{"location":"languages/scala/#supported-types","text":"val donutsBought : Int = 5 val bigNumberOfDonuts : Long = 100000000L val smallNumberOfDonuts : Short = 1 val priceOfDonut : Double = 2.50 val donutPrice : Float = 2.50f val donutStoreName : String = \"allaboutscala Donut Store\" val donutByte : Byte = 0xa val donutFirstLetter : Char = 'D' val nothing : Unit = ()","title":"Supported Types"},{"location":"languages/scala/#collections","text":"","title":"Collections"},{"location":"languages/scala/#lists","text":"// List of Strings val fruit : List [ String ] = List ( \"apples\" , \"oranges\" , \"pears\" ) // List of Integers val nums : List [ Int ] = List ( 1 , 2 , 3 , 4 ) // Empty List. val empty : List [ Nothing ] = List () // Two dimensional list val dim : List [ List [ Int ]] = List ( List ( 1 , 0 , 0 ), List ( 0 , 1 , 0 ), List ( 0 , 0 , 1 ) ) // Basic Examples of common methods // another way of defining the fruit List above using cons (::) val fruit = \"apples\" :: ( \"oranges\" :: ( \"pears\" :: Nil )) val nums = Nil println ( \"Head of fruit : \" + fruit . head ) println ( \"Tail of fruit : \" + fruit . tail ) println ( \"Check if fruit is empty : \" + fruit . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty )","title":"Lists"},{"location":"languages/scala/#sets","text":"// Empty set of integer type var s : Set [ Int ] = Set () // Set of integer type var s : Set [ Int ] = Set ( 1 , 3 , 5 , 7 ) or var s = Set ( 1 , 3 , 5 , 7 ) // find common elements between two sets val num1 = Set ( 5 , 6 , 9 , 20 , 30 , 45 ) val num2 = Set ( 50 , 60 , 9 , 20 , 35 , 55 ) println ( \"num1.&(num2) : \" + num1 .&( num2 ) ) println ( \"num1.intersect(num2) : \" + num1 . intersect ( num2 ) )","title":"Sets"},{"location":"languages/scala/#maps","text":"// Empty hash table whose keys are strings and values are integers: var A : Map [ Char , Int ] = Map () // A map with keys and values. val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" ) // Basic examples of common methods val colors = Map ( \"red\" -> \"#FF0000\" , \"azure\" -> \"#F0FFFF\" , \"peru\" -> \"#CD853F\" ) val nums : Map [ Int , Int ] = Map () println ( \"Keys in colors : \" + colors . keys ) println ( \"Values in colors : \" + colors . values ) println ( \"Check if colors is empty : \" + colors . isEmpty ) println ( \"Check if nums is empty : \" + nums . isEmpty )","title":"Maps"},{"location":"languages/scala/#tuples","text":"// Short version val t = ( 1 , \"hello\" , Console ) // Long version val t = new Tuple3 ( 1 , \"hello\" , Console ) // Example Usage val t = ( 4 , 3 , 2 , 1 ) val sum = t . _1 + t . _2 + t . _3 + t . _4 println ( \"Sum of elements: \" + sum ) val t = new Tuple3 ( 1 , \"hello\" , Console ) println ( \"Concatenated String: \" + t . toString () )","title":"Tuples"},{"location":"languages/scala/#options","text":"// An Option[T] can be either Some[T] or None object, which represents a missing value. // Example object Demo { def main ( args : Array [ String ]) { val capitals = Map ( \"France\" -> \"Paris\" , \"Japan\" -> \"Tokyo\" ) println ( \"capitals.get( \\\"France\\\" ) : \" + capitals . get ( \"France\" )) println ( \"capitals.get( \\\"India\\\" ) : \" + capitals . get ( \"India\" )) } } // Example Output capitals . get ( \"France\" ) : Some ( Paris ) capitals.get ( \" India \" ) : None","title":"Options"},{"location":"languages/scala/#iterators","text":"val it = Iterator ( \"a\" , \"number\" , \"of\" , \"words\" ) while ( it . hasNext ){ println ( it . next ()) } val ita = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) val itb = Iterator ( 20 , 40 , 2 , 50 , 69 , 90 ) println ( \"Maximum valued element \" + ita . max ) println ( \"Minimum valued element \" + itb . min ) println ( \"Value of ita.size : \" + ita . size ) println ( \"Value of itb.length : \" + itb . length )","title":"Iterators"},{"location":"languages/scala/#conditionalsmatching","text":"","title":"Conditionals/Matching"},{"location":"languages/scala/#if-statements","text":"var x = 10 ; if ( x < 20 ) 100 else if ( x > 20 ) 200 else 0","title":"If Statements"},{"location":"languages/scala/#pattern-matching","text":"import scala.util.Random val x : Int = Random . nextInt ( 10 ) x match { case 0 => \"zero\" case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } def matchTest ( x : Int ) : String = x match { case 1 => \"one\" case 2 => \"two\" case _ => \"many\" } matchTest ( 3 ) // many matchTest ( 1 ) // one","title":"Pattern Matching"},{"location":"languages/scala/#loops","text":"","title":"Loops"},{"location":"languages/scala/#for","text":"var a = 0 ; // for loop execution with a single range for ( a <- 1 to 10 ){ println ( \"Value of a: \" + a ); } var a = 0 ; var b = 0 ; // for loop execution with multiple ranges for ( a <- 1 to 3 ; b <- 1 to 3 ){ println ( \"Value of a: \" + a ); println ( \"Value of b: \" + b ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 ); // for loop execution with a collection for ( a <- numList ){ println ( \"Value of a: \" + a ); } var a = 0 ; val numList = List ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ); // for loop execution with multiple filters for ( a <- numList if a != 3 ; if a < 8 ){ println ( \"Value of a: \" + a ); }","title":"For"},{"location":"languages/scala/#while","text":"var a = 10 ; // while loop execution while ( a < 20 ){ println ( \"Value of a: \" + a ); a = a + 1 ; }","title":"While"},{"location":"publications/dev/","text":"Publications My primary publications are posted over at dev.together . You can access my main account for an up to date list of my recent activity/publications or review the ocassionally updated list below. 2018 - DEV.to Publications Visualizing Fibonacci: For the Music Lover in You! Leveling Up: From Create-React-App to Express (& some neat styling libraries) Building A Portfolio: The Painful Way The Inspiration API: A project built with Scala & Play Framework A year in Professional Software Development Integrating Docker with your Personal Projects Lucas - A Webscraper in Go","title":"Publications"},{"location":"publications/dev/#publications","text":"My primary publications are posted over at dev.together . You can access my main account for an up to date list of my recent activity/publications or review the ocassionally updated list below.","title":"Publications"},{"location":"publications/dev/#2018-devto-publications","text":"Visualizing Fibonacci: For the Music Lover in You! Leveling Up: From Create-React-App to Express (& some neat styling libraries) Building A Portfolio: The Painful Way The Inspiration API: A project built with Scala & Play Framework A year in Professional Software Development Integrating Docker with your Personal Projects Lucas - A Webscraper in Go","title":"2018 - DEV.to Publications"}]}